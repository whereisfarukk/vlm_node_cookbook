{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "<div align=\"center\">\n",
        "<p align=\"center\" style=\"width: 100%;\">\n",
        "    <img src=\"https://raw.githubusercontent.com/vlm-run/.github/refs/heads/main/profile/assets/vlm-black.svg\" alt=\"VLM Run Logo\" width=\"80\" style=\"margin-bottom: -5px; color: #2e3138; vertical-align: middle; padding-right: 5px;\"><br>\n",
        "</p>\n",
        "<p align=\"center\"><a href=\"https://docs.vlm.run\"><b>Website</b></a> | <a href=\"https://docs.vlm.run/\"><b>API Docs</b></a> | <a href=\"https://docs.vlm.run/blog\"><b>Blog</b></a> | <a href=\"https://discord.gg/AMApC2UzVY\"><b>Discord</b></a> | <a href=\"https://chat.vlm.run\"><b>Chat</b></a>\n",
        "</p>\n",
        "</div>\n",
        "\n",
        "# VLM Run Orion - Image Understanding, Reasoning and Execution (Node.js)\n",
        "\n",
        "This comprehensive cookbook demonstrates [VLM Run Orion's](https://vlm.run/orion) image understanding, reasoning and execution capabilities using **Node.js/TypeScript**. For more details on the API, see the [Agent API docs](https://docs.vlm.run/agents/introduction).\n",
        "\n",
        "For this notebook, we'll cover how to use the **VLM Run Agent Chat Completions API** - an OpenAI-compatible interface for building powerful visual intelligence with the same familiar chat-completions interface.\n",
        "\n",
        "We'll cover the following topics:\n",
        " 1. Image VQA (captioning, tagging, question-answering)\n",
        " 2. Object Detection (people, faces, objects, etc.)\n",
        " 3. Object Segmentation (semantic, instance, etc.)\n",
        " 4. UI Parsing (Graphical UI parsing and understanding)\n",
        " 5. OCR (text detection, recognition, and understanding)\n",
        " 6. Image Generation (text-to-image, in-painting, out-painting, etc.)\n",
        " 7. Image Tools (cropping, super-resolution, rotating, etc.)\n",
        "\n",
        "## Prerequisites\n",
        "\n",
        "- Node.js 18+\n",
        "- VLM Run API key (get one at [app.vlm.run](https://app.vlm.run))\n",
        "- Deno or tslab kernel for running TypeScript in Jupyter\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Setup\n",
        "\n",
        "First, install the required packages and configure the environment.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "// Install the VLM Run SDK\n",
        "// npm install vlmrun openai zod zod-to-json-schema\n",
        "\n",
        "// If using Deno kernel, install dependencies via npm specifiers\n",
        "// For tslab, run: npm install vlmrun openai zod zod-to-json-schema in your project directory\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [],
      "source": [
        "// Import the VLM Run SDK and dependencies\n",
        "import { VlmRun } from \"vlmrun\";\n",
        "import { z } from \"zod\";\n",
        "import { zodToJsonSchema } from \"zod-to-json-schema\";\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "✓ API Key loaded successfully\n"
          ]
        }
      ],
      "source": [
        "// Get API key from environment variable\n",
        "const VLMRUN_API_KEY = Deno.env.get(\"VLMRUN_API_KEY\");\n",
        "\n",
        "if (!VLMRUN_API_KEY) {\n",
        "    throw new Error(\"Please set the VLMRUN_API_KEY environment variable\");\n",
        "}\n",
        "\n",
        "console.log(\"✓ API Key loaded successfully\");\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Initialize the VLM Run Client\n",
        "\n",
        "We use the OpenAI-compatible chat completions interface through the VLM Run SDK.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "✓ VLM Run client initialized successfully!\n",
            "Base URL: https://agent.vlm.run/v1\n",
            "Model: vlmrun-orion-1\n"
          ]
        }
      ],
      "source": [
        "// Initialize the VLM Run client using the SDK\n",
        "const client = new VlmRun({\n",
        "    apiKey: VLMRUN_API_KEY,\n",
        "    baseURL: \"https://agent.vlm.run/v1\"  // Use the agent API endpoint\n",
        "});\n",
        "\n",
        "console.log(\"✓ VLM Run client initialized successfully!\");\n",
        "console.log(\"Base URL: https://agent.vlm.run/v1\");\n",
        "console.log(\"Model: vlmrun-orion-1\");\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Response Models (Schemas)\n",
        "\n",
        "We define Zod schemas for structured outputs. These schemas provide type-safe, validated responses.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "✓ Response schemas defined successfully!\n",
            "Schemas include type-safe validation for structured outputs.\n"
          ]
        }
      ],
      "source": [
        "// Helper function to download images from URLs\n",
        "async function downloadImage(url: string): Promise<Uint8Array> {\n",
        "    const response = await fetch(url);\n",
        "    if (!response.ok) {\n",
        "        throw new Error(`Failed to download image: ${response.statusText}`);\n",
        "    }\n",
        "    return new Uint8Array(await response.arrayBuffer());\n",
        "}\n",
        "\n",
        "// Image URL Response Schema\n",
        "const ImageUrlResponseSchema = z.object({\n",
        "    url: z.string().describe(\"Pre-signed URL to the image\")\n",
        "});\n",
        "\n",
        "type ImageUrlResponse = z.infer<typeof ImageUrlResponseSchema>;\n",
        "\n",
        "// Image URL List Response Schema\n",
        "const ImageUrlListResponseSchema = z.object({\n",
        "    urls: z.array(ImageUrlResponseSchema).describe(\"List of pre-signed image URL responses\")\n",
        "});\n",
        "\n",
        "type ImageUrlListResponse = z.infer<typeof ImageUrlListResponseSchema>;\n",
        "\n",
        "// Detection Schema\n",
        "const DetectionSchema = z.object({\n",
        "    label: z.string().describe(\"Name of the detected object\"),\n",
        "    xywh: z.tuple([z.number(), z.number(), z.number(), z.number()])\n",
        "        .describe(\"Bounding box (x, y, width, height) normalized from 0-1\"),\n",
        "    confidence: z.number().nullable().optional().describe(\"Detection confidence score from 0-1\")\n",
        "});\n",
        "\n",
        "// Detections Response Schema\n",
        "const DetectionsResponseSchema = z.object({\n",
        "    detections: z.array(DetectionSchema).describe(\"List of detected objects with bounding boxes\")\n",
        "});\n",
        "\n",
        "type DetectionsResponse = z.infer<typeof DetectionsResponseSchema>;\n",
        "\n",
        "// Keypoint Schema\n",
        "const KeypointSchema = z.object({\n",
        "    xy: z.tuple([z.number(), z.number()])\n",
        "        .describe(\"Normalized keypoint coordinates (x, y) between 0-1\"),\n",
        "    label: z.string().describe(\"Label of the keypoint\")\n",
        "});\n",
        "\n",
        "// Keypoints Response Schema\n",
        "const KeypointsResponseSchema = z.object({\n",
        "    keypoints: z.array(KeypointSchema).describe(\"List of detected keypoints\")\n",
        "});\n",
        "\n",
        "type KeypointsResponse = z.infer<typeof KeypointsResponseSchema>;\n",
        "\n",
        "console.log(\"✓ Response schemas defined successfully!\");\n",
        "console.log(\"Schemas include type-safe validation for structured outputs.\");\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Helper Functions\n",
        "\n",
        "We create helper functions to simplify making chat completion requests with structured outputs.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "✓ Helper functions defined!\n"
          ]
        }
      ],
      "source": [
        "/**\n",
        " * Make a chat completion request with optional images and structured output.\n",
        " * \n",
        " * @param prompt - The text prompt/instruction\n",
        " * @param images - Optional list of images to process (URLs)\n",
        " * @param responseSchema - Optional Zod schema for structured output\n",
        " * @param model - Model to use (default: vlmrun-orion-1:auto)\n",
        " * @returns Parsed response if responseSchema provided, else raw response text\n",
        " */\n",
        "async function chatCompletion<T>(\n",
        "    prompt: string,\n",
        "    images?: string[],\n",
        "    responseSchema?: z.ZodSchema<T>,\n",
        "    model: string = \"vlmrun-orion-1:auto\"\n",
        "): Promise<T | string> {\n",
        "    const content: any[] = [];\n",
        "    content.push({ type: \"text\", text: prompt });\n",
        "\n",
        "    if (images) {\n",
        "        for (const image of images) {\n",
        "            if (typeof image === \"string\") {\n",
        "                if (!image.startsWith(\"http\")) {\n",
        "                    throw new Error(\"Image URLs must start with http or https\");\n",
        "                }\n",
        "                content.push({\n",
        "                    type: \"image_url\",\n",
        "                    image_url: { url: image, detail: \"auto\" }\n",
        "                });\n",
        "            }\n",
        "        }\n",
        "    }\n",
        "\n",
        "    const kwargs: any = {\n",
        "        model: model,\n",
        "        messages: [{ role: \"user\", content: content }]\n",
        "    };\n",
        "\n",
        "    if (responseSchema) {\n",
        "        kwargs.response_format = {\n",
        "            type: \"json_schema\",\n",
        "            schema: zodToJsonSchema(responseSchema)\n",
        "        } as any;\n",
        "    }\n",
        "\n",
        "    const response = await client.agent.completions.create(kwargs);\n",
        "    const responseText = response.choices[0].message.content || \"\";\n",
        "\n",
        "    if (responseSchema) {\n",
        "        const parsed = JSON.parse(responseText);\n",
        "        return responseSchema.parse(parsed) as T;\n",
        "    }\n",
        "\n",
        "    return responseText;\n",
        "}\n",
        "\n",
        "console.log(\"✓ Helper functions defined!\");\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Image Understanding, Reasoning, and Execution Capabilities\n",
        "\n",
        "VLM Run agents can perform a wide range of image processing tasks including object detection, face detection, segmentation, OCR, and more.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 1. Captioning & Tagging\n",
        "\n",
        "The simplest operation - load an image from a URL and caption it.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            ">> RESPONSE\n",
            "The image features a classic Volkswagen Beetle, painted in a vibrant aqua green or mint green color, parked parallel to a building. The car is well-preserved, showcasing chrome hubcaps and bumpers, with visible side mirrors. In the background, there's a light yellow-orange wall, possibly stucco, which shows signs of age and character. Set into this wall are two dark brown wooden doors: a double door with distinct arched top panels on the left and a simpler single door on the right. The ground is paved with textured, greyish-tan cobblestones or similar paving stones. The overall atmosphere is quaint, nostalgic, and warm, suggesting a sunny day in a historic or charming locale.\n",
            "\n",
            ">> IMAGE URL: https://storage.googleapis.com/vlm-data-public-prod/hub/examples/image.caption/car.jpg\n"
          ]
        }
      ],
      "source": [
        "const IMAGE_URL = \"https://storage.googleapis.com/vlm-data-public-prod/hub/examples/image.caption/car.jpg\";\n",
        "\n",
        "const result = await chatCompletion(\n",
        "    \"Generate a detailed description of this image.\",\n",
        "    [IMAGE_URL]\n",
        ");\n",
        "\n",
        "console.log(\">> RESPONSE\");\n",
        "console.log(result);\n",
        "console.log(\"\\n>> IMAGE URL:\", IMAGE_URL);\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 2a. Object Detection\n",
        "\n",
        "Detect objects in images with bounding boxes. The agent can detect common objects like people, vehicles, animals, and more.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            ">> RESPONSE\n",
            "{\n",
            "  detections: [\n",
            "    {\n",
            "      label: \"Marlin\",\n",
            "      xywh: [ 0.276, 0.343, 0.213, 0.295 ],\n",
            "      confidence: 0.95\n",
            "    },\n",
            "    {\n",
            "      label: \"Dory\",\n",
            "      xywh: [ 0.426, 0.14, 0.34, 0.49 ],\n",
            "      confidence: 0.96\n",
            "    },\n",
            "    {\n",
            "      label: \"Marlin\",\n",
            "      xywh: [ 0.375, 0.642, 0.233, 0.344 ],\n",
            "      confidence: 0.94\n",
            "    },\n",
            "    {\n",
            "      label: \"seahorse\",\n",
            "      xywh: [ 0.014, 0.473, 0.16, 0.505 ],\n",
            "      confidence: 0.92\n",
            "    },\n",
            "    {\n",
            "      label: \"turtle\",\n",
            "      xywh: [ 0.782, 0.4, 0.176, 0.2 ],\n",
            "      confidence: 0.91\n",
            "    },\n",
            "    {\n",
            "      label: \"fish\",\n",
            "      xywh: [ 0.774, 0.568, 0.192, 0.262 ],\n",
            "      confidence: 0.93\n",
            "    },\n",
            "    {\n",
            "      label: \"fish\",\n",
            "      xywh: [ 0.144, 0.525, 0.133, 0.365 ],\n",
            "      confidence: 0.92\n",
            "    },\n",
            "    {\n",
            "      label: \"fish\",\n",
            "      xywh: [ 0.488, 0.035, 0.073, 0.1 ],\n",
            "      confidence: 0.91\n",
            "    },\n",
            "    {\n",
            "      label: \"fish\",\n",
            "      xywh: [ 0.613, 0.008, 0.07, 0.119 ],\n",
            "      confidence: 0.9\n",
            "    },\n",
            "    {\n",
            "      label: \"shark\",\n",
            "      xywh: [ 0.161, 0.035, 0.163, 0.192 ],\n",
            "      confidence: 0.94\n",
            "    },\n",
            "    {\n",
            "      label: \"shark\",\n",
            "      xywh: [ 0.128, 0.235, 0.186, 0.148 ],\n",
            "      confidence: 0.93\n",
            "    },\n",
            "    {\n",
            "      label: \"shark\",\n",
            "      xywh: [ 0.013, 0.073, 0.175, 0.235 ],\n",
            "      confidence: 0.92\n",
            "    },\n",
            "    {\n",
            "      label: \"crab\",\n",
            "      xywh: [ 0.224, 0.743, 0.206, 0.246 ],\n",
            "      confidence: 0.9\n",
            "    },\n",
            "    {\n",
            "      label: \"octopus\",\n",
            "      xywh: [ 0.607, 0.586, 0.228, 0.357 ],\n",
            "      confidence: 0.91\n",
            "    },\n",
            "    {\n",
            "      label: \"puffer fish\",\n",
            "      xywh: [ 0.68, 0.064, 0.117, 0.163 ],\n",
            "      confidence: 0.9\n",
            "    },\n",
            "    {\n",
            "      label: \"turtle\",\n",
            "      xywh: [ 0.818, 0.075, 0.153, 0.2 ],\n",
            "      confidence: 0.91\n",
            "    }\n",
            "  ]\n",
            "}\n",
            "\n",
            ">> Detected 16 objects\n",
            "  1. Marlin: xywh=[0.276, 0.343, 0.213, 0.295]\n",
            "  2. Dory: xywh=[0.426, 0.140, 0.340, 0.490]\n",
            "  3. Marlin: xywh=[0.375, 0.642, 0.233, 0.344]\n",
            "  4. seahorse: xywh=[0.014, 0.473, 0.160, 0.505]\n",
            "  5. turtle: xywh=[0.782, 0.400, 0.176, 0.200]\n",
            "  6. fish: xywh=[0.774, 0.568, 0.192, 0.262]\n",
            "  7. fish: xywh=[0.144, 0.525, 0.133, 0.365]\n",
            "  8. fish: xywh=[0.488, 0.035, 0.073, 0.100]\n",
            "  9. fish: xywh=[0.613, 0.008, 0.070, 0.119]\n",
            "  10. shark: xywh=[0.161, 0.035, 0.163, 0.192]\n",
            "  11. shark: xywh=[0.128, 0.235, 0.186, 0.148]\n",
            "  12. shark: xywh=[0.013, 0.073, 0.175, 0.235]\n",
            "  13. crab: xywh=[0.224, 0.743, 0.206, 0.246]\n",
            "  14. octopus: xywh=[0.607, 0.586, 0.228, 0.357]\n",
            "  15. puffer fish: xywh=[0.680, 0.064, 0.117, 0.163]\n",
            "  16. turtle: xywh=[0.818, 0.075, 0.153, 0.200]\n"
          ]
        }
      ],
      "source": [
        "const IMAGE_URL = \"https://storage.googleapis.com/vlm-data-public-prod/hub/examples/image.agent/10-finding-nemo.jpeg\";\n",
        "\n",
        "const result = await chatCompletion(\n",
        "    \"Detect all the sea creatures in this image\",\n",
        "    [IMAGE_URL],\n",
        "    DetectionsResponseSchema\n",
        ") as DetectionsResponse;\n",
        "\n",
        "console.log(\">> RESPONSE\");\n",
        "console.log(result);\n",
        "console.log(`\\n>> Detected ${result.detections.length} objects`);\n",
        "result.detections.forEach((det, i) => {\n",
        "    console.log(`  ${i + 1}. ${det.label}: xywh=[${det.xywh.map(v => v.toFixed(3)).join(\", \")}]`);\n",
        "});\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 2b. Object Detection with Specific Prompt\n",
        "\n",
        "You can specify exactly which objects to detect using natural language.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            ">> RESPONSE\n",
            "{\n",
            "  detections: [\n",
            "    {\n",
            "      label: \"car\",\n",
            "      xywh: [ 0.054, 0.343, 0.88, 0.428 ],\n",
            "      confidence: 0.98\n",
            "    },\n",
            "    {\n",
            "      label: \"wheel\",\n",
            "      xywh: [ 0.142, 0.575, 0.147, 0.192 ],\n",
            "      confidence: 0.98\n",
            "    },\n",
            "    {\n",
            "      label: \"wheel\",\n",
            "      xywh: [ 0.707, 0.569, 0.149, 0.198 ],\n",
            "      confidence: 0.97\n",
            "    }\n",
            "  ]\n",
            "}\n",
            "\n",
            ">> Detected 3 objects\n",
            "  1. car: xywh=[0.054, 0.343, 0.880, 0.428]\n",
            "  2. wheel: xywh=[0.142, 0.575, 0.147, 0.192]\n",
            "  3. wheel: xywh=[0.707, 0.569, 0.149, 0.198]\n"
          ]
        }
      ],
      "source": [
        "const IMAGE_URL = \"https://storage.googleapis.com/vlm-data-public-prod/hub/examples/image.caption/car.jpg\";\n",
        "\n",
        "const result = await chatCompletion(\n",
        "    \"Detect the 'car' and its 'wheels' in the image\",\n",
        "    [IMAGE_URL],\n",
        "    DetectionsResponseSchema\n",
        ") as DetectionsResponse;\n",
        "\n",
        "console.log(\">> RESPONSE\");\n",
        "console.log(result);\n",
        "console.log(`\\n>> Detected ${result.detections.length} objects`);\n",
        "result.detections.forEach((det, i) => {\n",
        "    console.log(`  ${i + 1}. ${det.label}: xywh=[${det.xywh.map(v => v.toFixed(3)).join(\", \")}]`);\n",
        "});\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 2c. Face Detection\n",
        "\n",
        "Detect and localize faces in images with bounding boxes.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            ">> RESPONSE\n",
            "{\n",
            "  detections: [\n",
            "    {\n",
            "      label: \"face\",\n",
            "      xywh: [ 0.063, 0.197, 0.281, 0.527 ],\n",
            "      confidence: 0.98\n",
            "    },\n",
            "    {\n",
            "      label: \"face\",\n",
            "      xywh: [ 0.359, 0.193, 0.278, 0.531 ],\n",
            "      confidence: 0.97\n",
            "    },\n",
            "    {\n",
            "      label: \"face\",\n",
            "      xywh: [ 0.657, 0.197, 0.28, 0.527 ],\n",
            "      confidence: 0.96\n",
            "    }\n",
            "  ]\n",
            "}\n",
            "\n",
            ">> Detected 3 faces\n",
            "  Face 1: face, xywh=[0.063, 0.197, 0.281, 0.527]\n",
            "  Face 2: face, xywh=[0.359, 0.193, 0.278, 0.531]\n",
            "  Face 3: face, xywh=[0.657, 0.197, 0.280, 0.527]\n"
          ]
        }
      ],
      "source": [
        "const IMAGE_URL = \"https://storage.googleapis.com/vlm-data-public-prod/hub/examples/media.tv-news/finance_bb_3_speakers.jpg\";\n",
        "\n",
        "const result = await chatCompletion(\n",
        "    \"Detect all the faces in the image\",\n",
        "    [IMAGE_URL],\n",
        "    DetectionsResponseSchema\n",
        ") as DetectionsResponse;\n",
        "\n",
        "console.log(\">> RESPONSE\");\n",
        "console.log(result);\n",
        "console.log(`\\n>> Detected ${result.detections.length} faces`);\n",
        "result.detections.forEach((det, i) => {\n",
        "    console.log(`  Face ${i + 1}: ${det.label}, xywh=[${det.xywh.map(v => v.toFixed(3)).join(\", \")}]`);\n",
        "});\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 2d. Person Detection\n",
        "\n",
        "Detect and localize people in images with bounding boxes.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            ">> RESPONSE\n",
            "{\n",
            "  detections: [\n",
            "    {\n",
            "      label: \"person\",\n",
            "      xywh: [ 0.044, 0.306, 0.078, 0.24 ],\n",
            "      confidence: null\n",
            "    },\n",
            "    {\n",
            "      label: \"person\",\n",
            "      xywh: [ 0.094, 0.292, 0.081, 0.273 ],\n",
            "      confidence: null\n",
            "    },\n",
            "    {\n",
            "      label: \"person\",\n",
            "      xywh: [ 0.172, 0.287, 0.085, 0.259 ],\n",
            "      confidence: null\n",
            "    },\n",
            "    {\n",
            "      label: \"person\",\n",
            "      xywh: [ 0.242, 0.285, 0.084, 0.3 ],\n",
            "      confidence: null\n",
            "    },\n",
            "    {\n",
            "      label: \"person\",\n",
            "      xywh: [ 0.318, 0.322, 0.086, 0.28 ],\n",
            "      confidence: null\n",
            "    },\n",
            "    {\n",
            "      label: \"person\",\n",
            "      xywh: [ 0.39, 0.317, 0.09, 0.275 ],\n",
            "      confidence: null\n",
            "    },\n",
            "    {\n",
            "      label: \"person\",\n",
            "      xywh: [ 0.47, 0.303, 0.092, 0.297 ],\n",
            "      confidence: null\n",
            "    },\n",
            "    {\n",
            "      label: \"person\",\n",
            "      xywh: [ 0.55, 0.33, 0.086, 0.28 ],\n",
            "      confidence: null\n",
            "    },\n",
            "    {\n",
            "      label: \"person\",\n",
            "      xywh: [ 0.629, 0.318, 0.086, 0.302 ],\n",
            "      confidence: null\n",
            "    },\n",
            "    {\n",
            "      label: \"person\",\n",
            "      xywh: [ 0.703, 0.34, 0.085, 0.29 ],\n",
            "      confidence: null\n",
            "    },\n",
            "    {\n",
            "      label: \"person\",\n",
            "      xywh: [ 0.776, 0.317, 0.1, 0.323 ],\n",
            "      confidence: null\n",
            "    }\n",
            "  ]\n",
            "}\n",
            "\n",
            ">> Detected 11 people\n",
            "  Person 1: person, xywh=[0.044, 0.306, 0.078, 0.240]\n",
            "  Person 2: person, xywh=[0.094, 0.292, 0.081, 0.273]\n",
            "  Person 3: person, xywh=[0.172, 0.287, 0.085, 0.259]\n",
            "  Person 4: person, xywh=[0.242, 0.285, 0.084, 0.300]\n",
            "  Person 5: person, xywh=[0.318, 0.322, 0.086, 0.280]\n",
            "  Person 6: person, xywh=[0.390, 0.317, 0.090, 0.275]\n",
            "  Person 7: person, xywh=[0.470, 0.303, 0.092, 0.297]\n",
            "  Person 8: person, xywh=[0.550, 0.330, 0.086, 0.280]\n",
            "  Person 9: person, xywh=[0.629, 0.318, 0.086, 0.302]\n",
            "  Person 10: person, xywh=[0.703, 0.340, 0.085, 0.290]\n",
            "  Person 11: person, xywh=[0.776, 0.317, 0.100, 0.323]\n"
          ]
        }
      ],
      "source": [
        "const IMAGE_URL = \"https://storage.googleapis.com/vlm-data-public-prod/hub/examples/image.agent/lunch-skyscraper.jpg\";\n",
        "\n",
        "const result = await chatCompletion(\n",
        "    \"Detect all the people in the image\",\n",
        "    [IMAGE_URL],\n",
        "    DetectionsResponseSchema\n",
        ") as DetectionsResponse;\n",
        "\n",
        "console.log(\">> RESPONSE\");\n",
        "console.log(result);\n",
        "console.log(`\\n>> Detected ${result.detections.length} people`);\n",
        "result.detections.forEach((det, i) => {\n",
        "    console.log(`  Person ${i + 1}: ${det.label}, xywh=[${det.xywh.map(v => v.toFixed(3)).join(\", \")}]`);\n",
        "});\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 2e. Detect and blur faces\n",
        "\n",
        "Detect faces and blur them for privacy protection. Here we combine object / face detection with an image tool.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            ">> RESPONSE\n",
            "{\n",
            "  url: \"https://storage.googleapis.com/vlm-userdata-prod/agents/artifacts/ae8ae740-ddd6-426b-bde1-75540f99f277/e69bf92d-2bcb-4520-91b6-aeb59810df37/img_5e39d8.jpg?X-Goog-Algorithm=GOOG4-RSA-SHA256&X-Goog-Credential=vlm-deployments%40vlm-infra-prod.iam.gserviceaccount.com%2F20251219%2Fauto%2Fstorage%2Fgoog4_request&X-Goog-Date=20251219T083700Z&X-Goog-Expires=604800&X-Goog-SignedHeaders=host&X-Goog-Signature=5618099a04c626b9d3368c6573d5f31f4cfcadcb843052f31301074d719f7bbe10a1870710fb808c68e5daab21c299abdd69f51d4cd69432fa07f316808f154a35b8d24dc9de3c4daff8396a852319be5711af7378eece165b437cd9b7b7e3a662ae26f91724364fcc81491c06286b646f78b37e4ba9132463b7115054c9478eed4e41e443979c5a008f673be37e54ab01502ec53a0298d67eece40d389e8460ab4828ffe9541bc79d39c382f959f43e5559f4856611afb3f74e218a2426b48537d7afe859110435a1df37a04e6b899a8cf97eba2eea45bccc7a5c9b109792c8b5517668e33bf3525f54f4ead64e7783335eaf122557a78d8e778456b755c0ee\"\n",
            "}\n",
            "\n",
            ">> Blurred image URL: https://storage.googleapis.com/vlm-userdata-prod/agents/artifacts/ae8ae740-ddd6-426b-bde1-75540f99f277/e69bf92d-2bcb-4520-91b6-aeb59810df37/img_5e39d8.jpg?X-Goog-Algorithm=GOOG4-RSA-SHA256&X-Goog-Credential=vlm-deployments%40vlm-infra-prod.iam.gserviceaccount.com%2F20251219%2Fauto%2Fstorage%2Fgoog4_request&X-Goog-Date=20251219T083700Z&X-Goog-Expires=604800&X-Goog-SignedHeaders=host&X-Goog-Signature=5618099a04c626b9d3368c6573d5f31f4cfcadcb843052f31301074d719f7bbe10a1870710fb808c68e5daab21c299abdd69f51d4cd69432fa07f316808f154a35b8d24dc9de3c4daff8396a852319be5711af7378eece165b437cd9b7b7e3a662ae26f91724364fcc81491c06286b646f78b37e4ba9132463b7115054c9478eed4e41e443979c5a008f673be37e54ab01502ec53a0298d67eece40d389e8460ab4828ffe9541bc79d39c382f959f43e5559f4856611afb3f74e218a2426b48537d7afe859110435a1df37a04e6b899a8cf97eba2eea45bccc7a5c9b109792c8b5517668e33bf3525f54f4ead64e7783335eaf122557a78d8e778456b755c0ee\n"
          ]
        }
      ],
      "source": [
        "const IMAGE_URL = \"https://storage.googleapis.com/vlm-data-public-prod/hub/examples/media.tv-news/finance_bb_3_speakers.jpg\";\n",
        "\n",
        "const result = await chatCompletion(\n",
        "    \"Blur all the faces in this image and return the blurred image\",\n",
        "    [IMAGE_URL],\n",
        "    ImageUrlResponseSchema\n",
        ") as ImageUrlResponse;\n",
        "\n",
        "console.log(\">> RESPONSE\");\n",
        "console.log(result);\n",
        "console.log(\"\\n>> Blurred image URL:\", result.url);\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 3. Keypoint Detection\n",
        "\n",
        "Detect keypoints in images for counting and localization tasks.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            ">> RESPONSE\n",
            "{\n",
            "  keypoints: [\n",
            "    { xy: [ 0.1094, 0.1094 ], label: \"donuts\" },\n",
            "    { xy: [ 0.3594, 0.0781 ], label: \"donuts\" },\n",
            "    { xy: [ 0.5479, 0.1094 ], label: \"donuts\" },\n",
            "    { xy: [ 0.7881, 0.1094 ], label: \"donuts\" },\n",
            "    { xy: [ 0.7881, 0.2842 ], label: \"donuts\" },\n",
            "    { xy: [ 0.5, 0.5 ], label: \"donuts\" },\n",
            "    { xy: [ 0.2656, 0.3594 ], label: \"donuts\" },\n",
            "    { xy: [ 0.0537, 0.5 ], label: \"donuts\" },\n",
            "    { xy: [ 0.0293, 0.8525 ], label: \"donuts\" },\n",
            "    { xy: [ 0.2305, 0.7441 ], label: \"donuts\" },\n",
            "    { xy: [ 0.5, 0.832 ], label: \"donuts\" },\n",
            "    { xy: [ 0.7881, 0.7441 ], label: \"donuts\" },\n",
            "    { xy: [ 0.8486, 0.9414 ], label: \"donuts\" },\n",
            "    { xy: [ 0.9639, 0.5 ], label: \"donuts\" }\n",
            "  ]\n",
            "}\n",
            "\n",
            ">> Detected 14 keypoints\n",
            "  1. donuts: xy=[0.109, 0.109]\n",
            "  2. donuts: xy=[0.359, 0.078]\n",
            "  3. donuts: xy=[0.548, 0.109]\n",
            "  4. donuts: xy=[0.788, 0.109]\n",
            "  5. donuts: xy=[0.788, 0.284]\n",
            "  6. donuts: xy=[0.500, 0.500]\n",
            "  7. donuts: xy=[0.266, 0.359]\n",
            "  8. donuts: xy=[0.054, 0.500]\n",
            "  9. donuts: xy=[0.029, 0.853]\n",
            "  10. donuts: xy=[0.231, 0.744]\n",
            "  11. donuts: xy=[0.500, 0.832]\n",
            "  12. donuts: xy=[0.788, 0.744]\n",
            "  13. donuts: xy=[0.849, 0.941]\n",
            "  14. donuts: xy=[0.964, 0.500]\n"
          ]
        }
      ],
      "source": [
        "const IMAGE_URL = \"https://storage.googleapis.com/vlm-data-public-prod/hub/examples/image.object-detection/donuts.png\";\n",
        "\n",
        "const result = await chatCompletion(\n",
        "    \"Detect all the donuts as keypoints and return the coordinates.\",\n",
        "    [IMAGE_URL],\n",
        "    KeypointsResponseSchema\n",
        ") as KeypointsResponse;\n",
        "\n",
        "console.log(\">> RESPONSE\");\n",
        "console.log(result);\n",
        "console.log(`\\n>> Detected ${result.keypoints.length} keypoints`);\n",
        "result.keypoints.forEach((kp, i) => {\n",
        "    console.log(`  ${i + 1}. ${kp.label}: xy=[${kp.xy.map(v => v.toFixed(3)).join(\", \")}]`);\n",
        "});\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 4. Segmentation\n",
        "\n",
        "Create pixel-level segmentation masks for objects, people or regions in images.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            ">> RESPONSE\n",
            "{\n",
            "  url: \"https://storage.googleapis.com/vlm-userdata-prod/agents/artifacts/ae8ae740-ddd6-426b-bde1-75540f99f277/dfc02661-6871-49eb-9232-35b35d543952/img_981194.jpg?X-Goog-Algorithm=GOOG4-RSA-SHA256&X-Goog-Credential=vlm-deployments%40vlm-infra-prod.iam.gserviceaccount.com%2F20251219%2Fauto%2Fstorage%2Fgoog4_request&X-Goog-Date=20251219T084334Z&X-Goog-Expires=604800&X-Goog-SignedHeaders=host&X-Goog-Signature=74aa63c9d557a95f5676f489ab73fd0ad0b3a1cdc61c23bfeec6790328568d2721a07422a5818b103b8855bac6aee95d024e59ec5a99aa796032fad045ef2e29909eb8751f04f8f682413d755fb4b0de22db3b4a739f43c1a1d7d4190a9b621f219d6af2a2e1c8c7998e595af690afb940a05bd27d84ddd9f0c6b94543f9d104ce1ed612d7c7a71238c54295629d64265df741becf120738b865378c7b657717b9aeb4e387d704ba9064a384667630cd798bac4cacb6979b0b4bb747b7d6ef424769fe5ef7dcb1f8aceb287774e77594ec333e35e4bc78eec729b8398b02fdcb1002a0fa8850e2a8806abc462c4e27cd597f6fee933dc03e42f9908c13e0361e\"\n",
            "}\n",
            "\n",
            ">> Segmented image URL: https://storage.googleapis.com/vlm-userdata-prod/agents/artifacts/ae8ae740-ddd6-426b-bde1-75540f99f277/dfc02661-6871-49eb-9232-35b35d543952/img_981194.jpg?X-Goog-Algorithm=GOOG4-RSA-SHA256&X-Goog-Credential=vlm-deployments%40vlm-infra-prod.iam.gserviceaccount.com%2F20251219%2Fauto%2Fstorage%2Fgoog4_request&X-Goog-Date=20251219T084334Z&X-Goog-Expires=604800&X-Goog-SignedHeaders=host&X-Goog-Signature=74aa63c9d557a95f5676f489ab73fd0ad0b3a1cdc61c23bfeec6790328568d2721a07422a5818b103b8855bac6aee95d024e59ec5a99aa796032fad045ef2e29909eb8751f04f8f682413d755fb4b0de22db3b4a739f43c1a1d7d4190a9b621f219d6af2a2e1c8c7998e595af690afb940a05bd27d84ddd9f0c6b94543f9d104ce1ed612d7c7a71238c54295629d64265df741becf120738b865378c7b657717b9aeb4e387d704ba9064a384667630cd798bac4cacb6979b0b4bb747b7d6ef424769fe5ef7dcb1f8aceb287774e77594ec333e35e4bc78eec729b8398b02fdcb1002a0fa8850e2a8806abc462c4e27cd597f6fee933dc03e42f9908c13e0361e\n"
          ]
        }
      ],
      "source": [
        "const IMAGE_URL = \"https://storage.googleapis.com/vlm-data-public-prod/hub/examples/image.agent/lunch-skyscraper.jpg\";\n",
        "\n",
        "const result = await chatCompletion(\n",
        "    \"Detect all the people in this image, and segment them.\",\n",
        "    [IMAGE_URL],\n",
        "    ImageUrlResponseSchema\n",
        ") as ImageUrlResponse;\n",
        "\n",
        "console.log(\">> RESPONSE\");\n",
        "console.log(result);\n",
        "console.log(\"\\n>> Segmented image URL:\", result.url);\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 5. OCR (Optical Character Recognition)\n",
        "\n",
        "Extract text from images using OCR capabilities.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            ">> RESPONSE\n",
            "Today is Thursday, October 20th - But it definitely feels like a Friday. I'm already considering making a second cup of coffee - and I haven't even finished my first. Do I have a problem? Sometimes I'll flip through older notes I've taken, and my handwriting is unrecognizable. Perhaps it depends on the type of pen I use? I've tried writing in all caps BUT IT LOOKS SO FORCED AND UNNATURAL. Often times, I'll just take notes on my laptop, but I still seem to gravitate toward pen and paper. Any advice on what to improve? I already feel stressed out looking back at what I've just written - it looks like 3 different people wrote this!\n",
            "\n",
            ">> IMAGE URL: https://storage.googleapis.com/vlm-data-public-prod/hub/examples/agent_use_cases/hand_writting_beautification/image-ocr.jpg\n"
          ]
        }
      ],
      "source": [
        "const IMAGE_URL = \"https://storage.googleapis.com/vlm-data-public-prod/hub/examples/agent_use_cases/hand_writting_beautification/image-ocr.jpg\";\n",
        "\n",
        "const result = await chatCompletion(\n",
        "    \"Read the text in this image\",\n",
        "    [IMAGE_URL]\n",
        ");\n",
        "\n",
        "console.log(\">> RESPONSE\");\n",
        "console.log(result);\n",
        "console.log(\"\\n>> IMAGE URL:\", IMAGE_URL);\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 6. Image Generation\n",
        "\n",
        "Create, modify and remix images from text prompts or existing visuals.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 6a. Virtual Try-On\n",
        "\n",
        "Generate a virtual try-on of a dress on a person, with unique views and a seamless compositing.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Dress URL: https://storage.googleapis.com/vlm-data-public-prod/hub/examples/agent_use_cases/virtual_try_on/dress.png\n",
            "Person URL: https://storage.googleapis.com/vlm-data-public-prod/hub/examples/agent_use_cases/virtual_try_on/person.png\n"
          ]
        }
      ],
      "source": [
        "const DRESS_URL = \"https://storage.googleapis.com/vlm-data-public-prod/hub/examples/agent_use_cases/virtual_try_on/dress.png\";\n",
        "const PERSON_URL = \"https://storage.googleapis.com/vlm-data-public-prod/hub/examples/agent_use_cases/virtual_try_on/person.png\";\n",
        "\n",
        "console.log(\"Dress URL:\", DRESS_URL);\n",
        "console.log(\"Person URL:\", PERSON_URL);\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            ">> RESPONSE\n",
            "{\n",
            "  urls: [\n",
            "    {\n",
            "      url: \"https://storage.googleapis.com/vlm-userdata-prod/agents/artifacts/ae8ae740-ddd6-426b-bde1-75540f99f277/ec3fc215-4583-4093-b3ee-55201aee2e34/img_3dc78a.jpg?X-Goog-Algorithm=GOOG4-RSA-SHA256&X-Goog-Credential=vlm-deployments%40vlm-infra-prod.iam.gserviceaccount.com%2F20251219%2Fauto%2Fstorage%2Fgoog4_request&X-Goog-Date=20251219T084509Z&X-Goog-Expires=604800&X-Goog-SignedHeaders=host&X-Goog-Signature=45c8f19fc28a12f92bc94e093185b236f13ac7a2e47843e4c77bde94ca5592bf9d8bfcd5e5fae1231e33ad712c27cf023556207cbb9c35e79c318ac304e9aa3941837941f70b31da86b66c43b8d68a00053b7290580510c45e002db165994f49ac816bebb5121f6828b7a9825f7715a8f280934ef55eef4c9d1f88c90f2ee206be6a14ba0715d9094347fd1bedaba6274770611df36c16afafde82b18d1c2c093e825d6f6c41d6c49d1570cdb59ec699071d33f770df379f1a8be3066277f4a60bef717b6c3a52c8bdcb4087a45e12be15b6c1ac5bdc9ed3840fe4d4ecaf7e47ccc933f643cfc27d9d7f33afb32bb077cb5d4f6b9f14f6e6eddb2e076469e31f\"\n",
            "    },\n",
            "    {\n",
            "      url: \"https://storage.googleapis.com/vlm-userdata-prod/agents/artifacts/ae8ae740-ddd6-426b-bde1-75540f99f277/ec3fc215-4583-4093-b3ee-55201aee2e34/img_a80a58.jpg?X-Goog-Algorithm=GOOG4-RSA-SHA256&X-Goog-Credential=vlm-deployments%40vlm-infra-prod.iam.gserviceaccount.com%2F20251219%2Fauto%2Fstorage%2Fgoog4_request&X-Goog-Date=20251219T084509Z&X-Goog-Expires=604800&X-Goog-SignedHeaders=host&X-Goog-Signature=80b8cda5194de511bd188b46e28bbd3270e504b2a0636be7d27e6c6e40841416d59bc9d414fcacb2c1df2f29d6c427aa61d60ce67e21998607bbd9895801ef81b1e8736a2554ed619f082c556cb913dd2b6d6a0651d89cb030a4e5b7dd48994b50c3a21c3fd80cab56d95f385a9d36f9fe7c56682bb9921410fbbb16d763e8a7961d5c14ed6af90bc0c59b35246a6a7755d5fbd77a019edadbf2c293cad59483f0238acd441bfafa605b0fed195c1b9334b208b565795455fd1a38397ff5160a8349dfe4c119dac0bd9d856c406168d78a167d6871c74ba28f6fc11d08b34624a51f6bdf46c023cf2dc5ae801c69f87b3cc5a01fd213e2918069c6cf962859dc\"\n",
            "    }\n",
            "  ]\n",
            "}\n",
            "\n",
            ">> Generated 2 images\n",
            "  Image 1: https://storage.googleapis.com/vlm-userdata-prod/agents/artifacts/ae8ae740-ddd6-426b-bde1-75540f99f277/ec3fc215-4583-4093-b3ee-55201aee2e34/img_3dc78a.jpg?X-Goog-Algorithm=GOOG4-RSA-SHA256&X-Goog-Credential=vlm-deployments%40vlm-infra-prod.iam.gserviceaccount.com%2F20251219%2Fauto%2Fstorage%2Fgoog4_request&X-Goog-Date=20251219T084509Z&X-Goog-Expires=604800&X-Goog-SignedHeaders=host&X-Goog-Signature=45c8f19fc28a12f92bc94e093185b236f13ac7a2e47843e4c77bde94ca5592bf9d8bfcd5e5fae1231e33ad712c27cf023556207cbb9c35e79c318ac304e9aa3941837941f70b31da86b66c43b8d68a00053b7290580510c45e002db165994f49ac816bebb5121f6828b7a9825f7715a8f280934ef55eef4c9d1f88c90f2ee206be6a14ba0715d9094347fd1bedaba6274770611df36c16afafde82b18d1c2c093e825d6f6c41d6c49d1570cdb59ec699071d33f770df379f1a8be3066277f4a60bef717b6c3a52c8bdcb4087a45e12be15b6c1ac5bdc9ed3840fe4d4ecaf7e47ccc933f643cfc27d9d7f33afb32bb077cb5d4f6b9f14f6e6eddb2e076469e31f\n",
            "  Image 2: https://storage.googleapis.com/vlm-userdata-prod/agents/artifacts/ae8ae740-ddd6-426b-bde1-75540f99f277/ec3fc215-4583-4093-b3ee-55201aee2e34/img_a80a58.jpg?X-Goog-Algorithm=GOOG4-RSA-SHA256&X-Goog-Credential=vlm-deployments%40vlm-infra-prod.iam.gserviceaccount.com%2F20251219%2Fauto%2Fstorage%2Fgoog4_request&X-Goog-Date=20251219T084509Z&X-Goog-Expires=604800&X-Goog-SignedHeaders=host&X-Goog-Signature=80b8cda5194de511bd188b46e28bbd3270e504b2a0636be7d27e6c6e40841416d59bc9d414fcacb2c1df2f29d6c427aa61d60ce67e21998607bbd9895801ef81b1e8736a2554ed619f082c556cb913dd2b6d6a0651d89cb030a4e5b7dd48994b50c3a21c3fd80cab56d95f385a9d36f9fe7c56682bb9921410fbbb16d763e8a7961d5c14ed6af90bc0c59b35246a6a7755d5fbd77a019edadbf2c293cad59483f0238acd441bfafa605b0fed195c1b9334b208b565795455fd1a38397ff5160a8349dfe4c119dac0bd9d856c406168d78a167d6871c74ba28f6fc11d08b34624a51f6bdf46c023cf2dc5ae801c69f87b3cc5a01fd213e2918069c6cf962859dc\n"
          ]
        }
      ],
      "source": [
        "// Generate a virtual try-on of a dress on a person, with unique views\n",
        "const result = await chatCompletion(\n",
        "    \"You are provided with two images: one of a dress(the first image) and one of a person(the second image). Generate a few highly realistic virtual try-on by seamlessly compositing the dress onto the person, ensuring natural fit, alignment, and that the person appears fully and appropriately dressed. Provide 2 images (9:16 aspect ratio) as output: one from the front and one from the side.\",\n",
        "    [DRESS_URL, PERSON_URL],\n",
        "    ImageUrlListResponseSchema\n",
        ") as ImageUrlListResponse;\n",
        "\n",
        "console.log(\">> RESPONSE\");\n",
        "console.log(result);\n",
        "console.log(`\\n>> Generated ${result.urls.length} images`);\n",
        "result.urls.forEach((url, i) => {\n",
        "    console.log(`  Image ${i + 1}: ${url.url}`);\n",
        "});\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 7. Template Matching\n",
        "\n",
        "Find a template image within a larger reference image.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Template URL: https://storage.googleapis.com/vlm-data-public-prod/hub/examples/agent_use_cases/template-search/image-12.png\n",
            "Reference URL: https://storage.googleapis.com/vlm-data-public-prod/hub/examples/agent_use_cases/template-search/image-13.png\n"
          ]
        }
      ],
      "source": [
        "const TEMPLATE_URL = \"https://storage.googleapis.com/vlm-data-public-prod/hub/examples/agent_use_cases/template-search/image-12.png\";\n",
        "const REFERENCE_URL = \"https://storage.googleapis.com/vlm-data-public-prod/hub/examples/agent_use_cases/template-search/image-13.png\";\n",
        "\n",
        "console.log(\"Template URL:\", TEMPLATE_URL);\n",
        "console.log(\"Reference URL:\", REFERENCE_URL);\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            ">> RESPONSE\n",
            "{\n",
            "  detections: [\n",
            "    {\n",
            "      label: \"lemon\",\n",
            "      xywh: [ 0.252, 0.231, 0.15, 0.137 ],\n",
            "      confidence: 0.98\n",
            "    },\n",
            "    {\n",
            "      label: \"lemon\",\n",
            "      xywh: [ 0.505, 0.625, 0.209, 0.21 ],\n",
            "      confidence: 0.97\n",
            "    },\n",
            "    {\n",
            "      label: \"lemon\",\n",
            "      xywh: [ 0.737, 0.254, 0.118, 0.118 ],\n",
            "      confidence: 0.96\n",
            "    },\n",
            "    {\n",
            "      label: \"lemon\",\n",
            "      xywh: [ 0.118, 0.43, 0.138, 0.141 ],\n",
            "      confidence: 0.95\n",
            "    }\n",
            "  ]\n",
            "}\n",
            "\n",
            ">> Found 4 matches\n",
            "  1. lemon: xywh=[0.252, 0.231, 0.150, 0.137]\n",
            "  2. lemon: xywh=[0.505, 0.625, 0.209, 0.210]\n",
            "  3. lemon: xywh=[0.737, 0.254, 0.118, 0.118]\n",
            "  4. lemon: xywh=[0.118, 0.430, 0.138, 0.141]\n"
          ]
        }
      ],
      "source": [
        "const result = await chatCompletion(\n",
        "    \"Given two images, identify the specified item from the second image within the first image. Clearly highlight and draw bounding boxes around all occurrences of the item in the first image. Provide a brief description of the results.\",\n",
        "    [TEMPLATE_URL, REFERENCE_URL],\n",
        "    DetectionsResponseSchema\n",
        ") as DetectionsResponse;\n",
        "\n",
        "console.log(\">> RESPONSE\");\n",
        "console.log(result);\n",
        "console.log(`\\n>> Found ${result.detections.length} matches`);\n",
        "result.detections.forEach((det, i) => {\n",
        "    console.log(`  ${i + 1}. ${det.label}: xywh=[${det.xywh.map(v => v.toFixed(3)).join(\", \")}]`);\n",
        "});\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 8. UI Parsing\n",
        "\n",
        "Parse user interface elements from screenshots.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            ">> RESPONSE\n",
            "Detected 47 UI elements\n",
            "  1. search: xywh=[0.378, 0.110, 0.033, 0.021]\n",
            "  2. Store: xywh=[0.497, 0.231, 0.076, 0.119]\n",
            "  3. Microsoft: xywh=[0.287, 0.227, 0.076, 0.115]\n",
            "  4. Aox: xywh=[0.361, 0.346, 0.067, 0.099]\n",
            "  5. Mole: xywh=[0.637, 0.596, 0.053, 0.042]\n",
            "  6. (12) png: xywh=[0.305, 0.646, 0.198, 0.082]\n",
            "  7. Tonday alls: xywh=[0.305, 0.727, 0.200, 0.092]\n",
            "  8. (II} png: xywh=[0.516, 0.649, 0.181, 0.078]\n",
            "  9. Waiuz: xywh=[0.925, 0.938, 0.068, 0.056]\n",
            "  10. (Blpng: xywh=[0.517, 0.726, 0.178, 0.091]\n",
            "  ... and 37 more\n"
          ]
        }
      ],
      "source": [
        "const IMAGE_URL = \"https://storage.googleapis.com/vlm-data-public-prod/hub/examples/web.ui-automation/win11.jpeg\";\n",
        "\n",
        "const result = await chatCompletion(\n",
        "    \"Parse the UI of this screenshot and detect all the UI elements.\",\n",
        "    [IMAGE_URL],\n",
        "    DetectionsResponseSchema\n",
        ") as DetectionsResponse;\n",
        "\n",
        "console.log(\">> RESPONSE\");\n",
        "console.log(`Detected ${result.detections.length} UI elements`);\n",
        "result.detections.slice(0, 10).forEach((det, i) => {\n",
        "    console.log(`  ${i + 1}. ${det.label}: xywh=[${det.xywh.map(v => v.toFixed(3)).join(\", \")}]`);\n",
        "});\n",
        "if (result.detections.length > 10) {\n",
        "    console.log(`  ... and ${result.detections.length - 10} more`);\n",
        "}\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 9. Streaming Responses\n",
        "\n",
        "For long-running tasks, you can use streaming to get partial results as they become available.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Streaming response:\n",
            "----------------------------------------\n",
            "\n",
            "----------------------------------------\n",
            "\n",
            ">> Full response length: 252 characters\n"
          ]
        }
      ],
      "source": [
        "const IMAGE_URL = \"https://storage.googleapis.com/vlm-data-public-prod/hub/examples/image.caption/car.jpg\";\n",
        "\n",
        "const stream = await client.agent.completions.create({\n",
        "    model: \"vlmrun-orion-1:auto\",\n",
        "    messages: [{\n",
        "        role: \"user\",\n",
        "        content: [\n",
        "            { type: \"text\", text: \"Describe this image in detail\" },\n",
        "            { type: \"image_url\", image_url: { url: IMAGE_URL } }\n",
        "        ]\n",
        "    }],\n",
        "    stream: true\n",
        "});\n",
        "\n",
        "console.log(\"Streaming response:\");\n",
        "console.log(\"----------------------------------------\");\n",
        "let fullResponse = \"\";\n",
        "for await (const chunk of stream) {\n",
        "    const content = chunk.choices[0]?.delta?.content;\n",
        "    if (content) {\n",
        "        fullResponse += content;\n",
        "        // In a real notebook, you might want to display this incrementally\n",
        "        process.stdout.write(content);\n",
        "    }\n",
        "}\n",
        "console.log(\"\\n----------------------------------------\");\n",
        "console.log(\"\\n>> Full response length:\", fullResponse.length, \"characters\");\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "## Conclusion\n",
        "\n",
        "This cookbook demonstrated the comprehensive capabilities of the **VLM Run Orion Image Agent API** using Node.js/TypeScript.\n",
        "\n",
        "### Key Takeaways\n",
        "\n",
        "1. **OpenAI-Compatible Interface**: The API follows the OpenAI chat completions format, making it easy to integrate with existing workflows and tools.\n",
        "2. **Structured Outputs**: Use Zod schemas with `response_format` parameter to get type-safe, validated responses with automatic parsing.\n",
        "3. **Type Safety**: TypeScript and Zod provide compile-time and runtime type checking for better developer experience.\n",
        "4. **Streaming Support**: For long-running tasks, enable streaming to receive partial results as they become available, improving user experience.\n",
        "5. **Flexible Prompting**: Natural language prompts allow you to combine multiple operations in a single request, reducing API calls and latency.\n",
        "6. **Rich Capabilities**: The API supports object detection, segmentation, OCR, image generation, UI parsing, and more.\n",
        "\n",
        "### Next Steps\n",
        "\n",
        "- Explore the [VLM Run Documentation](https://docs.vlm.run) for more details\n",
        "- Join our [Discord community](https://discord.gg/AMApC2UzVY) for support\n",
        "- Check out more examples in the [VLM Run Cookbook](https://github.com/vlm-run/vlmrun-cookbook)\n",
        "- Review the [VLM Run Node.js SDK](https://github.com/vlm-run/vlmrun-node-sdk) documentation\n",
        "\n",
        "Happy building!\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Deno",
      "language": "typescript",
      "name": "deno"
    },
    "language_info": {
      "codemirror_mode": "typescript",
      "file_extension": ".ts",
      "mimetype": "text/x.typescript",
      "name": "typescript",
      "nbconvert_exporter": "script",
      "pygments_lexer": "typescript",
      "version": "5.9.2"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
