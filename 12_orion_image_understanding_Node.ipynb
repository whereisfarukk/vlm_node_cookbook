{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "<div align=\"center\">\n",
        "<p align=\"center\" style=\"width: 100%;\">\n",
        "    <img src=\"https://raw.githubusercontent.com/vlm-run/.github/refs/heads/main/profile/assets/vlm-black.svg\" alt=\"VLM Run Logo\" width=\"80\" style=\"margin-bottom: -5px; color: #2e3138; vertical-align: middle; padding-right: 5px;\"><br>\n",
        "</p>\n",
        "<p align=\"center\"><a href=\"https://docs.vlm.run\"><b>Website</b></a> | <a href=\"https://docs.vlm.run/\"><b>API Docs</b></a> | <a href=\"https://docs.vlm.run/blog\"><b>Blog</b></a> | <a href=\"https://discord.gg/AMApC2UzVY\"><b>Discord</b></a> | <a href=\"https://chat.vlm.run\"><b>Chat</b></a>\n",
        "</p>\n",
        "</div>\n",
        "\n",
        "# VLM Run Orion - Image Understanding, Reasoning and Execution (Node.js)\n",
        "\n",
        "This comprehensive cookbook demonstrates [VLM Run Orion's](https://vlm.run/orion) image understanding, reasoning and execution capabilities using **Node.js/TypeScript**. For more details on the API, see the [Agent API docs](https://docs.vlm.run/agents/introduction).\n",
        "\n",
        "For this notebook, we'll cover how to use the **VLM Run Agent Chat Completions API** - an OpenAI-compatible interface for building powerful visual intelligence with the same familiar chat-completions interface.\n",
        "\n",
        "We'll cover the following topics:\n",
        " 1. Image VQA (captioning, tagging, question-answering)\n",
        " 2. Object Detection (people, faces, objects, etc.)\n",
        " 3. Object Segmentation (semantic, instance, etc.)\n",
        " 4. UI Parsing (Graphical UI parsing and understanding)\n",
        " 5. OCR (text detection, recognition, and understanding)\n",
        " 6. Image Generation (text-to-image, in-painting, out-painting, etc.)\n",
        " 7. Image Tools (cropping, super-resolution, rotating, etc.)\n",
        "\n",
        "## Prerequisites\n",
        "\n",
        "- Node.js 18+\n",
        "- VLM Run API key (get one at [app.vlm.run](https://app.vlm.run))\n",
        "- Deno or tslab kernel for running TypeScript in Jupyter\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Setup\n",
        "\n",
        "First, install the required packages and configure the environment.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [],
      "source": [
        "// Install the VLM Run SDK\n",
        "// npm install vlmrun openai zod zod-to-json-schema\n",
        "\n",
        "// If using Deno kernel, install dependencies via npm specifiers\n",
        "// For tslab, run: npm install vlmrun openai zod zod-to-json-schema in your project directory\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 62,
      "metadata": {},
      "outputs": [],
      "source": [
        "// Import the VLM Run SDK and dependencies\n",
        "import { VlmRun } from \"npm:vlmrun\";\n",
        "import { z } from \"npm:zod\";\n",
        "import { zodToJsonSchema } from \"npm:zod-to-json-schema\";\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 63,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "✓ API Key loaded successfully\n"
          ]
        }
      ],
      "source": [
        "// Get API key from environment variable\n",
        "const VLMRUN_API_KEY = Deno.env.get(\"VLMRUN_API_KEY\");\n",
        "\n",
        "if (!VLMRUN_API_KEY) {\n",
        "    throw new Error(\"Please set the VLMRUN_API_KEY environment variable\");\n",
        "}\n",
        "\n",
        "console.log(\"✓ API Key loaded successfully\");\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Initialize the VLM Run Client\n",
        "\n",
        "We use the OpenAI-compatible chat completions interface through the VLM Run SDK.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 64,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "✓ VLM Run client initialized successfully!\n",
            "Base URL: https://agent.vlm.run/v1\n",
            "Model: vlmrun-orion-1\n"
          ]
        }
      ],
      "source": [
        "// Initialize the VLM Run client using the SDK\n",
        "const client = new VlmRun({\n",
        "    apiKey: VLMRUN_API_KEY,\n",
        "    baseURL: \"https://agent.vlm.run/v1\"  // Use the agent API endpoint\n",
        "});\n",
        "\n",
        "console.log(\"✓ VLM Run client initialized successfully!\");\n",
        "console.log(\"Base URL: https://agent.vlm.run/v1\");\n",
        "console.log(\"Model: vlmrun-orion-1\");\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Response Models (Schemas)\n",
        "\n",
        "We define Zod schemas for structured outputs. These schemas provide type-safe, validated responses.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "✓ Response schemas defined successfully!\n",
            "Schemas include type-safe validation for structured outputs.\n"
          ]
        }
      ],
      "source": [
        "/**\n",
        " * Parse artifact URL to extract sessionId and objectId.\n",
        " * Artifact URLs format: .../artifacts/{sessionId}/{uuid}/{objectId}.jpg\n",
        " * \n",
        " * @param artifactUrl - The artifact URL from API response\n",
        " * @returns Object with sessionId and objectId, or null if parsing fails\n",
        " */\n",
        "function parseArtifactUrl(artifactUrl: string): { sessionId: string; objectId: string } | null {\n",
        "    try {\n",
        "        const url = new URL(artifactUrl);\n",
        "        const pathParts = url.pathname.split('/').filter(p => p);\n",
        "        const artifactsIndex = pathParts.indexOf('artifacts');\n",
        "        \n",
        "        if (artifactsIndex === -1 || artifactsIndex + 3 >= pathParts.length) {\n",
        "            return null;\n",
        "        }\n",
        "        \n",
        "        const sessionId = pathParts[artifactsIndex + 1];\n",
        "        const objectIdWithExt = pathParts[artifactsIndex + 3];\n",
        "        const objectId = objectIdWithExt.split('.')[0];\n",
        "        \n",
        "        if (sessionId && objectId) {\n",
        "            return { sessionId, objectId };\n",
        "        }\n",
        "    } catch (e) {\n",
        "        // URL parsing failed\n",
        "    }\n",
        "    return null;\n",
        "}\n",
        "\n",
        "/**\n",
        " * Get artifact image using the artifacts API.\n",
        " * \n",
        " * @param artifactUrl - The artifact URL from API response (optional if sessionId/objectId provided)\n",
        " * @param sessionId - Optional: hardcoded session ID\n",
        " * @param objectId - Optional: hardcoded object ID\n",
        " * @returns Image data as Uint8Array, or null if failed\n",
        " */\n",
        "async function getArtifactImage(\n",
        "    artifactUrl?: string,\n",
        "    sessionId?: string,\n",
        "    objectId?: string\n",
        "): Promise<Uint8Array | null> {\n",
        "    let parsed: { sessionId: string; objectId: string } | null = null;\n",
        "    \n",
        "    // Use provided sessionId/objectId or parse from URL\n",
        "    if (sessionId && objectId) {\n",
        "        parsed = { sessionId, objectId };\n",
        "    } else if (artifactUrl) {\n",
        "        parsed = parseArtifactUrl(artifactUrl);\n",
        "        if (!parsed) {\n",
        "            console.error(\"Could not parse artifact URL\");\n",
        "            return null;\n",
        "        }\n",
        "    } else {\n",
        "        console.error(\"Must provide either artifactUrl or both sessionId and objectId\");\n",
        "        return null;\n",
        "    }\n",
        "    \n",
        "    try {\n",
        "        // Use SDK artifacts API\n",
        "        const artifact = await client.artifacts.get({\n",
        "            sessionId: parsed.sessionId,\n",
        "            objectId: parsed.objectId\n",
        "        });\n",
        "        \n",
        "        // Convert Buffer to Uint8Array\n",
        "        if (artifact instanceof Buffer) {\n",
        "            return new Uint8Array(artifact);\n",
        "        } else if (artifact instanceof Uint8Array) {\n",
        "            return artifact;\n",
        "        }\n",
        "        \n",
        "        return null;\n",
        "    } catch (error) {\n",
        "        const err = error instanceof Error ? error : new Error(String(error));\n",
        "        console.error(`Failed to get artifact: ${err.message}`);\n",
        "        return null;\n",
        "    }\n",
        "}\n",
        "\n",
        "// Image URL Response Schema\n",
        "const ImageUrlResponseSchema = z.object({\n",
        "    url: z.string().describe(\"Pre-signed URL to the image\")\n",
        "});\n",
        "\n",
        "type ImageUrlResponse = z.infer<typeof ImageUrlResponseSchema>;\n",
        "\n",
        "// Image URL List Response Schema\n",
        "const ImageUrlListResponseSchema = z.object({\n",
        "    urls: z.array(ImageUrlResponseSchema).describe(\"List of pre-signed image URL responses\")\n",
        "});\n",
        "\n",
        "type ImageUrlListResponse = z.infer<typeof ImageUrlListResponseSchema>;\n",
        "\n",
        "// Detection Schema\n",
        "const DetectionSchema = z.object({\n",
        "    label: z.string().describe(\"Name of the detected object\"),\n",
        "    xywh: z.tuple([z.number(), z.number(), z.number(), z.number()])\n",
        "        .describe(\"Bounding box (x, y, width, height) normalized from 0-1\"),\n",
        "    confidence: z.number().nullable().optional().describe(\"Detection confidence score from 0-1\")\n",
        "});\n",
        "\n",
        "// Detections Response Schema\n",
        "const DetectionsResponseSchema = z.object({\n",
        "    detections: z.array(DetectionSchema).describe(\"List of detected objects with bounding boxes\")\n",
        "});\n",
        "\n",
        "type DetectionsResponse = z.infer<typeof DetectionsResponseSchema>;\n",
        "\n",
        "// Keypoint Schema\n",
        "const KeypointSchema = z.object({\n",
        "    xy: z.tuple([z.number(), z.number()])\n",
        "        .describe(\"Normalized keypoint coordinates (x, y) between 0-1\"),\n",
        "    label: z.string().describe(\"Label of the keypoint\")\n",
        "});\n",
        "\n",
        "// Keypoints Response Schema\n",
        "const KeypointsResponseSchema = z.object({\n",
        "    keypoints: z.array(KeypointSchema).describe(\"List of detected keypoints\")\n",
        "});\n",
        "\n",
        "type KeypointsResponse = z.infer<typeof KeypointsResponseSchema>;\n",
        "\n",
        "console.log(\"✓ Response schemas defined successfully!\");\n",
        "console.log(\"Schemas include type-safe validation for structured outputs.\");\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Helper Functions\n",
        "\n",
        "We create helper functions to simplify making chat completion requests with structured outputs.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 67,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "✓ Helper functions defined!\n"
          ]
        }
      ],
      "source": [
        "/**\n",
        " * Make a chat completion request with optional images and structured output.\n",
        " * \n",
        " * @param prompt - The text prompt/instruction\n",
        " * @param images - Optional list of images to process (URLs)\n",
        " * @param responseSchema - Optional Zod schema for structured output\n",
        " * @param model - Model to use (default: vlmrun-orion-1:auto)\n",
        " * @returns Parsed response if responseSchema provided, else raw response text\n",
        " */\n",
        "async function chatCompletion<T>(\n",
        "    prompt: string,\n",
        "    images?: string[],\n",
        "    responseSchema?: z.ZodSchema<T>,\n",
        "    model: string = \"vlmrun-orion-1:auto\"\n",
        "): Promise<T | string> {\n",
        "    const content: any[] = [];\n",
        "    content.push({ type: \"text\", text: prompt });\n",
        "\n",
        "    if (images) {\n",
        "        for (const image of images) {\n",
        "            if (typeof image === \"string\") {\n",
        "                if (!image.startsWith(\"http\")) {\n",
        "                    throw new Error(\"Image URLs must start with http or https\");\n",
        "                }\n",
        "                content.push({\n",
        "                    type: \"image_url\",\n",
        "                    image_url: { url: image, detail: \"auto\" }\n",
        "                });\n",
        "            }\n",
        "        }\n",
        "    }\n",
        "\n",
        "    const kwargs: any = {\n",
        "        model: model,\n",
        "        messages: [{ role: \"user\", content: content }]\n",
        "    };\n",
        "\n",
        "    if (responseSchema) {\n",
        "        kwargs.response_format = {\n",
        "            type: \"json_schema\",\n",
        "            schema: zodToJsonSchema(responseSchema)\n",
        "        } as any;\n",
        "    }\n",
        "\n",
        "    const response = await client.agent.completions.create(kwargs);\n",
        "    const responseText = response.choices[0].message.content || \"\";\n",
        "\n",
        "    if (responseSchema) {\n",
        "        const parsed = JSON.parse(responseText);\n",
        "        return responseSchema.parse(parsed) as T;\n",
        "    }\n",
        "\n",
        "    return responseText;\n",
        "}\n",
        "\n",
        "console.log(\"✓ Helper functions defined!\");\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Image Understanding, Reasoning, and Execution Capabilities\n",
        "\n",
        "VLM Run agents can perform a wide range of image processing tasks including object detection, face detection, segmentation, OCR, and more.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 1. Captioning & Tagging\n",
        "\n",
        "The simplest operation - load an image from a URL and caption it.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 48,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            ">> RESPONSE\n",
            "The image features a vintage, mint green Volkswagen Beetle parked on a cobblestone street. The car is a classic Volkswagen Beetle, identifiable by its iconic rounded body shape, light mint green or seafoam green color. It has chrome bumpers, chrome trim around the windows, chrome hubcaps with a visible logo on the wheels, a white accent stripe along the side below the windows, side mirrors, and running boards. The car is positioned in front of a rustic building with a weathered, faded yellow or beige facade, likely made of stucco or plaster, showing signs of age and discoloration. The building has two prominent openings: on the left is a recessed dark brown wooden structure, possibly shutters or a window, characterized by two arched top sections. On the right, there is a large, dark brown wooden entrance door with vertical panels, set within a distinct white frame. The building has a flat roofline, and the ground in front of it is paved with light-colored cobblestones or patterned blocks. The overall scene evokes a sense of nostalgia and charm, likely set in a historic town.\n",
            "\n",
            ">> IMAGE URL: https://storage.googleapis.com/vlm-data-public-prod/hub/examples/image.caption/car.jpg\n"
          ]
        }
      ],
      "source": [
        "const IMAGE_URL = \"https://storage.googleapis.com/vlm-data-public-prod/hub/examples/image.caption/car.jpg\";\n",
        "\n",
        "const result = await chatCompletion(\n",
        "    \"Generate a detailed description of this image.\",\n",
        "    [IMAGE_URL]\n",
        ");\n",
        "\n",
        "console.log(\">> RESPONSE\");\n",
        "console.log(result);\n",
        "console.log(\"\\n>> IMAGE URL:\", IMAGE_URL);\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 2a. Object Detection\n",
        "\n",
        "Detect objects in images with bounding boxes. The agent can detect common objects like people, vehicles, animals, and more.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            ">> RESPONSE\n",
            "{\n",
            "  detections: [\n",
            "    { label: \"Nemo\", xywh: [ 0, 0, 0, 0 ], confidence: null },\n",
            "    { label: \"Dory\", xywh: [ 0, 0, 0, 0 ], confidence: null },\n",
            "    { label: \"Marlin\", xywh: [ 0, 0, 0, 0 ], confidence: null },\n",
            "    { label: \"Crush\", xywh: [ 0, 0, 0, 0 ], confidence: null },\n",
            "    { label: \"Squirt\", xywh: [ 0, 0, 0, 0 ], confidence: null },\n",
            "    { label: \"Hank\", xywh: [ 0, 0, 0, 0 ], confidence: null }\n",
            "  ]\n",
            "}\n",
            "\n",
            ">> Detected 6 objects\n",
            "  1. Nemo: xywh=[0.000, 0.000, 0.000, 0.000]\n",
            "  2. Dory: xywh=[0.000, 0.000, 0.000, 0.000]\n",
            "  3. Marlin: xywh=[0.000, 0.000, 0.000, 0.000]\n",
            "  4. Crush: xywh=[0.000, 0.000, 0.000, 0.000]\n",
            "  5. Squirt: xywh=[0.000, 0.000, 0.000, 0.000]\n",
            "  6. Hank: xywh=[0.000, 0.000, 0.000, 0.000]\n"
          ]
        }
      ],
      "source": [
        "const IMAGE_URL = \"https://storage.googleapis.com/vlm-data-public-prod/hub/examples/image.agent/10-finding-nemo.jpeg\";\n",
        "\n",
        "const result = await chatCompletion(\n",
        "    \"Detect all the sea creatures in this image\",\n",
        "    [IMAGE_URL],\n",
        "    DetectionsResponseSchema\n",
        ") as DetectionsResponse;\n",
        "\n",
        "console.log(\">> RESPONSE\");\n",
        "console.log(result);\n",
        "console.log(`\\n>> Detected ${result.detections.length} objects`);\n",
        "result.detections.forEach((det, i) => {\n",
        "    console.log(`  ${i + 1}. ${det.label}: xywh=[${det.xywh.map(v => v.toFixed(3)).join(\", \")}]`);\n",
        "});\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 2b. Object Detection with Specific Prompt\n",
        "\n",
        "You can specify exactly which objects to detect using natural language.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            ">> RESPONSE\n",
            "{\n",
            "  detections: [\n",
            "    {\n",
            "      label: \"car\",\n",
            "      xywh: [ 0.053, 0.343, 0.881, 0.424 ],\n",
            "      confidence: 0.99\n",
            "    },\n",
            "    {\n",
            "      label: \"wheel\",\n",
            "      xywh: [ 0.148, 0.579, 0.159, 0.19 ],\n",
            "      confidence: 0.98\n",
            "    },\n",
            "    {\n",
            "      label: \"wheel\",\n",
            "      xywh: [ 0.702, 0.579, 0.161, 0.19 ],\n",
            "      confidence: 0.97\n",
            "    }\n",
            "  ]\n",
            "}\n",
            "\n",
            ">> Detected 3 objects\n",
            "  1. car: xywh=[0.053, 0.343, 0.881, 0.424]\n",
            "  2. wheel: xywh=[0.148, 0.579, 0.159, 0.190]\n",
            "  3. wheel: xywh=[0.702, 0.579, 0.161, 0.190]\n"
          ]
        }
      ],
      "source": [
        "const IMAGE_URL = \"https://storage.googleapis.com/vlm-data-public-prod/hub/examples/image.caption/car.jpg\";\n",
        "\n",
        "const result = await chatCompletion(\n",
        "    \"Detect the 'car' and its 'wheels' in the image\",\n",
        "    [IMAGE_URL],\n",
        "    DetectionsResponseSchema\n",
        ") as DetectionsResponse;\n",
        "\n",
        "console.log(\">> RESPONSE\");\n",
        "console.log(result);\n",
        "console.log(`\\n>> Detected ${result.detections.length} objects`);\n",
        "result.detections.forEach((det, i) => {\n",
        "    console.log(`  ${i + 1}. ${det.label}: xywh=[${det.xywh.map(v => v.toFixed(3)).join(\", \")}]`);\n",
        "});\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 2c. Face Detection\n",
        "\n",
        "Detect and localize faces in images with bounding boxes.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            ">> RESPONSE\n",
            "{\n",
            "  detections: [\n",
            "    {\n",
            "      label: \"face\",\n",
            "      xywh: [ 0.066, 0.197, 0.268, 0.522 ],\n",
            "      confidence: null\n",
            "    },\n",
            "    {\n",
            "      label: \"face\",\n",
            "      xywh: [ 0.354, 0.186, 0.268, 0.533 ],\n",
            "      confidence: null\n",
            "    },\n",
            "    {\n",
            "      label: \"face\",\n",
            "      xywh: [ 0.655, 0.197, 0.268, 0.522 ],\n",
            "      confidence: null\n",
            "    }\n",
            "  ]\n",
            "}\n",
            "\n",
            ">> Detected 3 faces\n",
            "  Face 1: face, xywh=[0.066, 0.197, 0.268, 0.522]\n",
            "  Face 2: face, xywh=[0.354, 0.186, 0.268, 0.533]\n",
            "  Face 3: face, xywh=[0.655, 0.197, 0.268, 0.522]\n"
          ]
        }
      ],
      "source": [
        "const IMAGE_URL = \"https://storage.googleapis.com/vlm-data-public-prod/hub/examples/media.tv-news/finance_bb_3_speakers.jpg\";\n",
        "\n",
        "const result = await chatCompletion(\n",
        "    \"Detect all the faces in the image\",\n",
        "    [IMAGE_URL],\n",
        "    DetectionsResponseSchema\n",
        ") as DetectionsResponse;\n",
        "\n",
        "console.log(\">> RESPONSE\");\n",
        "console.log(result);\n",
        "console.log(`\\n>> Detected ${result.detections.length} faces`);\n",
        "result.detections.forEach((det, i) => {\n",
        "    console.log(`  Face ${i + 1}: ${det.label}, xywh=[${det.xywh.map(v => v.toFixed(3)).join(\", \")}]`);\n",
        "});\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 2d. Person Detection\n",
        "\n",
        "Detect and localize people in images with bounding boxes.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            ">> RESPONSE\n",
            "{\n",
            "  detections: [\n",
            "    {\n",
            "      label: \"person\",\n",
            "      xywh: [ 0.04, 0.304, 0.082, 0.25 ],\n",
            "      confidence: 0.98\n",
            "    },\n",
            "    {\n",
            "      label: \"person\",\n",
            "      xywh: [ 0.089, 0.288, 0.088, 0.276 ],\n",
            "      confidence: 0.97\n",
            "    },\n",
            "    {\n",
            "      label: \"person\",\n",
            "      xywh: [ 0.168, 0.285, 0.09, 0.266 ],\n",
            "      confidence: 0.96\n",
            "    },\n",
            "    {\n",
            "      label: \"person\",\n",
            "      xywh: [ 0.232, 0.282, 0.09, 0.303 ],\n",
            "      confidence: 0.95\n",
            "    },\n",
            "    {\n",
            "      label: \"person\",\n",
            "      xywh: [ 0.304, 0.318, 0.09, 0.28 ],\n",
            "      confidence: 0.94\n",
            "    },\n",
            "    {\n",
            "      label: \"person\",\n",
            "      xywh: [ 0.372, 0.315, 0.09, 0.27 ],\n",
            "      confidence: 0.93\n",
            "    },\n",
            "    {\n",
            "      label: \"person\",\n",
            "      xywh: [ 0.446, 0.299, 0.09, 0.293 ],\n",
            "      confidence: 0.92\n",
            "    },\n",
            "    {\n",
            "      label: \"person\",\n",
            "      xywh: [ 0.524, 0.326, 0.09, 0.275 ],\n",
            "      confidence: 0.91\n",
            "    },\n",
            "    {\n",
            "      label: \"person\",\n",
            "      xywh: [ 0.6, 0.315, 0.09, 0.3 ],\n",
            "      confidence: 0.9\n",
            "    },\n",
            "    {\n",
            "      label: \"person\",\n",
            "      xywh: [ 0.674, 0.335, 0.09, 0.29 ],\n",
            "      confidence: 0.89\n",
            "    },\n",
            "    {\n",
            "      label: \"person\",\n",
            "      xywh: [ 0.748, 0.311, 0.137, 0.324 ],\n",
            "      confidence: 0.88\n",
            "    }\n",
            "  ]\n",
            "}\n",
            "\n",
            ">> Detected 11 people\n",
            "  Person 1: person, xywh=[0.040, 0.304, 0.082, 0.250]\n",
            "  Person 2: person, xywh=[0.089, 0.288, 0.088, 0.276]\n",
            "  Person 3: person, xywh=[0.168, 0.285, 0.090, 0.266]\n",
            "  Person 4: person, xywh=[0.232, 0.282, 0.090, 0.303]\n",
            "  Person 5: person, xywh=[0.304, 0.318, 0.090, 0.280]\n",
            "  Person 6: person, xywh=[0.372, 0.315, 0.090, 0.270]\n",
            "  Person 7: person, xywh=[0.446, 0.299, 0.090, 0.293]\n",
            "  Person 8: person, xywh=[0.524, 0.326, 0.090, 0.275]\n",
            "  Person 9: person, xywh=[0.600, 0.315, 0.090, 0.300]\n",
            "  Person 10: person, xywh=[0.674, 0.335, 0.090, 0.290]\n",
            "  Person 11: person, xywh=[0.748, 0.311, 0.137, 0.324]\n"
          ]
        }
      ],
      "source": [
        "const IMAGE_URL = \"https://storage.googleapis.com/vlm-data-public-prod/hub/examples/image.agent/lunch-skyscraper.jpg\";\n",
        "\n",
        "const result = await chatCompletion(\n",
        "    \"Detect all the people in the image\",\n",
        "    [IMAGE_URL],\n",
        "    DetectionsResponseSchema\n",
        ") as DetectionsResponse;\n",
        "\n",
        "console.log(\">> RESPONSE\");\n",
        "console.log(result);\n",
        "console.log(`\\n>> Detected ${result.detections.length} people`);\n",
        "result.detections.forEach((det, i) => {\n",
        "    console.log(`  Person ${i + 1}: ${det.label}, xywh=[${det.xywh.map(v => v.toFixed(3)).join(\", \")}]`);\n",
        "});\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 2e. Detect and blur faces\n",
        "\n",
        "Detect faces and blur them for privacy protection. Here we combine object / face detection with an image tool.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            ">> RESPONSE\n",
            "{\n",
            "  url: \"https://storage.googleapis.com/vlm-userdata-prod/agents/artifacts/ae8ae740-ddd6-426b-bde1-75540f99f277/37d626ae-2fff-47c0-88c4-f1c7ef4d2b0d/img_bde11c.jpg?X-Goog-Algorithm=GOOG4-RSA-SHA256&X-Goog-Credential=vlm-deployments%40vlm-infra-prod.iam.gserviceaccount.com%2F20251221%2Fauto%2Fstorage%2Fgoog4_request&X-Goog-Date=20251221T081707Z&X-Goog-Expires=604800&X-Goog-SignedHeaders=host&X-Goog-Signature=2ebb1f563dcd0d68b1cfec0e893bfb3975493e872548eebaaccb87c6d3017dcaeedcb83897fe11a555c0fc3f1277998d6b81da228f9a50cd9d7e040d6e110abe56f3ed93048a964b062d1003312c59cf29fe2339c0dfc78b64613a4d9fc97750243ff8520d5b5bd833effc2b330ca23643d7cc0691044116febadf02abf9e08bd2129bd5aa620b3752e05bfed98074b7959b407b28ff72cc5c9f8ddfd6f7343b69cb01ba22ab09438c9f91b9c1c2ecb79ce1426a1e5d600306802dd6980bc8db5ac1c0618c7748b0cfbbf10a016c6f3bcb90ada3ea7d812452ab06f20609f225b73a3a584317a23d7c2e97b577110a22a89e951a97088a40c63b02264793ae16\"\n",
            "}\n",
            "\n",
            ">> Blurred image URL: https://storage.googleapis.com/vlm-userdata-prod/agents/artifacts/ae8ae740-ddd6-426b-bde1-75540f99f277/37d626ae-2fff-47c0-88c4-f1c7ef4d2b0d/img_bde11c.jpg?X-Goog-Algorithm=GOOG4-RSA-SHA256&X-Goog-Credential=vlm-deployments%40vlm-infra-prod.iam.gserviceaccount.com%2F20251221%2Fauto%2Fstorage%2Fgoog4_request&X-Goog-Date=20251221T081707Z&X-Goog-Expires=604800&X-Goog-SignedHeaders=host&X-Goog-Signature=2ebb1f563dcd0d68b1cfec0e893bfb3975493e872548eebaaccb87c6d3017dcaeedcb83897fe11a555c0fc3f1277998d6b81da228f9a50cd9d7e040d6e110abe56f3ed93048a964b062d1003312c59cf29fe2339c0dfc78b64613a4d9fc97750243ff8520d5b5bd833effc2b330ca23643d7cc0691044116febadf02abf9e08bd2129bd5aa620b3752e05bfed98074b7959b407b28ff72cc5c9f8ddfd6f7343b69cb01ba22ab09438c9f91b9c1c2ecb79ce1426a1e5d600306802dd6980bc8db5ac1c0618c7748b0cfbbf10a016c6f3bcb90ada3ea7d812452ab06f20609f225b73a3a584317a23d7c2e97b577110a22a89e951a97088a40c63b02264793ae16\n"
          ]
        }
      ],
      "source": [
        "const IMAGE_URL = \"https://storage.googleapis.com/vlm-data-public-prod/hub/examples/media.tv-news/finance_bb_3_speakers.jpg\";\n",
        "\n",
        "const result = await chatCompletion(\n",
        "    \"Blur all the faces in this image and return the blurred image\",\n",
        "    [IMAGE_URL],\n",
        "    ImageUrlResponseSchema\n",
        ") as ImageUrlResponse;\n",
        "\n",
        "console.log(\">> RESPONSE\");\n",
        "console.log(result);\n",
        "console.log(\"\\n>> Blurred image URL:\", result.url);\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 3. Keypoint Detection\n",
        "\n",
        "Detect keypoints in images for counting and localization tasks.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            ">> RESPONSE\n",
            "{\n",
            "  keypoints: [\n",
            "    { xy: [ 0.1094, 0.1094 ], label: \"donuts\" },\n",
            "    { xy: [ 0.3594, 0.0781 ], label: \"donuts\" },\n",
            "    { xy: [ 0.5479, 0.1094 ], label: \"donuts\" },\n",
            "    { xy: [ 0.7881, 0.1094 ], label: \"donuts\" },\n",
            "    { xy: [ 0.7686, 0.2842 ], label: \"donuts\" },\n",
            "    { xy: [ 0.5, 0.5 ], label: \"donuts\" },\n",
            "    { xy: [ 0.2725, 0.3594 ], label: \"donuts\" },\n",
            "    { xy: [ 0.0537, 0.5 ], label: \"donuts\" },\n",
            "    { xy: [ 0.0293, 0.8525 ], label: \"donuts\" },\n",
            "    { xy: [ 0.2305, 0.7441 ], label: \"donuts\" },\n",
            "    { xy: [ 0.5, 0.832 ], label: \"donuts\" },\n",
            "    { xy: [ 0.7881, 0.7441 ], label: \"donuts\" },\n",
            "    { xy: [ 0.8486, 0.9414 ], label: \"donuts\" },\n",
            "    { xy: [ 0.9639, 0.5 ], label: \"donuts\" }\n",
            "  ]\n",
            "}\n",
            "\n",
            ">> Detected 14 keypoints\n",
            "  1. donuts: xy=[0.109, 0.109]\n",
            "  2. donuts: xy=[0.359, 0.078]\n",
            "  3. donuts: xy=[0.548, 0.109]\n",
            "  4. donuts: xy=[0.788, 0.109]\n",
            "  5. donuts: xy=[0.769, 0.284]\n",
            "  6. donuts: xy=[0.500, 0.500]\n",
            "  7. donuts: xy=[0.273, 0.359]\n",
            "  8. donuts: xy=[0.054, 0.500]\n",
            "  9. donuts: xy=[0.029, 0.853]\n",
            "  10. donuts: xy=[0.231, 0.744]\n",
            "  11. donuts: xy=[0.500, 0.832]\n",
            "  12. donuts: xy=[0.788, 0.744]\n",
            "  13. donuts: xy=[0.849, 0.941]\n",
            "  14. donuts: xy=[0.964, 0.500]\n"
          ]
        }
      ],
      "source": [
        "const IMAGE_URL = \"https://storage.googleapis.com/vlm-data-public-prod/hub/examples/image.object-detection/donuts.png\";\n",
        "\n",
        "const result = await chatCompletion(\n",
        "    \"Detect all the donuts as keypoints and return the coordinates.\",\n",
        "    [IMAGE_URL],\n",
        "    KeypointsResponseSchema\n",
        ") as KeypointsResponse;\n",
        "\n",
        "console.log(\">> RESPONSE\");\n",
        "console.log(result);\n",
        "console.log(`\\n>> Detected ${result.keypoints.length} keypoints`);\n",
        "result.keypoints.forEach((kp, i) => {\n",
        "    console.log(`  ${i + 1}. ${kp.label}: xy=[${kp.xy.map(v => v.toFixed(3)).join(\", \")}]`);\n",
        "});\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 4. Segmentation\n",
        "\n",
        "Create pixel-level segmentation masks for objects, people or regions in images.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            ">> RESPONSE\n",
            "{\n",
            "  url: \"https://storage.googleapis.com/vlm-userdata-prod/agents/artifacts/ae8ae740-ddd6-426b-bde1-75540f99f277/bf055d72-2cbb-4caa-8967-a838467012ec/img_c2d1f3.jpg?X-Goog-Algorithm=GOOG4-RSA-SHA256&X-Goog-Credential=vlm-deployments%40vlm-infra-prod.iam.gserviceaccount.com%2F20251221%2Fauto%2Fstorage%2Fgoog4_request&X-Goog-Date=20251221T081926Z&X-Goog-Expires=604800&X-Goog-SignedHeaders=host&X-Goog-Signature=68dfcf62c093c15418ecf0213df0a876464f2fe2d4429210ce44926f0baef71b26b53db9f11d43d120fda84ad7c3e3ed5f45611963ab7f067e95fa7fad8235782566fcaca7379865ea25dbe2fd1d3c63fe917acd6eda2c2909152810ddc7875d7b7bb0e1138a7dc4b7195eac97d061b73df4bf9beb23b1725ea191a07bdce87491381b0a7c0907a72dd83073b626e7ee4495791a45146a43fa121ce5b6c1a95cc9e1d7091cd320d1c0def0941db00a4d7dd39bf96523cfe0fbb048c5318b0b49d9ead768533ac011adfff85cba001b85a52f3e9d870906555e9cc1139e4176e4e7ea6b069b3c0ae5d7bded69cec12ed9beb05978f137d6cd56fd52b78c8dd76b\"\n",
            "}\n",
            "\n",
            ">> Segmented image URL: https://storage.googleapis.com/vlm-userdata-prod/agents/artifacts/ae8ae740-ddd6-426b-bde1-75540f99f277/bf055d72-2cbb-4caa-8967-a838467012ec/img_c2d1f3.jpg?X-Goog-Algorithm=GOOG4-RSA-SHA256&X-Goog-Credential=vlm-deployments%40vlm-infra-prod.iam.gserviceaccount.com%2F20251221%2Fauto%2Fstorage%2Fgoog4_request&X-Goog-Date=20251221T081926Z&X-Goog-Expires=604800&X-Goog-SignedHeaders=host&X-Goog-Signature=68dfcf62c093c15418ecf0213df0a876464f2fe2d4429210ce44926f0baef71b26b53db9f11d43d120fda84ad7c3e3ed5f45611963ab7f067e95fa7fad8235782566fcaca7379865ea25dbe2fd1d3c63fe917acd6eda2c2909152810ddc7875d7b7bb0e1138a7dc4b7195eac97d061b73df4bf9beb23b1725ea191a07bdce87491381b0a7c0907a72dd83073b626e7ee4495791a45146a43fa121ce5b6c1a95cc9e1d7091cd320d1c0def0941db00a4d7dd39bf96523cfe0fbb048c5318b0b49d9ead768533ac011adfff85cba001b85a52f3e9d870906555e9cc1139e4176e4e7ea6b069b3c0ae5d7bded69cec12ed9beb05978f137d6cd56fd52b78c8dd76b\n"
          ]
        }
      ],
      "source": [
        "const IMAGE_URL = \"https://storage.googleapis.com/vlm-data-public-prod/hub/examples/image.agent/lunch-skyscraper.jpg\";\n",
        "\n",
        "const result = await chatCompletion(\n",
        "    \"Detect all the people in this image, and segment them.\",\n",
        "    [IMAGE_URL],\n",
        "    ImageUrlResponseSchema\n",
        ") as ImageUrlResponse;\n",
        "\n",
        "console.log(\">> RESPONSE\");\n",
        "console.log(result);\n",
        "console.log(\"\\n>> Segmented image URL:\", result.url);\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 5. OCR (Optical Character Recognition)\n",
        "\n",
        "Extract text from images using OCR capabilities.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            ">> RESPONSE\n",
            "The text in the image `img_eb6ce4` reads:\n",
            "\n",
            "\"Today is Thursday, October 20th- But it definitely feels like a Friday. I'm already considering making a second cup of coffee- and I haven't even finished my first. Do I have a problem? Sometimes I'll flip through older notes I've taken, and my handwriting is unrecognizable, Perhaps it depends on the type of pen I use? I've tried writing in all caps But IT Looks So FORCED AND UNNATURAL Often times, I'll just take notes on my laptop, but I still seem to gravitate toward pen and paper. Any advice on what to I'm prove ? I already feel stressed out looking back at what I've just written- it looks like 3 different people wrote this!\"\n",
            "\n",
            ">> IMAGE URL: https://storage.googleapis.com/vlm-data-public-prod/hub/examples/agent_use_cases/hand_writting_beautification/image-ocr.jpg\n"
          ]
        }
      ],
      "source": [
        "const IMAGE_URL = \"https://storage.googleapis.com/vlm-data-public-prod/hub/examples/agent_use_cases/hand_writting_beautification/image-ocr.jpg\";\n",
        "\n",
        "const result = await chatCompletion(\n",
        "    \"Read the text in this image\",\n",
        "    [IMAGE_URL]\n",
        ");\n",
        "\n",
        "console.log(\">> RESPONSE\");\n",
        "console.log(result);\n",
        "console.log(\"\\n>> IMAGE URL:\", IMAGE_URL);\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 6. Image Generation\n",
        "\n",
        "Create, modify and remix images from text prompts or existing visuals.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 6a. Virtual Try-On\n",
        "\n",
        "Generate a virtual try-on of a dress on a person, with unique views and a seamless compositing.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Dress URL: https://storage.googleapis.com/vlm-data-public-prod/hub/examples/agent_use_cases/virtual_try_on/dress.png\n",
            "Person URL: https://storage.googleapis.com/vlm-data-public-prod/hub/examples/agent_use_cases/virtual_try_on/person.png\n"
          ]
        }
      ],
      "source": [
        "const DRESS_URL = \"https://storage.googleapis.com/vlm-data-public-prod/hub/examples/agent_use_cases/virtual_try_on/dress.png\";\n",
        "const PERSON_URL = \"https://storage.googleapis.com/vlm-data-public-prod/hub/examples/agent_use_cases/virtual_try_on/person.png\";\n",
        "\n",
        "console.log(\"Dress URL:\", DRESS_URL);\n",
        "console.log(\"Person URL:\", PERSON_URL);\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            ">> RESPONSE\n",
            "{\n",
            "  urls: [\n",
            "    {\n",
            "      url: \"https://storage.googleapis.com/vlm-userdata-prod/agents/artifacts/ae8ae740-ddd6-426b-bde1-75540f99f277/afb27bf5-beb7-47fa-875d-e24546f6f4cc/img_193249.jpg?X-Goog-Algorithm=GOOG4-RSA-SHA256&X-Goog-Credential=vlm-deployments%40vlm-infra-prod.iam.gserviceaccount.com%2F20251221%2Fauto%2Fstorage%2Fgoog4_request&X-Goog-Date=20251221T082035Z&X-Goog-Expires=604800&X-Goog-SignedHeaders=host&X-Goog-Signature=64126c1487345cdb047fbeda9de5ba829c2b3f85e1f631a908000f429bc22e8569203d5c394a0d7d012907470bd447d71e93453a615b763dbc344c7f7f7bb0449356439da80f0aa9dadac8c9cbcfe14c1e96a0bad0f2ae69e167897f2f7037ad8912b76261440adf9eff80f6c1e95538ea93675e72d4f47efeacf72966182092ebcc8b65484342246080457c4a12bad72a6686bfa7e6522be1188c0104e4bb48213b6b26b9664673d04ef660d9cbf5198e8b86eb14fa9c37293cd14e205b89545a87fd9f73b73aadbba595d17a17505795f9a521ec6f28a021372132b0561cd5d9d3b1e41424ee1f75c553ddbd8f1a43162046f5a214840ef78fca210bd645a1\"\n",
            "    },\n",
            "    {\n",
            "      url: \"https://storage.googleapis.com/vlm-userdata-prod/agents/artifacts/ae8ae740-ddd6-426b-bde1-75540f99f277/afb27bf5-beb7-47fa-875d-e24546f6f4cc/img_abcf8a.jpg?X-Goog-Algorithm=GOOG4-RSA-SHA256&X-Goog-Credential=vlm-deployments%40vlm-infra-prod.iam.gserviceaccount.com%2F20251221%2Fauto%2Fstorage%2Fgoog4_request&X-Goog-Date=20251221T082035Z&X-Goog-Expires=604800&X-Goog-SignedHeaders=host&X-Goog-Signature=4f621d89c67d297d836b15fa3827c83b93d5698b7e81ac13d0445cf993fbce80cce733178187f388c0841b4feabf7c72458906cd6cda4a92a9a14ef28d684a5c0486a4e3be3c5d68bad990aa043c2929aa3929fc42e217c02d9c60b989cfae283244079825df1f5d1b3a36af92c33f6dc263582a1e9d1b6f5a402f50e80b47735353d7349a2960cc1978159d948d4bfd9d40dd04a23b51e3381189c42709e75c96d3806ba4f308445c3929cce6f98440b269b288e5c8db8a5a357dfc4fc1e0a2b99d5e87352355e87b997b2c844f6f0efcc5e926c3b55160f4d526d6fa7e94c2e066f768fb0ed57862e41c6972dd82e11c7e3dd5649cb8b69fd7dd36cd2ec859\"\n",
            "    }\n",
            "  ]\n",
            "}\n",
            "\n",
            ">> Generated 2 images\n",
            "  Image 1: https://storage.googleapis.com/vlm-userdata-prod/agents/artifacts/ae8ae740-ddd6-426b-bde1-75540f99f277/afb27bf5-beb7-47fa-875d-e24546f6f4cc/img_193249.jpg?X-Goog-Algorithm=GOOG4-RSA-SHA256&X-Goog-Credential=vlm-deployments%40vlm-infra-prod.iam.gserviceaccount.com%2F20251221%2Fauto%2Fstorage%2Fgoog4_request&X-Goog-Date=20251221T082035Z&X-Goog-Expires=604800&X-Goog-SignedHeaders=host&X-Goog-Signature=64126c1487345cdb047fbeda9de5ba829c2b3f85e1f631a908000f429bc22e8569203d5c394a0d7d012907470bd447d71e93453a615b763dbc344c7f7f7bb0449356439da80f0aa9dadac8c9cbcfe14c1e96a0bad0f2ae69e167897f2f7037ad8912b76261440adf9eff80f6c1e95538ea93675e72d4f47efeacf72966182092ebcc8b65484342246080457c4a12bad72a6686bfa7e6522be1188c0104e4bb48213b6b26b9664673d04ef660d9cbf5198e8b86eb14fa9c37293cd14e205b89545a87fd9f73b73aadbba595d17a17505795f9a521ec6f28a021372132b0561cd5d9d3b1e41424ee1f75c553ddbd8f1a43162046f5a214840ef78fca210bd645a1\n",
            "  Image 2: https://storage.googleapis.com/vlm-userdata-prod/agents/artifacts/ae8ae740-ddd6-426b-bde1-75540f99f277/afb27bf5-beb7-47fa-875d-e24546f6f4cc/img_abcf8a.jpg?X-Goog-Algorithm=GOOG4-RSA-SHA256&X-Goog-Credential=vlm-deployments%40vlm-infra-prod.iam.gserviceaccount.com%2F20251221%2Fauto%2Fstorage%2Fgoog4_request&X-Goog-Date=20251221T082035Z&X-Goog-Expires=604800&X-Goog-SignedHeaders=host&X-Goog-Signature=4f621d89c67d297d836b15fa3827c83b93d5698b7e81ac13d0445cf993fbce80cce733178187f388c0841b4feabf7c72458906cd6cda4a92a9a14ef28d684a5c0486a4e3be3c5d68bad990aa043c2929aa3929fc42e217c02d9c60b989cfae283244079825df1f5d1b3a36af92c33f6dc263582a1e9d1b6f5a402f50e80b47735353d7349a2960cc1978159d948d4bfd9d40dd04a23b51e3381189c42709e75c96d3806ba4f308445c3929cce6f98440b269b288e5c8db8a5a357dfc4fc1e0a2b99d5e87352355e87b997b2c844f6f0efcc5e926c3b55160f4d526d6fa7e94c2e066f768fb0ed57862e41c6972dd82e11c7e3dd5649cb8b69fd7dd36cd2ec859\n"
          ]
        }
      ],
      "source": [
        "// Generate a virtual try-on of a dress on a person, with unique views\n",
        "const result = await chatCompletion(\n",
        "    \"You are provided with two images: one of a dress(the first image) and one of a person(the second image). Generate a few highly realistic virtual try-on by seamlessly compositing the dress onto the person, ensuring natural fit, alignment, and that the person appears fully and appropriately dressed. Provide 2 images (9:16 aspect ratio) as output: one from the front and one from the side.\",\n",
        "    [DRESS_URL, PERSON_URL],\n",
        "    ImageUrlListResponseSchema\n",
        ") as ImageUrlListResponse;\n",
        "\n",
        "console.log(\">> RESPONSE\");\n",
        "console.log(result);\n",
        "console.log(`\\n>> Generated ${result.urls.length} images`);\n",
        "result.urls.forEach((url, i) => {\n",
        "    console.log(`  Image ${i + 1}: ${url.url}`);\n",
        "});\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 7. Template Matching\n",
        "\n",
        "Find a template image within a larger reference image.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Template URL: https://storage.googleapis.com/vlm-data-public-prod/hub/examples/agent_use_cases/template-search/image-12.png\n",
            "Reference URL: https://storage.googleapis.com/vlm-data-public-prod/hub/examples/agent_use_cases/template-search/image-13.png\n"
          ]
        }
      ],
      "source": [
        "const TEMPLATE_URL = \"https://storage.googleapis.com/vlm-data-public-prod/hub/examples/agent_use_cases/template-search/image-12.png\";\n",
        "const REFERENCE_URL = \"https://storage.googleapis.com/vlm-data-public-prod/hub/examples/agent_use_cases/template-search/image-13.png\";\n",
        "\n",
        "console.log(\"Template URL:\", TEMPLATE_URL);\n",
        "console.log(\"Reference URL:\", REFERENCE_URL);\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            ">> RESPONSE\n",
            "{\n",
            "  detections: [ { label: \"lemon\", xywh: [ 0, 0, 1, 1 ], confidence: 0.99 } ]\n",
            "}\n",
            "\n",
            ">> Found 1 matches\n",
            "  1. lemon: xywh=[0.000, 0.000, 1.000, 1.000]\n"
          ]
        }
      ],
      "source": [
        "const result = await chatCompletion(\n",
        "    \"Given two images, identify the specified item from the second image within the first image. Clearly highlight and draw bounding boxes around all occurrences of the item in the first image. Provide a brief description of the results.\",\n",
        "    [TEMPLATE_URL, REFERENCE_URL],\n",
        "    DetectionsResponseSchema\n",
        ") as DetectionsResponse;\n",
        "\n",
        "console.log(\">> RESPONSE\");\n",
        "console.log(result);\n",
        "console.log(`\\n>> Found ${result.detections.length} matches`);\n",
        "result.detections.forEach((det, i) => {\n",
        "    console.log(`  ${i + 1}. ${det.label}: xywh=[${det.xywh.map(v => v.toFixed(3)).join(\", \")}]`);\n",
        "});\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 8. UI Parsing\n",
        "\n",
        "Parse user interface elements from screenshots.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            ">> RESPONSE\n",
            "Detected 47 UI elements\n",
            "  1. text: search: xywh=[0.378, 0.110, 0.033, 0.021]\n",
            "  2. icon: Store: xywh=[0.497, 0.231, 0.076, 0.119]\n",
            "  3. icon: Microsoft: xywh=[0.287, 0.227, 0.076, 0.115]\n",
            "  4. icon: Aox: xywh=[0.361, 0.346, 0.067, 0.099]\n",
            "  5. icon: Mole: xywh=[0.637, 0.596, 0.053, 0.042]\n",
            "  6. icon: (12) png: xywh=[0.305, 0.646, 0.198, 0.082]\n",
            "  7. icon: Tonday alls: xywh=[0.305, 0.727, 0.200, 0.092]\n",
            "  8. icon: (II} png: xywh=[0.516, 0.649, 0.181, 0.078]\n",
            "  9. icon: Waiuz: xywh=[0.925, 0.938, 0.068, 0.056]\n",
            "  10. icon: (Blpng: xywh=[0.517, 0.726, 0.178, 0.091]\n",
            "  ... and 37 more\n"
          ]
        }
      ],
      "source": [
        "const IMAGE_URL = \"https://storage.googleapis.com/vlm-data-public-prod/hub/examples/web.ui-automation/win11.jpeg\";\n",
        "\n",
        "const result = await chatCompletion(\n",
        "    \"Parse the UI of this screenshot and detect all the UI elements.\",\n",
        "    [IMAGE_URL],\n",
        "    DetectionsResponseSchema\n",
        ") as DetectionsResponse;\n",
        "\n",
        "console.log(\">> RESPONSE\");\n",
        "console.log(`Detected ${result.detections.length} UI elements`);\n",
        "result.detections.slice(0, 10).forEach((det, i) => {\n",
        "    console.log(`  ${i + 1}. ${det.label}: xywh=[${det.xywh.map(v => v.toFixed(3)).join(\", \")}]`);\n",
        "});\n",
        "if (result.detections.length > 10) {\n",
        "    console.log(`  ... and ${result.detections.length - 10} more`);\n",
        "}\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 9. Streaming Responses\n",
        "\n",
        "For long-running tasks, you can use streaming to get partial results as they become available.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Streaming response:\n",
            "----------------------------------------\n",
            "\n",
            "----------------------------------------\n",
            "\n",
            ">> Full response length: 1118 characters\n"
          ]
        }
      ],
      "source": [
        "const IMAGE_URL = \"https://storage.googleapis.com/vlm-data-public-prod/hub/examples/image.caption/car.jpg\";\n",
        "\n",
        "const stream = await client.agent.completions.create({\n",
        "    model: \"vlmrun-orion-1:auto\",\n",
        "    messages: [{\n",
        "        role: \"user\",\n",
        "        content: [\n",
        "            { type: \"text\", text: \"Describe this image in detail\" },\n",
        "            { type: \"image_url\", image_url: { url: IMAGE_URL } }\n",
        "        ]\n",
        "    }],\n",
        "    stream: true\n",
        "});\n",
        "\n",
        "console.log(\"Streaming response:\");\n",
        "console.log(\"----------------------------------------\");\n",
        "let fullResponse = \"\";\n",
        "for await (const chunk of stream) {\n",
        "    const content = chunk.choices[0]?.delta?.content;\n",
        "    if (content) {\n",
        "        fullResponse += content;\n",
        "        // In a real notebook, you might want to display this incrementally\n",
        "        process.stdout.write(content);\n",
        "    }\n",
        "}\n",
        "console.log(\"\\n----------------------------------------\");\n",
        "console.log(\"\\n>> Full response length:\", fullResponse.length, \"characters\");\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 10. Using Artifacts API for Preview Images\n",
        "\n",
        "When the API returns image URLs (like in image generation, segmentation, or filtering operations), you can use the artifacts API to retrieve the actual image data for preview, saving, or further processing."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 73,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            ">> Generated image URL: https://storage.googleapis.com/vlm-userdata-prod/agents/artifacts/ae8ae740-ddd6-426b-bde1-75540f99f277/7193b85e-de88-40e5-bc7d-063dc6bfa0ea/img_9f6ca0.jpg?X-Goog-Algorithm=GOOG4-RSA-SHA256&X-Goog-Credential=vlm-deployments%40vlm-infra-prod.iam.gserviceaccount.com%2F20251221%2Fauto%2Fstorage%2Fgoog4_request&X-Goog-Date=20251221T123545Z&X-Goog-Expires=604800&X-Goog-SignedHeaders=host&X-Goog-Signature=51df172ae0204d3d7025cc9b075f129c2b33f447c735bb4cd292222c90016183f732b729b67c160e1e6ef6599124261e846f33490fe82a86712935ed64050601d4afd87c7a49458507f4f6c1e17e958e980bba98ae95533a5d64687e7a4529de30d4f10a61e1489d3e1ff16a012eb2149269ee93db8ce144f5f5427c96ef7452e28f1565b64e91046e6807792006f53874ecb58d504bab8ee9a86910a2dd9c162a1c4dbd98694f7c7215c8d3d95b89ac74e0f0900d1bb0ea680ac79c14ad65748c0fc384a931b666c345d3503de7d0582dad8ba6414f2b01172d75e38d324f17b07d4c1374526e121ff286e16d7cc641108c29eb687afd24d29d408d97d9045e\n",
            "\n",
            ">> Retrieving image using artifacts API...\n",
            "✓ Successfully retrieved: 103037 bytes\n",
            "\n",
            ">> The image data is now available for:\n",
            "  - Saving to file\n",
            "  - Displaying in notebook\n",
            "  - Further processing with image libraries\n"
          ]
        }
      ],
      "source": [
        "// Step 1: Generate a processed image\n",
        "const IMAGE_URL = \"https://storage.googleapis.com/vlm-data-public-prod/hub/examples/image.caption/car.jpg\";\n",
        "\n",
        "const result = await chatCompletion(\n",
        "    \"Apply a vintage filter to this image and return the processed image\",\n",
        "    [IMAGE_URL],\n",
        "    ImageUrlResponseSchema\n",
        ") as ImageUrlResponse;\n",
        "\n",
        "console.log(\">> Generated image URL:\", result.url);\n",
        "\n",
        "// Step 2: Retrieve the image using artifacts API\n",
        "console.log(\"\\n>> Retrieving image using artifacts API...\");\n",
        "const imageData = await getArtifactImage(result.url);\n",
        "\n",
        "if (imageData) {\n",
        "    console.log(`✓ Successfully retrieved: ${imageData.length} bytes`);\n",
        "    console.log(\"\\n>> The image data is now available for:\");\n",
        "    console.log(\"  - Saving to file\");\n",
        "    console.log(\"  - Displaying in notebook\");\n",
        "    console.log(\"  - Further processing with image libraries\");\n",
        "} else {\n",
        "    console.log(\"✗ Failed to retrieve artifact\");\n",
        "}\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            ">> Testing Artifacts API with hardcoded values\n",
            "Session ID: f2ab2e1f-f20e-478d-bad6-990449a4a9ee\n",
            "Object ID: img_330e53\n",
            "\n",
            "Trying SDK artifacts API...\n",
            "✓ Successfully retrieved via SDK: 103037 bytes\n",
            "\n",
            "Trying direct fetch: https://agent.vlm.run/v1/artifacts/f2ab2e1f-f20e-478d-bad6-990449a4a9ee/img_330e53\n",
            "✓ Successfully retrieved via direct fetch: 103037 bytes\n",
            "\n",
            ">> The image data is now available for:\n",
            "  - Saving to file\n",
            "  - Displaying in notebook\n",
            "  - Further processing with image libraries\n"
          ]
        }
      ],
      "source": [
        "// Test artifacts API with hardcoded sessionId and objectId\n",
        "// Replace these with your actual values from an artifact URL\n",
        "const SESSION_ID = \"f2ab2e1f-f20e-478d-bad6-990449a4a9ee\";\n",
        "const OBJECT_ID = \"img_330e53\";\n",
        "\n",
        "console.log(\">> Testing Artifacts API with hardcoded values\");\n",
        "console.log(`Session ID: ${SESSION_ID}`);\n",
        "console.log(`Object ID: ${OBJECT_ID}\\n`);\n",
        "\n",
        "try {\n",
        "    // Method 1: Try using SDK artifacts API if available\n",
        "    if (client.artifacts && typeof client.artifacts.get === 'function') {\n",
        "        console.log(\"Trying SDK artifacts API...\");\n",
        "        const artifact = await client.artifacts.get({\n",
        "            sessionId: SESSION_ID,\n",
        "            objectId: OBJECT_ID\n",
        "        });\n",
        "        if (artifact instanceof Buffer) {\n",
        "            const imageData = new Uint8Array(artifact);\n",
        "            console.log(`✓ Successfully retrieved via SDK: ${imageData.length} bytes`);\n",
        "        } else if (artifact instanceof Uint8Array) {\n",
        "            console.log(`✓ Successfully retrieved via SDK: ${artifact.length} bytes`);\n",
        "        } else {\n",
        "            console.log(`✓ Retrieved (type: ${typeof artifact})`);\n",
        "        }\n",
        "    } else {\n",
        "        console.log(\"SDK artifacts API not available, trying direct fetch...\");\n",
        "    }\n",
        "    \n",
        "    // Method 2: Direct fetch to artifacts API endpoint\n",
        "    const baseURL = \"https://agent.vlm.run/v1\";\n",
        "    const artifactsUrl = `${baseURL}/artifacts/${SESSION_ID}/${OBJECT_ID}`;\n",
        "    \n",
        "    console.log(`\\nTrying direct fetch: ${artifactsUrl}`);\n",
        "    const response = await fetch(artifactsUrl, {\n",
        "        headers: {\n",
        "            \"Authorization\": `Bearer ${VLMRUN_API_KEY}`,\n",
        "        },\n",
        "    });\n",
        "    \n",
        "    if (response.ok) {\n",
        "        const arrayBuffer = await response.arrayBuffer();\n",
        "        const imageData = new Uint8Array(arrayBuffer);\n",
        "        console.log(`✓ Successfully retrieved via direct fetch: ${imageData.length} bytes`);\n",
        "        console.log(\"\\n>> The image data is now available for:\");\n",
        "        console.log(\"  - Saving to file\");\n",
        "        console.log(\"  - Displaying in notebook\");\n",
        "        console.log(\"  - Further processing with image libraries\");\n",
        "    } else {\n",
        "        console.error(`✗ Failed: ${response.status} ${response.statusText}`);\n",
        "    }\n",
        "} catch (error) {\n",
        "    console.error(`✗ Error: ${error instanceof Error ? error.message : String(error)}`);\n",
        "}\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "## Conclusion\n",
        "\n",
        "This cookbook demonstrated the comprehensive capabilities of the **VLM Run Orion Image Agent API** using Node.js/TypeScript.\n",
        "\n",
        "### Key Takeaways\n",
        "\n",
        "1. **OpenAI-Compatible Interface**: The API follows the OpenAI chat completions format, making it easy to integrate with existing workflows and tools.\n",
        "2. **Structured Outputs**: Use Zod schemas with `response_format` parameter to get type-safe, validated responses with automatic parsing.\n",
        "3. **Type Safety**: TypeScript and Zod provide compile-time and runtime type checking for better developer experience.\n",
        "4. **Streaming Support**: For long-running tasks, enable streaming to receive partial results as they become available, improving user experience.\n",
        "5. **Flexible Prompting**: Natural language prompts allow you to combine multiple operations in a single request, reducing API calls and latency.\n",
        "6. **Rich Capabilities**: The API supports object detection, segmentation, OCR, image generation, UI parsing, and more.\n",
        "\n",
        "### Next Steps\n",
        "\n",
        "- Explore the [VLM Run Documentation](https://docs.vlm.run) for more details\n",
        "- Join our [Discord community](https://discord.gg/AMApC2UzVY) for support\n",
        "- Check out more examples in the [VLM Run Cookbook](https://github.com/vlm-run/vlmrun-cookbook)\n",
        "- Review the [VLM Run Node.js SDK](https://github.com/vlm-run/vlmrun-node-sdk) documentation\n",
        "\n",
        "Happy building!\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Deno",
      "language": "typescript",
      "name": "deno"
    },
    "language_info": {
      "codemirror_mode": "typescript",
      "file_extension": ".ts",
      "mimetype": "text/x.typescript",
      "name": "typescript",
      "nbconvert_exporter": "script",
      "pygments_lexer": "typescript",
      "version": "5.9.2"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
