{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "<div align=\"center\">\n",
        "<p align=\"center\" style=\"width: 100%;\">\n",
        "    <img src=\"https://raw.githubusercontent.com/vlm-run/.github/refs/heads/main/profile/assets/vlm-black.svg\" alt=\"VLM Run Logo\" width=\"80\" style=\"margin-bottom: -5px; color: #2e3138; vertical-align: middle; padding-right: 5px;\"><br>\n",
        "</p>\n",
        "<p align=\"center\"><a href=\"https://docs.vlm.run\"><b>Website</b></a> | <a href=\"https://docs.vlm.run/\"><b>API Docs</b></a> | <a href=\"https://docs.vlm.run/blog\"><b>Blog</b></a> | <a href=\"https://discord.gg/AMApC2UzVY\"><b>Discord</b></a> | <a href=\"https://chat.vlm.run\"><b>Chat</b></a>\n",
        "</p>\n",
        "</div>\n",
        "\n",
        "# VLM Run Orion - Video Understanding, Reasoning and Execution (Node.js)\n",
        "\n",
        "This comprehensive cookbook demonstrates [VLM Run Orion's](https://vlm.run/orion) video understanding, reasoning and execution capabilities using **Node.js/TypeScript**. For more details on the API, see the [Agent API docs](https://docs.vlm.run/agents/introduction).\n",
        "\n",
        "For this notebook, we'll cover how to use the **VLM Run Agent Chat Completions API** - an OpenAI-compatible interface for building powerful visual intelligence with the same familiar chat-completions interface.\n",
        "\n",
        "We'll cover the following topics:\n",
        " 1. Video uploads (load videos from URLs/files)\n",
        " 2. Video Captioning & Summarization (generate detailed captions, summaries, and chapters)\n",
        " 3. Video Frame Sampling (extract frames at specific timestamps or intervals)\n",
        " 4. Video Trimming (extract specific segments from videos)\n",
        " 5. Video Parsing & Analysis (parse video content, detect scene changes)\n",
        " 6. Video Generation (text-to-video generation)\n",
        " 7. Streaming Responses (for long-running video tasks)\n",
        "\n",
        "## Prerequisites\n",
        "\n",
        "- Node.js 18+\n",
        "- VLM Run API key (get one at [app.vlm.run](https://app.vlm.run))\n",
        "- Deno or tslab kernel for running TypeScript in Jupyter\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Setup\n",
        "\n",
        "First, install the required packages and configure the environment.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "// Install the VLM Run SDK\n",
        "// npm install vlmrun zod zod-to-json-schema\n",
        "\n",
        "// If using Deno kernel, install dependencies via npm specifiers\n",
        "// For tslab, run: npm install vlmrun zod zod-to-json-schema in your project directory\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "vscode": {
          "languageId": "javascript"
        }
      },
      "outputs": [],
      "source": [
        "// Import the VLM Run SDK and dependencies\n",
        "import { VlmRun } from \"npm:vlmrun\";\n",
        "import { z } from \"npm:zod\";\n",
        "import { zodToJsonSchema } from \"npm:zod-to-json-schema\";\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "vscode": {
          "languageId": "javascript"
        }
      },
      "outputs": [],
      "source": [
        "// Get API key from environment variable\n",
        "const VLMRUN_API_KEY = Deno.env.get(\"VLMRUN_API_KEY\");\n",
        "\n",
        "if (!VLMRUN_API_KEY) {\n",
        "    throw new Error(\"Please set the VLMRUN_API_KEY environment variable\");\n",
        "}\n",
        "\n",
        "console.log(\"✓ API Key loaded successfully\");\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Initialize the VLM Run Client\n",
        "\n",
        "We use the OpenAI-compatible chat completions interface through the VLM Run SDK.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "vscode": {
          "languageId": "javascript"
        }
      },
      "outputs": [],
      "source": [
        "// Initialize the VLM Run client using the SDK\n",
        "const client = new VlmRun({\n",
        "    apiKey: VLMRUN_API_KEY,\n",
        "    baseURL: \"https://agent.vlm.run/v1\"  // Use the agent API endpoint\n",
        "});\n",
        "\n",
        "console.log(\"✓ VLM Run client initialized successfully!\");\n",
        "console.log(\"Base URL: https://agent.vlm.run/v1\");\n",
        "console.log(\"Model: vlmrun-orion-1\");\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Response Models (Schemas)\n",
        "\n",
        "We define Zod schemas for structured outputs. These schemas provide type-safe, validated responses.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "vscode": {
          "languageId": "javascript"
        }
      },
      "outputs": [],
      "source": [
        "// Video URL Response Schema\n",
        "const VideoUrlResponseSchema = z.object({\n",
        "    url: z.string().describe(\"Pre-signed URL to the video\")\n",
        "});\n",
        "\n",
        "type VideoUrlResponse = z.infer<typeof VideoUrlResponseSchema>;\n",
        "\n",
        "// Video URL List Response Schema\n",
        "const VideoUrlListResponseSchema = z.object({\n",
        "    urls: z.array(VideoUrlResponseSchema).describe(\"List of pre-signed URLs to the videos\")\n",
        "});\n",
        "\n",
        "type VideoUrlListResponse = z.infer<typeof VideoUrlListResponseSchema>;\n",
        "\n",
        "// Video Chapter Schema\n",
        "const VideoChapterSchema = z.object({\n",
        "    start_time: z.string().describe(\"Start time of the chapter in HH:MM:SS format\"),\n",
        "    end_time: z.string().describe(\"End time of the chapter in HH:MM:SS format\"),\n",
        "    description: z.string().describe(\"Description of the chapter content\")\n",
        "});\n",
        "\n",
        "// Parsed Video Response Schema\n",
        "const ParsedVideoResponseSchema = z.object({\n",
        "    topic: z.string().describe(\"Main topic of the video\"),\n",
        "    summary: z.string().describe(\"Summary of the video content\"),\n",
        "    chapters: z.array(VideoChapterSchema).default([]).describe(\"List of video chapters with timestamps and descriptions\")\n",
        "});\n",
        "\n",
        "type ParsedVideoResponse = z.infer<typeof ParsedVideoResponseSchema>;\n",
        "\n",
        "// Video Frame Schema\n",
        "const VideoFrameSchema = z.object({\n",
        "    url: z.string().describe(\"URL of the video frame\"),\n",
        "    timestamp: z.string().describe(\"Timestamp of the frame in HH:MM:SS.MS format\")\n",
        "});\n",
        "\n",
        "// Video Frames Response Schema\n",
        "const VideoFramesResponseSchema = z.object({\n",
        "    frames: z.array(VideoFrameSchema).describe(\"List of extracted frames\")\n",
        "});\n",
        "\n",
        "type VideoFramesResponse = z.infer<typeof VideoFramesResponseSchema>;\n",
        "\n",
        "// Video Trim Response Schema\n",
        "const VideoTrimResponseSchema = z.object({\n",
        "    url: z.string().describe(\"URL of the trimmed video\"),\n",
        "    start_time: z.string().describe(\"Start time of the trimmed segment\"),\n",
        "    end_time: z.string().describe(\"End time of the trimmed segment\")\n",
        "});\n",
        "\n",
        "type VideoTrimResponse = z.infer<typeof VideoTrimResponseSchema>;\n",
        "\n",
        "// Video Highlight Schema\n",
        "const VideoHighlightSchema = z.object({\n",
        "    start_time: z.string().describe(\"Start time of the highlight in HH:MM:SS.MS format\"),\n",
        "    end_time: z.string().describe(\"End time of the highlight in HH:MM:SS.MS format\"),\n",
        "    url: z.string().describe(\"URL of the extracted highlight video\"),\n",
        "    description: z.string().default(\"\").describe(\"Description of the highlight\")\n",
        "});\n",
        "\n",
        "// Video Highlights Response Schema\n",
        "const VideoHighlightsResponseSchema = z.object({\n",
        "    highlights: z.array(VideoHighlightSchema).describe(\"List of extracted highlights\")\n",
        "});\n",
        "\n",
        "type VideoHighlightsResponse = z.infer<typeof VideoHighlightsResponseSchema>;\n",
        "\n",
        "console.log(\"✓ Response schemas defined successfully!\");\n",
        "console.log(\"Schemas include type-safe validation for structured outputs.\");\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "vscode": {
          "languageId": "javascript"
        }
      },
      "outputs": [],
      "source": [
        "/**\n",
        " * Make a chat completion request with optional videos and structured output.\n",
        " * \n",
        " * @param prompt - The text prompt/instruction\n",
        " * @param videos - Optional list of videos to process (URLs or file paths)\n",
        " * @param images - Optional list of images to process (URLs)\n",
        " * @param responseSchema - Optional Zod schema for structured output\n",
        " * @param model - Model to use (default: vlmrun-orion-1:auto)\n",
        " * @returns Parsed response if responseSchema provided, else raw response text\n",
        " */\n",
        "async function chatCompletion<T>(\n",
        "    prompt: string,\n",
        "    videos?: string[],\n",
        "    images?: string[],\n",
        "    responseSchema?: z.ZodSchema<T>,\n",
        "    model: string = \"vlmrun-orion-1:auto\"\n",
        "): Promise<T | string> {\n",
        "    const content: any[] = [];\n",
        "    content.push({ type: \"text\", text: prompt });\n",
        "\n",
        "    // Add images if provided\n",
        "    if (images) {\n",
        "        for (const image of images) {\n",
        "            if (typeof image === \"string\") {\n",
        "                if (!image.startsWith(\"http\")) {\n",
        "                    throw new Error(\"Image URLs must start with http or https\");\n",
        "                }\n",
        "                content.push({\n",
        "                    type: \"image_url\",\n",
        "                    image_url: { url: image, detail: \"auto\" }\n",
        "                });\n",
        "            }\n",
        "        }\n",
        "    }\n",
        "\n",
        "    // Add videos if provided\n",
        "    if (videos) {\n",
        "        for (const video of videos) {\n",
        "            if (typeof video === \"string\") {\n",
        "                if (video.startsWith(\"http\")) {\n",
        "                    // Video URL\n",
        "                    content.push({\n",
        "                        type: \"video_url\",\n",
        "                        video_url: { url: video }\n",
        "                    });\n",
        "                } else {\n",
        "                    // Local file path - upload first\n",
        "                    const file = await client.files.upload({\n",
        "                        filePath: video,\n",
        "                        purpose: \"assistants\"\n",
        "                    });\n",
        "                    content.push({\n",
        "                        type: \"input_file\",\n",
        "                        file_id: file.id\n",
        "                    });\n",
        "                }\n",
        "            }\n",
        "        }\n",
        "    }\n",
        "\n",
        "    const kwargs: any = {\n",
        "        model: model,\n",
        "        messages: [{ role: \"user\", content: content }]\n",
        "    };\n",
        "\n",
        "    if (responseSchema) {\n",
        "        kwargs.response_format = {\n",
        "            type: \"json_schema\",\n",
        "            schema: zodToJsonSchema(responseSchema)\n",
        "        } as any;\n",
        "    }\n",
        "\n",
        "    const response = await client.agent.completions.create(kwargs);\n",
        "    const responseText = response.choices[0].message.content || \"\";\n",
        "\n",
        "    if (responseSchema) {\n",
        "        const parsed = JSON.parse(responseText);\n",
        "        return responseSchema.parse(parsed) as T;\n",
        "    }\n",
        "\n",
        "    return responseText;\n",
        "}\n",
        "\n",
        "console.log(\"✓ Helper functions defined!\");\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 1. Video Uploads\n",
        "\n",
        "With the VLM Run Agent API, you can either upload videos from URLs or from local files and pass them to chat completions.\n",
        "\n",
        "In the `chatCompletion` helper function above, we use the following to upload videos:\n",
        "\n",
        "```typescript\n",
        "if (videos) {\n",
        "    for (const video of videos) {\n",
        "        if (typeof video === \"string\") {\n",
        "            if (video.startsWith(\"http\")) {\n",
        "                // Video URL\n",
        "                content.push({\n",
        "                    type: \"video_url\",\n",
        "                    video_url: { url: video }\n",
        "                });\n",
        "            } else {\n",
        "                // Local file path - upload first\n",
        "                const file = await client.files.upload({\n",
        "                    filePath: video,\n",
        "                    purpose: \"assistants\"\n",
        "                });\n",
        "                content.push({\n",
        "                    type: \"input_file\",\n",
        "                    file_id: file.id\n",
        "                });\n",
        "            }\n",
        "        }\n",
        "    }\n",
        "}\n",
        "```\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Let's look at a simple video below:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "vscode": {
          "languageId": "javascript"
        }
      },
      "outputs": [],
      "source": [
        "const VIDEO_URL = \"https://storage.googleapis.com/vlm-data-public-prod/hub/examples/video.transcription/bakery.mp4\";\n",
        "\n",
        "console.log(\">> VIDEO URL:\", VIDEO_URL);\n",
        "console.log(\"Note: In a Jupyter notebook, you can display videos using HTML or markdown cells\");\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 2. Video Captioning & Summarization\n",
        "\n",
        "Generate detailed captions, summaries, and chapter breakdowns for videos. The agent analyzes both visual and audio content to provide comprehensive descriptions.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 2a. Simple Video Description\n",
        "\n",
        "Get a quick, natural language description of a video without structured output.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "vscode": {
          "languageId": "javascript"
        }
      },
      "outputs": [],
      "source": [
        "const VIDEO_URL = \"https://storage.googleapis.com/vlm-data-public-prod/hub/examples/video.transcription/bakery.mp4\";\n",
        "\n",
        "const result = await chatCompletion(\n",
        "    \"Describe what happens in this video in 2-3 sentences.\",\n",
        "    [VIDEO_URL]\n",
        ");\n",
        "\n",
        "console.log(\">> RESPONSE\");\n",
        "console.log(result);\n",
        "console.log(\"\\n>> VIDEO URL:\", VIDEO_URL);\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 2b. Structured Video Understanding\n",
        "\n",
        "Parse a video and get a detailed summary with topic, summary, and chapter breakdowns.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "vscode": {
          "languageId": "javascript"
        }
      },
      "outputs": [],
      "source": [
        "const VIDEO_URL = \"https://storage.googleapis.com/vlm-data-public-prod/hub/examples/video.transcription/bakery.mp4\";\n",
        "\n",
        "const result = await chatCompletion(\n",
        "    \"Parse this video and provide a detailed summary with topic, summary, and chapter breakdowns.\",\n",
        "    [VIDEO_URL],\n",
        "    undefined,\n",
        "    ParsedVideoResponseSchema\n",
        ") as ParsedVideoResponse;\n",
        "\n",
        "console.log(\">> RESPONSE\");\n",
        "console.log(JSON.stringify(result, null, 2));\n",
        "\n",
        "let mdStr = \"\";\n",
        "mdStr += `Topic: ${result.topic}\\n`;\n",
        "mdStr += `\\nSummary: ${result.summary}\\n`;\n",
        "mdStr += `\\nChapters (${result.chapters.length} total):\\n`;\n",
        "result.chapters.forEach((chapter, i) => {\n",
        "    mdStr += `  ${String(i + 1).padStart(2, \"0\")}. [${chapter.start_time} - ${chapter.end_time}] ${chapter.description}\\n`;\n",
        "});\n",
        "\n",
        "console.log(\"\\n>> VIDEO URL:\", VIDEO_URL);\n",
        "console.log(\"\\n>> FORMATTED SUMMARY:\");\n",
        "console.log(mdStr);\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 3. Video Frame Sampling\n",
        "\n",
        "Extract frames from videos at specific timestamps or regular intervals. This is useful for thumbnail generation, video analysis, and content indexing.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "vscode": {
          "languageId": "javascript"
        }
      },
      "outputs": [],
      "source": [
        "const VIDEO_URL = \"https://storage.googleapis.com/vlm-data-public-prod/hub/examples/video.transcription/bakery.mp4\";\n",
        "\n",
        "// First, get the video summary to use for frame sampling\n",
        "const summaryResult = await chatCompletion(\n",
        "    \"Parse this video and provide a detailed summary with topic, summary, and chapter breakdowns.\",\n",
        "    [VIDEO_URL],\n",
        "    undefined,\n",
        "    ParsedVideoResponseSchema\n",
        ") as ParsedVideoResponse;\n",
        "\n",
        "// Now sample frames based on chapters\n",
        "const result = await chatCompletion(\n",
        "    `Given the chapter details from the video, sample a frame from every 4 chapters and return the frame URLs with timestamps. Summary: ${JSON.stringify(summaryResult)}`,\n",
        "    [VIDEO_URL],\n",
        "    undefined,\n",
        "    VideoFramesResponseSchema\n",
        ") as VideoFramesResponse;\n",
        "\n",
        "console.log(\">> RESPONSE\");\n",
        "console.log(`Extracted ${result.frames.length} frames:`);\n",
        "result.frames.forEach((frame, i) => {\n",
        "    console.log(`  ${i + 1}. ts=${frame.timestamp}, url: ${frame.url}`);\n",
        "});\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 4. Video Trimming\n",
        "\n",
        "Extract specific segments from videos by specifying start and end times. Perfect for creating clips, highlights, or removing unwanted portions.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "vscode": {
          "languageId": "javascript"
        }
      },
      "outputs": [],
      "source": [
        "const VIDEO_URL = \"https://storage.googleapis.com/vlm-data-public-prod/hub/examples/video.transcription/bakery.mp4\";\n",
        "\n",
        "const result = await chatCompletion(\n",
        "    \"Trim this video from 00:30 to 00:45 seconds and return the trimmed video URL.\",\n",
        "    [VIDEO_URL],\n",
        "    undefined,\n",
        "    VideoTrimResponseSchema\n",
        ") as VideoTrimResponse;\n",
        "\n",
        "console.log(\">> RESPONSE\");\n",
        "console.log(`Trimmed video URL: ${result.url}`);\n",
        "console.log(`Start time: ${result.start_time}`);\n",
        "console.log(`End time: ${result.end_time}`);\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 5. Video Highlight Extraction\n",
        "\n",
        "Automatically identify and extract the most interesting or important moments from a video. The agent analyzes the content to find key scenes.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "vscode": {
          "languageId": "javascript"
        }
      },
      "outputs": [],
      "source": [
        "const VIDEO_URL = \"https://storage.googleapis.com/vlm-data-public-prod/hub/examples/video.transcription/bakery.mp4\";\n",
        "\n",
        "const result = await chatCompletion(\n",
        "    \"Extract the 3 best/most interesting moments from this video as separate clips with timestamps and descriptions.\",\n",
        "    [VIDEO_URL],\n",
        "    undefined,\n",
        "    VideoHighlightsResponseSchema\n",
        ") as VideoHighlightsResponse;\n",
        "\n",
        "console.log(\">> RESPONSE\");\n",
        "console.log(`Extracted ${result.highlights.length} highlights:`);\n",
        "result.highlights.forEach((highlight, i) => {\n",
        "    console.log(`  ${String(i + 1).padStart(2, \"0\")}. [${highlight.start_time} - ${highlight.end_time}] ${highlight.description || \"\"}`);\n",
        "});\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 6. Video Duration & Metadata\n",
        "\n",
        "Get information about video duration and other metadata.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "vscode": {
          "languageId": "javascript"
        }
      },
      "outputs": [],
      "source": [
        "const VIDEO_URL = \"https://storage.googleapis.com/vlm-data-public-prod/hub/examples/video.agent/soccer_ball_juggling.mp4\";\n",
        "\n",
        "const result = await chatCompletion(\n",
        "    \"How long is this video in minutes and seconds? Also describe the video resolution and quality if you can determine it.\",\n",
        "    [VIDEO_URL]\n",
        ");\n",
        "\n",
        "console.log(\">> VIDEO URL:\", VIDEO_URL);\n",
        "console.log(\"\\n>> RESPONSE\");\n",
        "console.log(result);\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 7. Video Generation\n",
        "\n",
        "Generate videos from text descriptions + image inputs. The agent can create short video clips based on your prompts.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "vscode": {
          "languageId": "javascript"
        }
      },
      "outputs": [],
      "source": [
        "const result = await chatCompletion(\n",
        "    \"Generate a powerful paint explosion video effect of this logo in an empty room, spreading it's colors outwards onto the white walls. Return the pre-signed URL to the video.\",\n",
        "    undefined,\n",
        "    [\"https://raw.githubusercontent.com/vlm-run/.github/main/profile/assets/vlm-blue.png\"],\n",
        "    VideoUrlListResponseSchema\n",
        ") as VideoUrlListResponse;\n",
        "\n",
        "console.log(\">> RESPONSE\");\n",
        "console.log(\"Generated video URLs\");\n",
        "console.log(JSON.stringify(result, null, 2));\n",
        "\n",
        "console.log(`\\n>> Generated ${result.urls.length} video(s)`);\n",
        "result.urls.forEach((url, i) => {\n",
        "    console.log(`  Video ${i + 1}: ${url.url}`);\n",
        "});\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 8. Streaming Responses\n",
        "\n",
        "For long-running tasks, you can use streaming to get partial results as they become available.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "vscode": {
          "languageId": "javascript"
        }
      },
      "outputs": [],
      "source": [
        "const VIDEO_URL = \"https://storage.googleapis.com/vlm-data-public-prod/hub/examples/video.transcription/bakery.mp4\";\n",
        "\n",
        "const stream = await client.agent.completions.create({\n",
        "    model: \"vlmrun-orion-1:auto\",\n",
        "    messages: [{\n",
        "        role: \"user\",\n",
        "        content: [\n",
        "            { type: \"text\", text: \"Describe this video in detail\" },\n",
        "            { type: \"video_url\", video_url: { url: VIDEO_URL } }\n",
        "        ]\n",
        "    }],\n",
        "    stream: true\n",
        "});\n",
        "\n",
        "console.log(\"Streaming response:\");\n",
        "console.log(\"----------------------------------------\");\n",
        "let fullResponse = \"\";\n",
        "for await (const chunk of stream) {\n",
        "    const content = chunk.choices[0]?.delta?.content;\n",
        "    if (content) {\n",
        "        fullResponse += content;\n",
        "        // In a real notebook, you might want to display this incrementally\n",
        "        process.stdout.write(content);\n",
        "    }\n",
        "}\n",
        "console.log(\"\\n----------------------------------------\");\n",
        "console.log(\"\\n>> Full response length:\", fullResponse.length, \"characters\");\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "## Conclusion\n",
        "\n",
        "This cookbook demonstrated the comprehensive video understanding capabilities of the **VLM Run Orion Agent API** using Node.js/TypeScript.\n",
        "\n",
        "### Key Takeaways\n",
        "\n",
        "1. **OpenAI-Compatible Interface**: The API follows the OpenAI chat completions format, making it easy to integrate with existing workflows and tools.\n",
        "2. **Structured Outputs**: Use Zod schemas with `response_format` parameter to get type-safe, validated responses with automatic parsing.\n",
        "3. **Type Safety**: TypeScript and Zod provide compile-time and runtime type checking for better developer experience.\n",
        "4. **Video Processing**: Support for video loading, captioning, summarization, frame extraction, trimming, and highlight detection.\n",
        "5. **Video Generation**: Create videos from text descriptions using AI-powered generation.\n",
        "6. **Streaming Support**: For long-running tasks, enable streaming to receive partial results as they become available, improving user experience.\n",
        "7. **Flexible Prompting**: Natural language prompts allow you to combine multiple operations in a single request, reducing API calls and latency.\n",
        "\n",
        "### Video Capabilities Summary\n",
        "\n",
        "| Capability | Description |\n",
        "|------------|-------------|\n",
        "| **Captioning** | Generate detailed captions and summaries with chapter breakdowns |\n",
        "| **Frame Sampling** | Extract frames at specific timestamps or intervals |\n",
        "| **Trimming** | Cut videos to specific time ranges |\n",
        "| **Highlight Extraction** | Automatically identify and extract key moments |\n",
        "| **Video Generation** | Create videos from text descriptions |\n",
        "| **Watermarking (coming soon)** | Add overlays and watermarks to videos |\n",
        "| **YouTube Support (coming soon)** | Load and analyze YouTube videos directly |\n",
        "\n",
        "### Next Steps\n",
        "\n",
        "- Explore the [VLM Run Documentation](https://docs.vlm.run) for more details\n",
        "- Check out the [Video Capabilities Guide](https://docs.vlm.run/agents/capabilities/video) for advanced features\n",
        "- Join our [Discord community](https://discord.gg/AMApC2UzVY) for support\n",
        "- Check out more examples in the [VLM Run Cookbook](https://github.com/vlm-run/vlmrun-cookbook)\n",
        "- Review the [VLM Run Node.js SDK](https://github.com/vlm-run/vlmrun-node-sdk) documentation\n",
        "\n",
        "Happy building!\n"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
