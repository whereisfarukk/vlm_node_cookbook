{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "<div align=\"center\">\n",
        "<p align=\"center\" style=\"width: 100%;\">\n",
        "    <img src=\"https://raw.githubusercontent.com/vlm-run/.github/refs/heads/main/profile/assets/vlm-black.svg\" alt=\"VLM Run Logo\" width=\"80\" style=\"margin-bottom: -5px; color: #2e3138; vertical-align: middle; padding-right: 5px;\"><br>\n",
        "</p>\n",
        "<p align=\"center\"><a href=\"https://docs.vlm.run\"><b>Website</b></a> | <a href=\"https://docs.vlm.run/\"><b>API Docs</b></a> | <a href=\"https://docs.vlm.run/blog\"><b>Blog</b></a> | <a href=\"https://discord.gg/AMApC2UzVY\"><b>Discord</b></a> | <a href=\"https://chat.vlm.run\"><b>Chat</b></a>\n",
        "</p>\n",
        "</div>\n",
        "\n",
        "# VLM Run Orion - Video Understanding, Reasoning and Execution (Node.js)\n",
        "\n",
        "This comprehensive cookbook demonstrates [VLM Run Orion's](https://vlm.run/orion) video understanding, reasoning and execution capabilities using **Node.js/TypeScript**. For more details on the API, see the [Agent API docs](https://docs.vlm.run/agents/introduction).\n",
        "\n",
        "For this notebook, we'll cover how to use the **VLM Run Agent Chat Completions API** - an OpenAI-compatible interface for building powerful visual intelligence with the same familiar chat-completions interface.\n",
        "\n",
        "We'll cover the following topics:\n",
        " 1. Video uploads (load videos from URLs/files)\n",
        " 2. Video Captioning & Summarization (generate detailed captions, summaries, and chapters)\n",
        " 3. Video Frame Sampling (extract frames at specific timestamps or intervals)\n",
        " 4. Video Trimming (extract specific segments from videos)\n",
        " 5. Video Parsing & Analysis (parse video content, detect scene changes)\n",
        " 6. Video Generation (text-to-video generation)\n",
        " 7. Streaming Responses (for long-running video tasks)\n",
        "\n",
        "## Prerequisites\n",
        "\n",
        "- Node.js 18+\n",
        "- VLM Run API key (get one at [app.vlm.run](https://app.vlm.run))\n",
        "- Deno or tslab kernel for running TypeScript in Jupyter\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Setup\n",
        "\n",
        "First, install the required packages and configure the environment.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "// Install the VLM Run SDK\n",
        "// npm install vlmrun zod zod-to-json-schema\n",
        "\n",
        "// If using Deno kernel, install dependencies via npm specifiers\n",
        "// For tslab, run: npm install vlmrun zod zod-to-json-schema in your project directory\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [],
      "source": [
        "// Import the VLM Run SDK and dependencies\n",
        "import { VlmRun } from \"npm:vlmrun\";\n",
        "import { z } from \"npm:zod\";\n",
        "import { zodToJsonSchema } from \"npm:zod-to-json-schema\";\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "✓ API Key loaded successfully\n"
          ]
        }
      ],
      "source": [
        "// Get API key from environment variable\n",
        "const VLMRUN_API_KEY = Deno.env.get(\"VLMRUN_API_KEY\");\n",
        "\n",
        "if (!VLMRUN_API_KEY) {\n",
        "    throw new Error(\"Please set the VLMRUN_API_KEY environment variable\");\n",
        "}\n",
        "\n",
        "console.log(\"✓ API Key loaded successfully\");\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Initialize the VLM Run Client\n",
        "\n",
        "We use the OpenAI-compatible chat completions interface through the VLM Run SDK.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "✓ VLM Run client initialized successfully!\n",
            "Base URL: https://agent.vlm.run/v1\n",
            "Model: vlmrun-orion-1\n"
          ]
        }
      ],
      "source": [
        "// Initialize the VLM Run client using the SDK\n",
        "const client = new VlmRun({\n",
        "    apiKey: VLMRUN_API_KEY,\n",
        "    baseURL: \"https://agent.vlm.run/v1\"  // Use the agent API endpoint\n",
        "});\n",
        "\n",
        "console.log(\"✓ VLM Run client initialized successfully!\");\n",
        "console.log(\"Base URL: https://agent.vlm.run/v1\");\n",
        "console.log(\"Model: vlmrun-orion-1\");\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Response Models (Schemas)\n",
        "\n",
        "We define Zod schemas for structured outputs. These schemas provide type-safe, validated responses.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "✓ Response schemas defined successfully!\n",
            "Schemas include type-safe validation for structured outputs.\n"
          ]
        }
      ],
      "source": [
        "// Video URL Response Schema\n",
        "const VideoUrlResponseSchema = z.object({\n",
        "    url: z.string().describe(\"Pre-signed URL to the video\")\n",
        "});\n",
        "\n",
        "type VideoUrlResponse = z.infer<typeof VideoUrlResponseSchema>;\n",
        "\n",
        "// Video URL List Response Schema\n",
        "const VideoUrlListResponseSchema = z.object({\n",
        "    urls: z.array(VideoUrlResponseSchema).describe(\"List of pre-signed URLs to the videos\")\n",
        "});\n",
        "\n",
        "type VideoUrlListResponse = z.infer<typeof VideoUrlListResponseSchema>;\n",
        "\n",
        "// Video Chapter Schema\n",
        "const VideoChapterSchema = z.object({\n",
        "    start_time: z.string().describe(\"Start time of the chapter in HH:MM:SS format\"),\n",
        "    end_time: z.string().describe(\"End time of the chapter in HH:MM:SS format\"),\n",
        "    description: z.string().describe(\"Description of the chapter content\")\n",
        "});\n",
        "\n",
        "// Parsed Video Response Schema\n",
        "const ParsedVideoResponseSchema = z.object({\n",
        "    topic: z.string().describe(\"Main topic of the video\"),\n",
        "    summary: z.string().describe(\"Summary of the video content\"),\n",
        "    chapters: z.array(VideoChapterSchema).default([]).describe(\"List of video chapters with timestamps and descriptions\")\n",
        "});\n",
        "\n",
        "type ParsedVideoResponse = z.infer<typeof ParsedVideoResponseSchema>;\n",
        "\n",
        "// Video Frame Schema\n",
        "const VideoFrameSchema = z.object({\n",
        "    url: z.string().describe(\"URL of the video frame\"),\n",
        "    timestamp: z.string().describe(\"Timestamp of the frame in HH:MM:SS.MS format\")\n",
        "});\n",
        "\n",
        "// Video Frames Response Schema\n",
        "const VideoFramesResponseSchema = z.object({\n",
        "    frames: z.array(VideoFrameSchema).describe(\"List of extracted frames\")\n",
        "});\n",
        "\n",
        "type VideoFramesResponse = z.infer<typeof VideoFramesResponseSchema>;\n",
        "\n",
        "// Video Trim Response Schema\n",
        "const VideoTrimResponseSchema = z.object({\n",
        "    url: z.string().describe(\"URL of the trimmed video\"),\n",
        "    start_time: z.string().describe(\"Start time of the trimmed segment\"),\n",
        "    end_time: z.string().describe(\"End time of the trimmed segment\")\n",
        "});\n",
        "\n",
        "type VideoTrimResponse = z.infer<typeof VideoTrimResponseSchema>;\n",
        "\n",
        "// Video Highlight Schema\n",
        "const VideoHighlightSchema = z.object({\n",
        "    start_time: z.string().describe(\"Start time of the highlight in HH:MM:SS.MS format\"),\n",
        "    end_time: z.string().describe(\"End time of the highlight in HH:MM:SS.MS format\"),\n",
        "    url: z.string().describe(\"URL of the extracted highlight video\"),\n",
        "    description: z.string().default(\"\").describe(\"Description of the highlight\")\n",
        "});\n",
        "\n",
        "// Video Highlights Response Schema\n",
        "const VideoHighlightsResponseSchema = z.object({\n",
        "    highlights: z.array(VideoHighlightSchema).describe(\"List of extracted highlights\")\n",
        "});\n",
        "\n",
        "type VideoHighlightsResponse = z.infer<typeof VideoHighlightsResponseSchema>;\n",
        "\n",
        "console.log(\"✓ Response schemas defined successfully!\");\n",
        "console.log(\"Schemas include type-safe validation for structured outputs.\");\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "✓ Helper functions defined!\n"
          ]
        }
      ],
      "source": [
        "/**\n",
        " * Make a chat completion request with optional videos and structured output.\n",
        " * \n",
        " * @param prompt - The text prompt/instruction\n",
        " * @param videos - Optional list of videos to process (URLs or file paths)\n",
        " * @param images - Optional list of images to process (URLs)\n",
        " * @param responseSchema - Optional Zod schema for structured output\n",
        " * @param model - Model to use (default: vlmrun-orion-1:auto)\n",
        " * @returns Parsed response if responseSchema provided, else raw response text\n",
        " */\n",
        "async function chatCompletion<T>(\n",
        "    prompt: string,\n",
        "    videos?: string[],\n",
        "    images?: string[],\n",
        "    responseSchema?: z.ZodSchema<T>,\n",
        "    model: string = \"vlmrun-orion-1:auto\"\n",
        "): Promise<T | string> {\n",
        "    const content: any[] = [];\n",
        "    content.push({ type: \"text\", text: prompt });\n",
        "\n",
        "    // Add images if provided\n",
        "    if (images) {\n",
        "        for (const image of images) {\n",
        "            if (typeof image === \"string\") {\n",
        "                if (!image.startsWith(\"http\")) {\n",
        "                    throw new Error(\"Image URLs must start with http or https\");\n",
        "                }\n",
        "                content.push({\n",
        "                    type: \"image_url\",\n",
        "                    image_url: { url: image, detail: \"auto\" }\n",
        "                });\n",
        "            }\n",
        "        }\n",
        "    }\n",
        "\n",
        "    // Add videos if provided\n",
        "    if (videos) {\n",
        "        for (const video of videos) {\n",
        "            if (typeof video === \"string\") {\n",
        "                if (video.startsWith(\"http\")) {\n",
        "                    // Video URL\n",
        "                    content.push({\n",
        "                        type: \"video_url\",\n",
        "                        video_url: { url: video }\n",
        "                    });\n",
        "                } else {\n",
        "                    // Local file path - upload first\n",
        "                    const file = await client.files.upload({\n",
        "                        filePath: video,\n",
        "                        purpose: \"assistants\"\n",
        "                    });\n",
        "                    content.push({\n",
        "                        type: \"input_file\",\n",
        "                        file_id: file.id\n",
        "                    });\n",
        "                }\n",
        "            }\n",
        "        }\n",
        "    }\n",
        "\n",
        "    const kwargs: any = {\n",
        "        model: model,\n",
        "        messages: [{ role: \"user\", content: content }]\n",
        "    };\n",
        "\n",
        "    if (responseSchema) {\n",
        "        kwargs.response_format = {\n",
        "            type: \"json_schema\",\n",
        "            schema: zodToJsonSchema(responseSchema)\n",
        "        } as any;\n",
        "    }\n",
        "\n",
        "    const response = await client.agent.completions.create(kwargs);\n",
        "    const responseText = response.choices[0].message.content || \"\";\n",
        "\n",
        "    if (responseSchema) {\n",
        "        const parsed = JSON.parse(responseText);\n",
        "        return responseSchema.parse(parsed) as T;\n",
        "    }\n",
        "\n",
        "    return responseText;\n",
        "}\n",
        "\n",
        "console.log(\"✓ Helper functions defined!\");\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 1. Video Uploads\n",
        "\n",
        "With the VLM Run Agent API, you can either upload videos from URLs or from local files and pass them to chat completions.\n",
        "\n",
        "In the `chatCompletion` helper function above, we use the following to upload videos:\n",
        "\n",
        "```typescript\n",
        "if (videos) {\n",
        "    for (const video of videos) {\n",
        "        if (typeof video === \"string\") {\n",
        "            if (video.startsWith(\"http\")) {\n",
        "                // Video URL\n",
        "                content.push({\n",
        "                    type: \"video_url\",\n",
        "                    video_url: { url: video }\n",
        "                });\n",
        "            } else {\n",
        "                // Local file path - upload first\n",
        "                const file = await client.files.upload({\n",
        "                    filePath: video,\n",
        "                    purpose: \"assistants\"\n",
        "                });\n",
        "                content.push({\n",
        "                    type: \"input_file\",\n",
        "                    file_id: file.id\n",
        "                });\n",
        "            }\n",
        "        }\n",
        "    }\n",
        "}\n",
        "```\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Let's look at a simple video below:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            ">> VIDEO URL: https://storage.googleapis.com/vlm-data-public-prod/hub/examples/video.transcription/bakery.mp4\n",
            "Note: In a Jupyter notebook, you can display videos using HTML or markdown cells\n"
          ]
        }
      ],
      "source": [
        "const VIDEO_URL = \"https://storage.googleapis.com/vlm-data-public-prod/hub/examples/video.transcription/bakery.mp4\";\n",
        "\n",
        "console.log(\">> VIDEO URL:\", VIDEO_URL);\n",
        "console.log(\"Note: In a Jupyter notebook, you can display videos using HTML or markdown cells\");\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 2. Video Captioning & Summarization\n",
        "\n",
        "Generate detailed captions, summaries, and chapter breakdowns for videos. The agent analyzes both visual and audio content to provide comprehensive descriptions.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 2a. Simple Video Description\n",
        "\n",
        "Get a quick, natural language description of a video without structured output.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            ">> RESPONSE\n",
            "The video describes the history and operations of a multi-generational family bakery, showcasing its community roots and challenges faced over time, including a fire and economic hardships. It culminates in the demolition of the bakery building, reflecting on its legacy and connection to the town of McKees Rocks.\n",
            "\n",
            ">> VIDEO URL: https://storage.googleapis.com/vlm-data-public-prod/hub/examples/video.transcription/bakery.mp4\n"
          ]
        }
      ],
      "source": [
        "const VIDEO_URL = \"https://storage.googleapis.com/vlm-data-public-prod/hub/examples/video.transcription/bakery.mp4\";\n",
        "\n",
        "const result = await chatCompletion(\n",
        "    \"Describe what happens in this video in 2-3 sentences.\",\n",
        "    [VIDEO_URL]\n",
        ");\n",
        "\n",
        "console.log(\">> RESPONSE\");\n",
        "console.log(result);\n",
        "console.log(\"\\n>> VIDEO URL:\", VIDEO_URL);\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 2b. Structured Video Understanding\n",
        "\n",
        "Parse a video and get a detailed summary with topic, summary, and chapter breakdowns.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            ">> RESPONSE\n",
            "{\n",
            "  \"topic\": \"Jenny Lee Bakery: A Family Legacy\",\n",
            "  \"summary\": \"This video chronicles the history of the Jenny Lee Bakery, a multi-generational family business in McKees Rocks, Pennsylvania. It explores the bakery's origins, its deep roots in the community, the challenges it faced including a devastating fire and economic downturn, and the eventual transition and legacy of the family's involvement in baking.\",\n",
            "  \"chapters\": [\n",
            "    {\n",
            "      \"start_time\": \"00:00\",\n",
            "      \"end_time\": \"00:18\",\n",
            "      \"description\": \"The video opens with a scenic view of McKees Rocks, Pennsylvania, featuring a prominent bridge. Scott Baker introduces himself and discusses his family's long-standing connection to the community, dating back to 1941 when his grandfather established \\\"Jenny Lee Bakery.\\\" Archival footage and images showcase the bakery's early days and its delivery operations.\"\n",
            "    },\n",
            "    {\n",
            "      \"start_time\": \"00:19\",\n",
            "      \"end_time\": \"00:32\",\n",
            "      \"description\": \"This segment features interviews with employees, such as Donna, who share personal memories of the bakery, including traditions like visiting after church and specific products like wedding cakes. The footage shows the interior of the bakery, highlighting customers and the variety of baked goods.\"\n",
            "    },\n",
            "    {\n",
            "      \"start_time\": \"00:32\",\n",
            "      \"end_time\": \"00:48\",\n",
            "      \"description\": \"Bernie Baker (likely Scott's father) speaks about his career as a baker and his early involvement in the family business, mentioning his father taking over from the grandfather. Childhood photos of family members, some dressed in baking attire, are shown, illustrating the generational aspect of the business.\"\n",
            "    },\n",
            "    {\n",
            "      \"start_time\": \"00:49\",\n",
            "      \"end_time\": \"01:03\",\n",
            "      \"description\": \"A visual interlude features a train passing under the large blue bridge seen in the opening shot. Scott Baker briefly references the bridge and community events, providing a sense of place and time.\"\n",
            "    },\n",
            "    {\n",
            "      \"start_time\": \"01:03\",\n",
            "      \"end_time\": \"01:36\",\n",
            "      \"description\": \"The narrative shifts to the significant challenges faced by the bakery. Donna recounts a personal anecdote about the bakery's importance. The segment details a major fire that caused extensive damage to the bakery complex, followed by the impact of an economic recession. Scott Baker shares his emotional experience of witnessing the aftermath and the despair it brought. Archival footage depicts the damage caused by the fire.\"\n",
            "    },\n",
            "    {\n",
            "      \"start_time\": \"01:37\",\n",
            "      \"end_time\": \"01:57\",\n",
            "      \"description\": \"Footage shows the demolition of the former Jenny Lee Bakery building, marking a physical end to the establishment. Bernie Baker reflects on his family's deep connection to baking and the community, sharing historical photographs of horse-drawn bakery wagons and earlier generations of the family involved in the business.\"\n",
            "    },\n",
            "    {\n",
            "      \"start_time\": \"01:57\",\n",
            "      \"end_time\": \"02:22\",\n",
            "      \"description\": \"Archival street scenes of McKees Rocks are shown, including a visible \\\"Jenny Lee Bakery\\\" sign. The video then presents data on retail bakery market trends, including a Google search for \\\"retail bakery growth projections\\\" and charts indicating declining consumer spending on bakery products. Scott Baker concludes by explaining the realization that the family needed to adapt by focusing on their own baking and selling directly to stores, reflecting on the evolving landscape of the business.\"\n",
            "    }\n",
            "  ]\n",
            "}\n",
            "\n",
            ">> VIDEO URL: https://storage.googleapis.com/vlm-data-public-prod/hub/examples/video.transcription/bakery.mp4\n",
            "\n",
            ">> FORMATTED SUMMARY:\n",
            "Topic: Jenny Lee Bakery: A Family Legacy\n",
            "\n",
            "Summary: This video chronicles the history of the Jenny Lee Bakery, a multi-generational family business in McKees Rocks, Pennsylvania. It explores the bakery's origins, its deep roots in the community, the challenges it faced including a devastating fire and economic downturn, and the eventual transition and legacy of the family's involvement in baking.\n",
            "\n",
            "Chapters (7 total):\n",
            "  01. [00:00 - 00:18] The video opens with a scenic view of McKees Rocks, Pennsylvania, featuring a prominent bridge. Scott Baker introduces himself and discusses his family's long-standing connection to the community, dating back to 1941 when his grandfather established \"Jenny Lee Bakery.\" Archival footage and images showcase the bakery's early days and its delivery operations.\n",
            "  02. [00:19 - 00:32] This segment features interviews with employees, such as Donna, who share personal memories of the bakery, including traditions like visiting after church and specific products like wedding cakes. The footage shows the interior of the bakery, highlighting customers and the variety of baked goods.\n",
            "  03. [00:32 - 00:48] Bernie Baker (likely Scott's father) speaks about his career as a baker and his early involvement in the family business, mentioning his father taking over from the grandfather. Childhood photos of family members, some dressed in baking attire, are shown, illustrating the generational aspect of the business.\n",
            "  04. [00:49 - 01:03] A visual interlude features a train passing under the large blue bridge seen in the opening shot. Scott Baker briefly references the bridge and community events, providing a sense of place and time.\n",
            "  05. [01:03 - 01:36] The narrative shifts to the significant challenges faced by the bakery. Donna recounts a personal anecdote about the bakery's importance. The segment details a major fire that caused extensive damage to the bakery complex, followed by the impact of an economic recession. Scott Baker shares his emotional experience of witnessing the aftermath and the despair it brought. Archival footage depicts the damage caused by the fire.\n",
            "  06. [01:37 - 01:57] Footage shows the demolition of the former Jenny Lee Bakery building, marking a physical end to the establishment. Bernie Baker reflects on his family's deep connection to baking and the community, sharing historical photographs of horse-drawn bakery wagons and earlier generations of the family involved in the business.\n",
            "  07. [01:57 - 02:22] Archival street scenes of McKees Rocks are shown, including a visible \"Jenny Lee Bakery\" sign. The video then presents data on retail bakery market trends, including a Google search for \"retail bakery growth projections\" and charts indicating declining consumer spending on bakery products. Scott Baker concludes by explaining the realization that the family needed to adapt by focusing on their own baking and selling directly to stores, reflecting on the evolving landscape of the business.\n",
            "\n"
          ]
        }
      ],
      "source": [
        "const VIDEO_URL = \"https://storage.googleapis.com/vlm-data-public-prod/hub/examples/video.transcription/bakery.mp4\";\n",
        "\n",
        "const result = await chatCompletion(\n",
        "    \"Parse this video and provide a detailed summary with topic, summary, and chapter breakdowns.\",\n",
        "    [VIDEO_URL],\n",
        "    undefined,\n",
        "    ParsedVideoResponseSchema\n",
        ") as ParsedVideoResponse;\n",
        "\n",
        "console.log(\">> RESPONSE\");\n",
        "console.log(JSON.stringify(result, null, 2));\n",
        "\n",
        "let mdStr = \"\";\n",
        "mdStr += `Topic: ${result.topic}\\n`;\n",
        "mdStr += `\\nSummary: ${result.summary}\\n`;\n",
        "mdStr += `\\nChapters (${result.chapters.length} total):\\n`;\n",
        "result.chapters.forEach((chapter, i) => {\n",
        "    mdStr += `  ${String(i + 1).padStart(2, \"0\")}. [${chapter.start_time} - ${chapter.end_time}] ${chapter.description}\\n`;\n",
        "});\n",
        "\n",
        "console.log(\"\\n>> VIDEO URL:\", VIDEO_URL);\n",
        "console.log(\"\\n>> FORMATTED SUMMARY:\");\n",
        "console.log(mdStr);\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 3. Video Frame Sampling\n",
        "\n",
        "Extract frames from videos at specific timestamps or regular intervals. This is useful for thumbnail generation, video analysis, and content indexing.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            ">> RESPONSE\n",
            "Extracted 2 frames:\n",
            "  1. ts=00:00, url: https://storage.googleapis.com/vlm-userdata-prod/agents/artifacts/ae8ae740-ddd6-426b-bde1-75540f99f277/8c7d5255-1717-4ae1-b45b-0d1888062bac/img_c21a90.jpg?X-Goog-Algorithm=GOOG4-RSA-SHA256&X-Goog-Credential=vlm-deployments%40vlm-infra-prod.iam.gserviceaccount.com%2F20251221%2Fauto%2Fstorage%2Fgoog4_request&X-Goog-Date=20251221T105559Z&X-Goog-Expires=604800&X-Goog-SignedHeaders=host&X-Goog-Signature=58a5772702dae6736ec3ab3a9dc4732e916204d861cc89ae2280fd84d47c92db919aab9420e3592c254b6c7f3cf90523d2df3c19d5815388575b2bd0a7542942866fad7db690007bde987d29459c751f1b3806e5e7460d642d240141d75ebe6b9d65cc5c84dd83bdd991b5901223140c4e68efb57ee50dabe531aa483298efa1a86677031966d2de2d8d71135c3042f39f5573adcfb5c078a294238ae2aa84b484787d1def57c5353026d076b9dad429d3ea976346695b036ceae22f7fd753cab0dae470ef8c5ea03df6aa873b8c7818b2a79cf67e65c3158b646e3829b90c44ca08e04451267e223749b56f410c2707564bf23c8609e4711fec03071f8c9478\n",
            "  2. ts=01:43, url: https://storage.googleapis.com/vlm-userdata-prod/agents/artifacts/ae8ae740-ddd6-426b-bde1-75540f99f277/8c7d5255-1717-4ae1-b45b-0d1888062bac/img_f31521.jpg?X-Goog-Algorithm=GOOG4-RSA-SHA256&X-Goog-Credential=vlm-deployments%40vlm-infra-prod.iam.gserviceaccount.com%2F20251221%2Fauto%2Fstorage%2Fgoog4_request&X-Goog-Date=20251221T105604Z&X-Goog-Expires=604800&X-Goog-SignedHeaders=host&X-Goog-Signature=1dea9ebb84a4843ec326a3d1d5b8990f958d7bd09341b168958348029c96099608886c22a46ce3842c9e07ea2e7dd674e5e6eb7b4d0f025f4510da92736b54f68e7e5607c54c649721eafdcb830e71bf2007d5e76e50283d48152dd90943bfbe7bb4fee9b689d099903f7fc88dc3ef61205f812ce37c8172db945270a785266284e1e03d54abbfc1bea1289ef1e0913c889ecd78bce78f81a719eed353f47838b7da03f67f22b684229eac6a5eb71cbfe3b9749dbef7785fa64444ba2c6d94f1da8a3339b652b698b87dde3a5161f7e921e6469f883ec17d6bdde1be9538306b66f09dccfc3ca8f549c26c36197f0bb14cb20e04d42c69238d39d2325f31633e\n"
          ]
        }
      ],
      "source": [
        "const VIDEO_URL = \"https://storage.googleapis.com/vlm-data-public-prod/hub/examples/video.transcription/bakery.mp4\";\n",
        "\n",
        "// First, get the video summary to use for frame sampling\n",
        "const summaryResult = await chatCompletion(\n",
        "    \"Parse this video and provide a detailed summary with topic, summary, and chapter breakdowns.\",\n",
        "    [VIDEO_URL],\n",
        "    undefined,\n",
        "    ParsedVideoResponseSchema\n",
        ") as ParsedVideoResponse;\n",
        "\n",
        "// Now sample frames based on chapters\n",
        "const result = await chatCompletion(\n",
        "    `Given the chapter details from the video, sample a frame from every 4 chapters and return the frame URLs with timestamps. Summary: ${JSON.stringify(summaryResult)}`,\n",
        "    [VIDEO_URL],\n",
        "    undefined,\n",
        "    VideoFramesResponseSchema\n",
        ") as VideoFramesResponse;\n",
        "\n",
        "console.log(\">> RESPONSE\");\n",
        "console.log(`Extracted ${result.frames.length} frames:`);\n",
        "result.frames.forEach((frame, i) => {\n",
        "    console.log(`  ${i + 1}. ts=${frame.timestamp}, url: ${frame.url}`);\n",
        "});\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 4. Video Trimming\n",
        "\n",
        "Extract specific segments from videos by specifying start and end times. Perfect for creating clips, highlights, or removing unwanted portions.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            ">> RESPONSE\n",
            "Trimmed video URL: https://storage.googleapis.com/vlm-userdata-prod/agents/artifacts/ae8ae740-ddd6-426b-bde1-75540f99f277/83297ff4-6f27-4bea-9ef8-6a44d3953eff/vid_64d067.mp4?X-Goog-Algorithm=GOOG4-RSA-SHA256&X-Goog-Credential=vlm-deployments%40vlm-infra-prod.iam.gserviceaccount.com%2F20251221%2Fauto%2Fstorage%2Fgoog4_request&X-Goog-Date=20251221T105640Z&X-Goog-Expires=604800&X-Goog-SignedHeaders=host&X-Goog-Signature=87ea5c94e69349312921d60cb726a793156b84ae1d12398b12459e5b1f5a92007ebe94034743147595c60452e8e2172a2025b135b2d43d77a06c7f8620598129c037b2b2f33265ec1e4d67f4ef5fd388187473a0d33a196a85adb4de3340b2e8bcdf5d22ad6c6060157e065ffcc177e6277e2f3824aaebf6a15c0cd78e76d36f51a08fe73924e2fd68d3401cae63b5b680c3bc66e06e9b212bad3965528c9654c5ca4a14bd1c70a62fe6aaee8f0f758fea2352e70fe42d1a5954de8ca68359c62d14f8508221fb019dc0a3d15709923df98e74ec0a92230c35a9c9b641ecd4ba1cc9c00e20cbbd4f4fe9f26b554befb180b579a291ecf5955293196fdf2d55b4\n",
            "Start time: 00:30\n",
            "End time: 00:45\n"
          ]
        }
      ],
      "source": [
        "const VIDEO_URL = \"https://storage.googleapis.com/vlm-data-public-prod/hub/examples/video.transcription/bakery.mp4\";\n",
        "\n",
        "const result = await chatCompletion(\n",
        "    \"Trim this video from 00:30 to 00:45 seconds and return the trimmed video URL.\",\n",
        "    [VIDEO_URL],\n",
        "    undefined,\n",
        "    VideoTrimResponseSchema\n",
        ") as VideoTrimResponse;\n",
        "\n",
        "console.log(\">> RESPONSE\");\n",
        "console.log(`Trimmed video URL: ${result.url}`);\n",
        "console.log(`Start time: ${result.start_time}`);\n",
        "console.log(`End time: ${result.end_time}`);\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 5. Video Highlight Extraction\n",
        "\n",
        "Automatically identify and extract the most interesting or important moments from a video. The agent analyzes the content to find key scenes.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            ">> RESPONSE\n",
            "Extracted 3 highlights:\n",
            "  01. [00:49 - 00:59] A train with an orange locomotive speeds along railroad tracks, passing beneath a large, striking blue arched bridge that spans the landscape.\n",
            "  02. [01:11 - 01:23] A newspaper headline announces \"Fire damages bakery complex,\" followed by black and white footage showing the aftermath of a fire at a building, with smoke, a ladder, and firefighters present.\n",
            "  03. [01:37 - 01:42] Heavy construction equipment, including an excavator and a front-loader, is shown actively demolishing a brick building, creating a dramatic scene of destruction with dust and falling debris.\n"
          ]
        }
      ],
      "source": [
        "const VIDEO_URL = \"https://storage.googleapis.com/vlm-data-public-prod/hub/examples/video.transcription/bakery.mp4\";\n",
        "\n",
        "const result = await chatCompletion(\n",
        "    \"Extract the 3 best/most interesting moments from this video as separate clips with timestamps and descriptions.\",\n",
        "    [VIDEO_URL],\n",
        "    undefined,\n",
        "    VideoHighlightsResponseSchema\n",
        ") as VideoHighlightsResponse;\n",
        "\n",
        "console.log(\">> RESPONSE\");\n",
        "console.log(`Extracted ${result.highlights.length} highlights:`);\n",
        "result.highlights.forEach((highlight, i) => {\n",
        "    console.log(`  ${String(i + 1).padStart(2, \"0\")}. [${highlight.start_time} - ${highlight.end_time}] ${highlight.description || \"\"}`);\n",
        "});\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 6. Video Duration & Metadata\n",
        "\n",
        "Get information about video duration and other metadata.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            ">> VIDEO URL: https://storage.googleapis.com/vlm-data-public-prod/hub/examples/video.agent/soccer_ball_juggling.mp4\n",
            "\n",
            ">> RESPONSE\n",
            "The video is 0 minutes and 19 seconds long. It has a resolution of 1920x1080 (Full HD) and is of high quality.\n"
          ]
        }
      ],
      "source": [
        "const VIDEO_URL = \"https://storage.googleapis.com/vlm-data-public-prod/hub/examples/video.agent/soccer_ball_juggling.mp4\";\n",
        "\n",
        "const result = await chatCompletion(\n",
        "    \"How long is this video in minutes and seconds? Also describe the video resolution and quality if you can determine it.\",\n",
        "    [VIDEO_URL]\n",
        ");\n",
        "\n",
        "console.log(\">> VIDEO URL:\", VIDEO_URL);\n",
        "console.log(\"\\n>> RESPONSE\");\n",
        "console.log(result);\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 7. Video Generation\n",
        "\n",
        "Generate videos from text descriptions + image inputs. The agent can create short video clips based on your prompts.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            ">> RESPONSE\n",
            "Generated video URLs\n",
            "{\n",
            "  \"urls\": [\n",
            "    {\n",
            "      \"url\": \"url_673a43\"\n",
            "    }\n",
            "  ]\n",
            "}\n",
            "\n",
            ">> Generated 1 video(s)\n",
            "  Video 1: url_673a43\n"
          ]
        }
      ],
      "source": [
        "const result = await chatCompletion(\n",
        "    \"Generate a powerful paint explosion video effect of this logo in an empty room, spreading it's colors outwards onto the white walls. Return the pre-signed URL to the video.\",\n",
        "    undefined,\n",
        "    [\"https://raw.githubusercontent.com/vlm-run/.github/main/profile/assets/vlm-blue.png\"],\n",
        "    VideoUrlListResponseSchema\n",
        ") as VideoUrlListResponse;\n",
        "\n",
        "console.log(\">> RESPONSE\");\n",
        "console.log(\"Generated video URLs\");\n",
        "console.log(JSON.stringify(result, null, 2));\n",
        "\n",
        "console.log(`\\n>> Generated ${result.urls.length} video(s)`);\n",
        "result.urls.forEach((url, i) => {\n",
        "    console.log(`  Video ${i + 1}: ${url.url}`);\n",
        "});\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 8. Streaming Responses\n",
        "\n",
        "For long-running tasks, you can use streaming to get partial results as they become available.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Streaming response:\n",
            "----------------------------------------\n",
            "\n",
            "----------------------------------------\n",
            "\n",
            ">> Full response length: 4573 characters\n"
          ]
        }
      ],
      "source": [
        "const VIDEO_URL = \"https://storage.googleapis.com/vlm-data-public-prod/hub/examples/video.transcription/bakery.mp4\";\n",
        "\n",
        "const stream = await client.agent.completions.create({\n",
        "    model: \"vlmrun-orion-1:auto\",\n",
        "    messages: [{\n",
        "        role: \"user\",\n",
        "        content: [\n",
        "            { type: \"text\", text: \"Describe this video in detail\" },\n",
        "            { type: \"video_url\", video_url: { url: VIDEO_URL } }\n",
        "        ]\n",
        "    }],\n",
        "    stream: true\n",
        "});\n",
        "\n",
        "console.log(\"Streaming response:\");\n",
        "console.log(\"----------------------------------------\");\n",
        "let fullResponse = \"\";\n",
        "for await (const chunk of stream) {\n",
        "    const content = chunk.choices[0]?.delta?.content;\n",
        "    if (content) {\n",
        "        fullResponse += content;\n",
        "        // In a real notebook, you might want to display this incrementally\n",
        "        process.stdout.write(content);\n",
        "    }\n",
        "}\n",
        "console.log(\"\\n----------------------------------------\");\n",
        "console.log(\"\\n>> Full response length:\", fullResponse.length, \"characters\");\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "## Conclusion\n",
        "\n",
        "This cookbook demonstrated the comprehensive video understanding capabilities of the **VLM Run Orion Agent API** using Node.js/TypeScript.\n",
        "\n",
        "### Key Takeaways\n",
        "\n",
        "1. **OpenAI-Compatible Interface**: The API follows the OpenAI chat completions format, making it easy to integrate with existing workflows and tools.\n",
        "2. **Structured Outputs**: Use Zod schemas with `response_format` parameter to get type-safe, validated responses with automatic parsing.\n",
        "3. **Type Safety**: TypeScript and Zod provide compile-time and runtime type checking for better developer experience.\n",
        "4. **Video Processing**: Support for video loading, captioning, summarization, frame extraction, trimming, and highlight detection.\n",
        "5. **Video Generation**: Create videos from text descriptions using AI-powered generation.\n",
        "6. **Streaming Support**: For long-running tasks, enable streaming to receive partial results as they become available, improving user experience.\n",
        "7. **Flexible Prompting**: Natural language prompts allow you to combine multiple operations in a single request, reducing API calls and latency.\n",
        "\n",
        "### Video Capabilities Summary\n",
        "\n",
        "| Capability | Description |\n",
        "|------------|-------------|\n",
        "| **Captioning** | Generate detailed captions and summaries with chapter breakdowns |\n",
        "| **Frame Sampling** | Extract frames at specific timestamps or intervals |\n",
        "| **Trimming** | Cut videos to specific time ranges |\n",
        "| **Highlight Extraction** | Automatically identify and extract key moments |\n",
        "| **Video Generation** | Create videos from text descriptions |\n",
        "| **Watermarking (coming soon)** | Add overlays and watermarks to videos |\n",
        "| **YouTube Support (coming soon)** | Load and analyze YouTube videos directly |\n",
        "\n",
        "### Next Steps\n",
        "\n",
        "- Explore the [VLM Run Documentation](https://docs.vlm.run) for more details\n",
        "- Check out the [Video Capabilities Guide](https://docs.vlm.run/agents/capabilities/video) for advanced features\n",
        "- Join our [Discord community](https://discord.gg/AMApC2UzVY) for support\n",
        "- Check out more examples in the [VLM Run Cookbook](https://github.com/vlm-run/vlmrun-cookbook)\n",
        "- Review the [VLM Run Node.js SDK](https://github.com/vlm-run/vlmrun-node-sdk) documentation\n",
        "\n",
        "Happy building!\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Deno",
      "language": "typescript",
      "name": "deno"
    },
    "language_info": {
      "codemirror_mode": "typescript",
      "file_extension": ".ts",
      "mimetype": "text/x.typescript",
      "name": "typescript",
      "nbconvert_exporter": "script",
      "pygments_lexer": "typescript",
      "version": "5.9.2"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
