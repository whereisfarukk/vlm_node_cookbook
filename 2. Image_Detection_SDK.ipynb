{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div align=\"center\">\n",
    "<p align=\"center\" style=\"width: 100%;\">\n",
    "    <img src=\"https://raw.githubusercontent.com/vlm-run/.github/refs/heads/main/profile/assets/vlm-black.svg\" alt=\"VLM Run Logo\" width=\"80\" style=\"margin-bottom: -5px; color: #2e3138; vertical-align: middle; padding-right: 5px;\"><br>\n",
    "</p>\n",
    "<p align=\"center\"><a href=\"https://docs.vlm.run\"><b>Website</b></a> | <a href=\"https://docs.vlm.run/\"><b>API Docs</b></a> | <a href=\"https://docs.vlm.run/blog\"><b>Blog</b></a> | <a href=\"https://discord.gg/AMApC2UzVY\"><b>Discord</b></a>\n",
    "</p>\n",
    "<p align=\"center\">\n",
    "<a href=\"https://discord.gg/AMApC2UzVY\"><img alt=\"Discord\" src=\"https://img.shields.io/badge/discord-chat-purple?color=%235765F2&label=discord&logo=discord\"></a>\n",
    "<a href=\"https://twitter.com/vlmrun\"><img alt=\"Twitter Follow\" src=\"https://img.shields.io/twitter/follow/vlmrun.svg?style=social&logo=twitter\"></a>\n",
    "</p>\n",
    "</div>\n",
    "\n",
    "Welcome to **[VLM Run Cookbooks](https://github.com/vlm-run/vlmrun-cookbook)**, a comprehensive collection of examples and notebooks demonstrating the power of structured visual understanding using the [VLM Run Platform](https://app.vlm.run).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Case Study: Object Detection with VLM Run SDK\n",
    "\n",
    "This notebook demonstrates how to use the **official VLM Run Node.js SDK** (`vlmrun` npm package) for object detection. We'll cover:\n",
    "\n",
    "- **Object Detection**: Detect 80+ COCO dataset classes (person, car, cat, etc.)\n",
    "- **Person Detection**: Specialized detection for people in images\n",
    "- **Face Detection**: Detect and locate faces with high precision\n",
    "- **SDK Features**: Explore models, hub domains, and more\n",
    "\n",
    "All detections return bounding boxes in normalized `xywh` format (x, y, width, height) where values are between 0-1, along with labels.\n",
    "\n",
    "Reference: [VLM Run Detection Documentation](https://docs.vlm.run/agents/capabilities/image/detection)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Environment Setup\n",
    "\n",
    "To get started, install the VLM Run SDK and sign up for an API key on the [VLM Run App](https://app.vlm.run).\n",
    "- Store the VLM Run API key under the `VLMRUN_API_KEY` environment variable.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prerequisites\n",
    "\n",
    "* Node.js 18+\n",
    "* VLM Run API key (get one at [app.vlm.run](https://app.vlm.run))\n",
    "* Deno or tslab kernel for running TypeScript in Jupyter\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup\n",
    "\n",
    "First, let's install the required packages:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "// Install the VLM Run SDK\n",
    "// npm install vlmrun openai zod zod-to-json-schema\n",
    "\n",
    "// If using Deno kernel, install dependencies via npm specifiers\n",
    "// For tslab, run: npm install vlmrun openai zod zod-to-json-schema in your project directory\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "// Import the VLM Run SDK and dependencies\n",
    "import { VlmRun } from \"vlmrun\";\n",
    "import { z } from \"zod\";\n",
    "import { zodToJsonSchema } from \"zod-to-json-schema\";\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "Error",
     "evalue": "Please set the VLMRUN_API_KEY environment variable",
     "output_type": "error",
     "traceback": [
      "Stack trace:",
      "Error: Please set the VLMRUN_API_KEY environment variable",
      "    at <anonymous>:4:9"
     ]
    }
   ],
   "source": [
    "// Get API key from environment variable\n",
    "const VLMRUN_API_KEY = Deno.env.get(\"VLMRUN_API_KEY\");\n",
    "\n",
    "if (!VLMRUN_API_KEY) {\n",
    "    throw new Error(\"Please set the VLMRUN_API_KEY environment variable\");\n",
    "}\n",
    "\n",
    "console.log(\"‚úì API Key loaded successfully\");\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's initialize the VLM Run client using the SDK\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úì VLM Run SDK client initialized!\n"
     ]
    }
   ],
   "source": [
    "// Initialize the VLM Run client using the SDK\n",
    "const client = new VlmRun({\n",
    "    apiKey: VLMRUN_API_KEY,\n",
    "    baseURL: \"https://agent.vlm.run/v1\"  // Use the agent API endpoint\n",
    "});\n",
    "\n",
    "console.log(\"‚úì VLM Run SDK client initialized!\");\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define Detection Schemas\n",
    "\n",
    "We'll define Zod schemas for the detection responses (TypeScript equivalent of Python's Pydantic):\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úì Detection schemas defined\n",
      "JSON Schema: {\n",
      "  \"type\": \"object\",\n",
      "  \"properties\": {\n",
      "    \"detections\": {\n",
      "      \"type\": \"array\",\n",
      "      \"items\": {\n",
      "        \"type\": \"object\",\n",
      "        \"properties\": {\n",
      "          \"label\": {\n",
      "            \"type\": \"string\",\n",
      "            \"description\": \"Name of the detected object\"\n",
      "          },\n",
      "          \"xywh\": {\n",
      "            \"type\": \"array\",\n",
      "            \"minItems\": 4,\n",
      "            \"maxItems\": 4,\n",
      "            \"items\": [\n",
      "              {\n",
      "                \"type\": \"number\"\n",
      "              },\n",
      "              {\n",
      "                \"type\": \"number\"\n",
      "              },\n",
      "              {\n",
      "                \"type\": \"number\"\n",
      "              },\n",
      "              {\n",
      "                \"type\": \"number\"\n",
      "              }\n",
      "            ],\n",
      "            \"description\": \"Bounding box (x, y, width, height) - normalized values 0-1\"\n",
      "          }\n",
      "        },\n",
      "        \"required\": [\n",
      "          \"label\",\n",
      "          \"xywh\"\n",
      "        ],\n",
      "        \"additionalProperties\": false\n",
      "      },\n",
      "      \"description\": \"List of detected objects\"\n",
      "    }\n",
      "  },\n",
      "  \"required\": [\n",
      "    \"detections\"\n",
      "  ],\n",
      "  \"additionalProperties\": false,\n",
      "  \"$schema\": \"http://json-schema.org/draft-07/schema#\"\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "// Define the Detection schema using Zod\n",
    "const DetectionSchema = z.object({\n",
    "    label: z.string().describe(\"Name of the detected object\"),\n",
    "    xywh: z.tuple([z.number(), z.number(), z.number(), z.number()])\n",
    "        .describe(\"Bounding box (x, y, width, height) - normalized values 0-1\")\n",
    "});\n",
    "\n",
    "// Define the Detections response schema\n",
    "const DetectionsSchema = z.object({\n",
    "    detections: z.array(DetectionSchema).describe(\"List of detected objects\")\n",
    "});\n",
    "\n",
    "// Type inference from schemas\n",
    "type Detection = z.infer<typeof DetectionSchema>;\n",
    "type Detections = z.infer<typeof DetectionsSchema>;\n",
    "\n",
    "// Convert Zod schema to JSON Schema for API\n",
    "const detectionsJsonSchema = zodToJsonSchema(DetectionsSchema);\n",
    "console.log(\"‚úì Detection schemas defined\");\n",
    "console.log(\"JSON Schema:\", JSON.stringify(detectionsJsonSchema, null, 2));\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Helper Functions\n",
    "\n",
    "Let's create utility functions for detection using the VLM Run SDK:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úì Helper functions defined\n"
     ]
    }
   ],
   "source": [
    "/**\n",
    " * Perform object detection on an image using VLM Run SDK.\n",
    " * \n",
    " * @param imageUrl - URL of the image to analyze\n",
    " * @param prompt - Detection prompt (e.g., \"Detect all objects\", \"Detect all people\")\n",
    " * @returns Detections object with bounding boxes\n",
    " */\n",
    "async function detectObjects(imageUrl: string, prompt: string = \"Detect all objects in this image\"): Promise<Detections> {\n",
    "    // Use the SDK's agent.completions interface\n",
    "    const response = await client.agent.completions.create({\n",
    "        model: \"vlm-agent-1\",\n",
    "        messages: [\n",
    "            {\n",
    "                role: \"user\",\n",
    "                content: [\n",
    "                    { type: \"text\", text: prompt },\n",
    "                    { type: \"image_url\", image_url: { url: imageUrl, detail: \"auto\" } }\n",
    "                ]\n",
    "            }\n",
    "        ],\n",
    "        // Use VLM Run's format - schema at top level\n",
    "        response_format: { \n",
    "            type: \"json_schema\", \n",
    "            schema: detectionsJsonSchema\n",
    "        } as any\n",
    "    });\n",
    "    \n",
    "    const rawContent = response.choices[0].message.content || \"{}\";\n",
    "    const parsed = JSON.parse(rawContent);\n",
    "    return DetectionsSchema.parse(parsed);\n",
    "}\n",
    "\n",
    "/**\n",
    " * Format detection results for display.\n",
    " */\n",
    "function formatDetections(detections: Detection[], imageWidth?: number, imageHeight?: number): void {\n",
    "    console.log(`\\nüì¶ Found ${detections.length} objects:\\n`);\n",
    "    \n",
    "    detections.forEach((det, i) => {\n",
    "        const [x, y, w, h] = det.xywh;\n",
    "        \n",
    "        if (imageWidth && imageHeight) {\n",
    "            const x_px = Math.round(x * imageWidth);\n",
    "            const y_px = Math.round(y * imageHeight);\n",
    "            const w_px = Math.round(w * imageWidth);\n",
    "            const h_px = Math.round(h * imageHeight);\n",
    "            console.log(`  ${i + 1}. ${det.label}`);\n",
    "            console.log(`     Normalized: x=${x.toFixed(3)}, y=${y.toFixed(3)}, w=${w.toFixed(3)}, h=${h.toFixed(3)}`);\n",
    "            console.log(`     Pixels:     x=${x_px}, y=${y_px}, w=${w_px}, h=${h_px}`);\n",
    "        } else {\n",
    "            console.log(`  ${i + 1}. ${det.label}: xywh=[${x.toFixed(3)}, ${y.toFixed(3)}, ${w.toFixed(3)}, ${h.toFixed(3)}]`);\n",
    "        }\n",
    "    });\n",
    "}\n",
    "\n",
    "/**\n",
    " * Group detections by label and count them.\n",
    " */\n",
    "function countByLabel(detections: Detection[]): Record<string, number> {\n",
    "    return detections.reduce((acc, det) => {\n",
    "        acc[det.label] = (acc[det.label] || 0) + 1;\n",
    "        return acc;\n",
    "    }, {} as Record<string, number>);\n",
    "}\n",
    "\n",
    "console.log(\"‚úì Helper functions defined\");\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example 1: Person Detection\n",
    "\n",
    "Let's detect people in an image using the VLM Run SDK.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üì∑ Image URL: https://images.unsplash.com/photo-1511632765486-a01980e01a18?w=800\n",
      "\n",
      "üîç Detecting people using VLM Run SDK...\n",
      "\n",
      "Found 4 people in the image:\n",
      "  Person 1: person, xywh=[0.325, 0.267, 0.128, 0.628]\n",
      "  Person 2: person, xywh=[0.401, 0.272, 0.144, 0.623]\n",
      "  Person 3: person, xywh=[0.488, 0.272, 0.140, 0.623]\n",
      "  Person 4: person, xywh=[0.592, 0.240, 0.116, 0.655]\n",
      "\n",
      "üìä Detection Summary:\n",
      "‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê\n",
      "‚îÇ (idx)  ‚îÇ Values ‚îÇ\n",
      "‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§\n",
      "‚îÇ person ‚îÇ      4 ‚îÇ\n",
      "‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò\n"
     ]
    }
   ],
   "source": [
    "// Image with people\n",
    "const peopleImageUrl = \"https://images.unsplash.com/photo-1511632765486-a01980e01a18?w=800\";\n",
    "\n",
    "console.log(\"üì∑ Image URL:\", peopleImageUrl);\n",
    "console.log(\"\\nüîç Detecting people using VLM Run SDK...\\n\");\n",
    "\n",
    "// Perform person detection\n",
    "const personResult = await detectObjects(peopleImageUrl, \"Detect all people in this image\");\n",
    "\n",
    "// Display results\n",
    "console.log(`Found ${personResult.detections.length} people in the image:`);\n",
    "personResult.detections.forEach((det, i) => {\n",
    "    console.log(`  Person ${i + 1}: ${det.label}, xywh=[${det.xywh.map(v => v.toFixed(3)).join(\", \")}]`);\n",
    "});\n",
    "\n",
    "// Show summary\n",
    "const labelCounts = countByLabel(personResult.detections);\n",
    "console.log(\"\\nüìä Detection Summary:\");\n",
    "console.table(labelCounts);\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example 2: General Object Detection\n",
    "\n",
    "Let's detect various objects in a street scene.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üì∑ Image URL: https://storage.googleapis.com/vlm-data-public-prod/hub/examples/image.object-detection/crossroad.jpg\n",
      "\n",
      "üîç Detecting all objects in the street scene...\n",
      "\n",
      "\n",
      "üì¶ Found 155 objects:\n",
      "\n",
      "  1. person\n",
      "     Normalized: x=0.141, y=0.385, w=0.112, h=0.445\n",
      "     Pixels:     x=113, y=231, w=90, h=267\n",
      "  2. person\n",
      "     Normalized: x=0.550, y=0.388, w=0.202, h=0.560\n",
      "     Pixels:     x=440, y=233, w=162, h=336\n",
      "  3. person\n",
      "     Normalized: x=0.550, y=0.470, w=0.078, h=0.425\n",
      "     Pixels:     x=440, y=282, w=62, h=255\n",
      "  4. person\n",
      "     Normalized: x=0.390, y=0.409, w=0.062, h=0.262\n",
      "     Pixels:     x=312, y=245, w=50, h=157\n",
      "  5. person\n",
      "     Normalized: x=0.531, y=0.624, w=0.059, h=0.274\n",
      "     Pixels:     x=425, y=374, w=47, h=164\n",
      "  6. person\n",
      "     Normalized: x=0.409, y=0.727, w=0.094, h=0.116\n",
      "     Pixels:     x=327, y=436, w=75, h=70\n",
      "  7. person\n",
      "     Normalized: x=0.052, y=0.420, w=0.028, h=0.026\n",
      "     Pixels:     x=42, y=252, w=22, h=16\n",
      "  8. person\n",
      "     Normalized: x=0.151, y=0.436, w=0.018, h=0.091\n",
      "     Pixels:     x=121, y=262, w=14, h=55\n",
      "  9. person\n",
      "     Normalized: x=0.727, y=0.426, w=0.018, h=0.035\n",
      "     Pixels:     x=582, y=256, w=14, h=21\n",
      "  10. person\n",
      "     Normalized: x=0.704, y=0.424, w=0.018, h=0.037\n",
      "     Pixels:     x=563, y=254, w=14, h=22\n",
      "  11. vehicle\n",
      "     Normalized: x=0.220, y=0.381, w=0.183, h=0.340\n",
      "     Pixels:     x=176, y=229, w=146, h=204\n",
      "  12. vehicle\n",
      "     Normalized: x=0.477, y=0.430, w=0.117, h=0.076\n",
      "     Pixels:     x=382, y=258, w=94, h=46\n",
      "  13. vehicle\n",
      "     Normalized: x=0.755, y=0.443, w=0.083, h=0.112\n",
      "     Pixels:     x=604, y=266, w=66, h=67\n",
      "  14. vehicle\n",
      "     Normalized: x=0.817, y=0.432, w=0.093, h=0.153\n",
      "     Pixels:     x=654, y=259, w=74, h=92\n",
      "  15. vehicle\n",
      "     Normalized: x=0.895, y=0.429, w=0.105, h=0.195\n",
      "     Pixels:     x=716, y=257, w=84, h=117\n",
      "  16. vehicle\n",
      "     Normalized: x=0.687, y=0.458, w=0.077, h=0.072\n",
      "     Pixels:     x=550, y=275, w=62, h=43\n",
      "  17. vehicle\n",
      "     Normalized: x=0.586, y=0.424, w=0.049, h=0.046\n",
      "     Pixels:     x=469, y=254, w=39, h=28\n",
      "  18. vehicle\n",
      "     Normalized: x=0.465, y=0.420, w=0.023, h=0.034\n",
      "     Pixels:     x=372, y=252, w=18, h=20\n",
      "  19. vehicle\n",
      "     Normalized: x=0.000, y=0.443, w=0.146, h=0.254\n",
      "     Pixels:     x=0, y=266, w=117, h=152\n",
      "  20. vehicle\n",
      "     Normalized: x=0.545, y=0.407, w=0.036, h=0.023\n",
      "     Pixels:     x=436, y=244, w=29, h=14\n",
      "  21. vehicle\n",
      "     Normalized: x=0.441, y=0.410, w=0.014, h=0.023\n",
      "     Pixels:     x=353, y=246, w=11, h=14\n",
      "  22. vehicle\n",
      "     Normalized: x=0.453, y=0.410, w=0.012, h=0.020\n",
      "     Pixels:     x=362, y=246, w=10, h=12\n",
      "  23. vehicle\n",
      "     Normalized: x=0.401, y=0.410, w=0.018, h=0.020\n",
      "     Pixels:     x=321, y=246, w=14, h=12\n",
      "  24. vehicle\n",
      "     Normalized: x=0.375, y=0.407, w=0.030, h=0.023\n",
      "     Pixels:     x=300, y=244, w=24, h=14\n",
      "  25. vehicle\n",
      "     Normalized: x=0.360, y=0.407, w=0.017, h=0.020\n",
      "     Pixels:     x=288, y=244, w=14, h=12\n",
      "  26. vehicle\n",
      "     Normalized: x=0.233, y=0.416, w=0.031, h=0.027\n",
      "     Pixels:     x=186, y=250, w=25, h=16\n",
      "  27. vehicle\n",
      "     Normalized: x=0.261, y=0.416, w=0.030, h=0.027\n",
      "     Pixels:     x=209, y=250, w=24, h=16\n",
      "  28. vehicle\n",
      "     Normalized: x=0.287, y=0.416, w=0.029, h=0.027\n",
      "     Pixels:     x=230, y=250, w=23, h=16\n",
      "  29. vehicle\n",
      "     Normalized: x=0.316, y=0.416, w=0.029, h=0.027\n",
      "     Pixels:     x=253, y=250, w=23, h=16\n",
      "  30. vehicle\n",
      "     Normalized: x=0.345, y=0.416, w=0.027, h=0.027\n",
      "     Pixels:     x=276, y=250, w=22, h=16\n",
      "  31. vehicle\n",
      "     Normalized: x=0.486, y=0.385, w=0.019, h=0.038\n",
      "     Pixels:     x=389, y=231, w=15, h=23\n",
      "  32. vehicle\n",
      "     Normalized: x=0.512, y=0.385, w=0.017, h=0.038\n",
      "     Pixels:     x=410, y=231, w=14, h=23\n",
      "  33. vehicle\n",
      "     Normalized: x=0.531, y=0.385, w=0.016, h=0.038\n",
      "     Pixels:     x=425, y=231, w=13, h=23\n",
      "  34. vehicle\n",
      "     Normalized: x=0.551, y=0.385, w=0.016, h=0.038\n",
      "     Pixels:     x=441, y=231, w=13, h=23\n",
      "  35. vehicle\n",
      "     Normalized: x=0.571, y=0.385, w=0.016, h=0.038\n",
      "     Pixels:     x=457, y=231, w=13, h=23\n",
      "  36. vehicle\n",
      "     Normalized: x=0.591, y=0.385, w=0.016, h=0.038\n",
      "     Pixels:     x=473, y=231, w=13, h=23\n",
      "  37. vehicle\n",
      "     Normalized: x=0.607, y=0.385, w=0.016, h=0.038\n",
      "     Pixels:     x=486, y=231, w=13, h=23\n",
      "  38. vehicle\n",
      "     Normalized: x=0.623, y=0.385, w=0.016, h=0.038\n",
      "     Pixels:     x=498, y=231, w=13, h=23\n",
      "  39. vehicle\n",
      "     Normalized: x=0.639, y=0.385, w=0.016, h=0.038\n",
      "     Pixels:     x=511, y=231, w=13, h=23\n",
      "  40. vehicle\n",
      "     Normalized: x=0.655, y=0.385, w=0.016, h=0.038\n",
      "     Pixels:     x=524, y=231, w=13, h=23\n",
      "  41. vehicle\n",
      "     Normalized: x=0.671, y=0.385, w=0.016, h=0.038\n",
      "     Pixels:     x=537, y=231, w=13, h=23\n",
      "  42. vehicle\n",
      "     Normalized: x=0.687, y=0.385, w=0.016, h=0.038\n",
      "     Pixels:     x=550, y=231, w=13, h=23\n",
      "  43. vehicle\n",
      "     Normalized: x=0.703, y=0.385, w=0.016, h=0.038\n",
      "     Pixels:     x=562, y=231, w=13, h=23\n",
      "  44. vehicle\n",
      "     Normalized: x=0.719, y=0.385, w=0.016, h=0.038\n",
      "     Pixels:     x=575, y=231, w=13, h=23\n",
      "  45. vehicle\n",
      "     Normalized: x=0.735, y=0.385, w=0.016, h=0.038\n",
      "     Pixels:     x=588, y=231, w=13, h=23\n",
      "  46. vehicle\n",
      "     Normalized: x=0.751, y=0.385, w=0.016, h=0.038\n",
      "     Pixels:     x=601, y=231, w=13, h=23\n",
      "  47. vehicle\n",
      "     Normalized: x=0.767, y=0.385, w=0.016, h=0.038\n",
      "     Pixels:     x=614, y=231, w=13, h=23\n",
      "  48. vehicle\n",
      "     Normalized: x=0.783, y=0.385, w=0.016, h=0.038\n",
      "     Pixels:     x=626, y=231, w=13, h=23\n",
      "  49. vehicle\n",
      "     Normalized: x=0.799, y=0.385, w=0.016, h=0.038\n",
      "     Pixels:     x=639, y=231, w=13, h=23\n",
      "  50. vehicle\n",
      "     Normalized: x=0.815, y=0.385, w=0.016, h=0.038\n",
      "     Pixels:     x=652, y=231, w=13, h=23\n",
      "  51. vehicle\n",
      "     Normalized: x=0.831, y=0.385, w=0.016, h=0.038\n",
      "     Pixels:     x=665, y=231, w=13, h=23\n",
      "  52. vehicle\n",
      "     Normalized: x=0.847, y=0.385, w=0.016, h=0.038\n",
      "     Pixels:     x=678, y=231, w=13, h=23\n",
      "  53. vehicle\n",
      "     Normalized: x=0.863, y=0.385, w=0.016, h=0.038\n",
      "     Pixels:     x=690, y=231, w=13, h=23\n",
      "  54. vehicle\n",
      "     Normalized: x=0.879, y=0.385, w=0.016, h=0.038\n",
      "     Pixels:     x=703, y=231, w=13, h=23\n",
      "  55. vehicle\n",
      "     Normalized: x=0.895, y=0.385, w=0.016, h=0.038\n",
      "     Pixels:     x=716, y=231, w=13, h=23\n",
      "  56. vehicle\n",
      "     Normalized: x=0.911, y=0.385, w=0.016, h=0.038\n",
      "     Pixels:     x=729, y=231, w=13, h=23\n",
      "  57. vehicle\n",
      "     Normalized: x=0.927, y=0.385, w=0.016, h=0.038\n",
      "     Pixels:     x=742, y=231, w=13, h=23\n",
      "  58. vehicle\n",
      "     Normalized: x=0.943, y=0.385, w=0.016, h=0.038\n",
      "     Pixels:     x=754, y=231, w=13, h=23\n",
      "  59. vehicle\n",
      "     Normalized: x=0.959, y=0.385, w=0.016, h=0.038\n",
      "     Pixels:     x=767, y=231, w=13, h=23\n",
      "  60. vehicle\n",
      "     Normalized: x=0.975, y=0.385, w=0.016, h=0.038\n",
      "     Pixels:     x=780, y=231, w=13, h=23\n",
      "  61. vehicle\n",
      "     Normalized: x=0.991, y=0.385, w=0.009, h=0.038\n",
      "     Pixels:     x=793, y=231, w=7, h=23\n",
      "  62. face\n",
      "     Normalized: x=0.511, y=0.599, w=0.033, h=0.066\n",
      "     Pixels:     x=409, y=359, w=26, h=40\n",
      "  63. face\n",
      "     Normalized: x=0.563, y=0.484, w=0.038, h=0.064\n",
      "     Pixels:     x=450, y=290, w=30, h=38\n",
      "  64. face\n",
      "     Normalized: x=0.625, y=0.394, w=0.042, h=0.080\n",
      "     Pixels:     x=500, y=236, w=34, h=48\n",
      "  65. face\n",
      "     Normalized: x=0.172, y=0.394, w=0.032, h=0.053\n",
      "     Pixels:     x=138, y=236, w=26, h=32\n",
      "  66. face\n",
      "     Normalized: x=0.420, y=0.416, w=0.016, h=0.024\n",
      "     Pixels:     x=336, y=250, w=13, h=14\n",
      "  67. face\n",
      "     Normalized: x=0.419, y=0.410, w=0.020, h=0.023\n",
      "     Pixels:     x=335, y=246, w=16, h=14\n",
      "  68. face\n",
      "     Normalized: x=0.542, y=0.645, w=0.026, h=0.042\n",
      "     Pixels:     x=434, y=387, w=21, h=25\n",
      "  69. text\n",
      "     Normalized: x=0.017, y=0.357, w=0.044, h=0.028\n",
      "     Pixels:     x=14, y=214, w=35, h=17\n",
      "  70. text\n",
      "     Normalized: x=0.241, y=0.624, w=0.036, h=0.024\n",
      "     Pixels:     x=193, y=374, w=29, h=14\n",
      "  71. text\n",
      "     Normalized: x=0.774, y=0.317, w=0.024, h=0.013\n",
      "     Pixels:     x=619, y=190, w=19, h=8\n",
      "  72. text\n",
      "     Normalized: x=0.847, y=0.023, w=0.038, h=0.078\n",
      "     Pixels:     x=678, y=14, w=30, h=47\n",
      "  73. text\n",
      "     Normalized: x=0.740, y=0.114, w=0.041, h=0.119\n",
      "     Pixels:     x=592, y=68, w=33, h=71\n",
      "  74. text\n",
      "     Normalized: x=0.416, y=0.108, w=0.022, h=0.129\n",
      "     Pixels:     x=333, y=65, w=18, h=77\n",
      "  75. text\n",
      "     Normalized: x=0.438, y=0.108, w=0.014, h=0.129\n",
      "     Pixels:     x=350, y=65, w=11, h=77\n",
      "  76. text\n",
      "     Normalized: x=0.401, y=0.108, w=0.012, h=0.129\n",
      "     Pixels:     x=321, y=65, w=10, h=77\n",
      "  77. text\n",
      "     Normalized: x=0.462, y=0.108, w=0.014, h=0.129\n",
      "     Pixels:     x=370, y=65, w=11, h=77\n",
      "  78. text\n",
      "     Normalized: x=0.476, y=0.108, w=0.012, h=0.129\n",
      "     Pixels:     x=381, y=65, w=10, h=77\n",
      "  79. text\n",
      "     Normalized: x=0.488, y=0.108, w=0.013, h=0.129\n",
      "     Pixels:     x=390, y=65, w=10, h=77\n",
      "  80. text\n",
      "     Normalized: x=0.501, y=0.108, w=0.012, h=0.129\n",
      "     Pixels:     x=401, y=65, w=10, h=77\n",
      "  81. text\n",
      "     Normalized: x=0.513, y=0.108, w=0.012, h=0.129\n",
      "     Pixels:     x=410, y=65, w=10, h=77\n",
      "  82. text\n",
      "     Normalized: x=0.525, y=0.108, w=0.012, h=0.129\n",
      "     Pixels:     x=420, y=65, w=10, h=77\n",
      "  83. text\n",
      "     Normalized: x=0.537, y=0.108, w=0.012, h=0.129\n",
      "     Pixels:     x=430, y=65, w=10, h=77\n",
      "  84. text\n",
      "     Normalized: x=0.549, y=0.108, w=0.012, h=0.129\n",
      "     Pixels:     x=439, y=65, w=10, h=77\n",
      "  85. text\n",
      "     Normalized: x=0.561, y=0.108, w=0.012, h=0.129\n",
      "     Pixels:     x=449, y=65, w=10, h=77\n",
      "  86. text\n",
      "     Normalized: x=0.573, y=0.108, w=0.012, h=0.129\n",
      "     Pixels:     x=458, y=65, w=10, h=77\n",
      "  87. text\n",
      "     Normalized: x=0.585, y=0.108, w=0.012, h=0.129\n",
      "     Pixels:     x=468, y=65, w=10, h=77\n",
      "  88. text\n",
      "     Normalized: x=0.597, y=0.108, w=0.012, h=0.129\n",
      "     Pixels:     x=478, y=65, w=10, h=77\n",
      "  89. text\n",
      "     Normalized: x=0.609, y=0.108, w=0.012, h=0.129\n",
      "     Pixels:     x=487, y=65, w=10, h=77\n",
      "  90. text\n",
      "     Normalized: x=0.621, y=0.108, w=0.012, h=0.129\n",
      "     Pixels:     x=497, y=65, w=10, h=77\n",
      "  91. text\n",
      "     Normalized: x=0.633, y=0.108, w=0.012, h=0.129\n",
      "     Pixels:     x=506, y=65, w=10, h=77\n",
      "  92. text\n",
      "     Normalized: x=0.645, y=0.108, w=0.012, h=0.129\n",
      "     Pixels:     x=516, y=65, w=10, h=77\n",
      "  93. text\n",
      "     Normalized: x=0.657, y=0.108, w=0.012, h=0.129\n",
      "     Pixels:     x=526, y=65, w=10, h=77\n",
      "  94. text\n",
      "     Normalized: x=0.669, y=0.108, w=0.012, h=0.129\n",
      "     Pixels:     x=535, y=65, w=10, h=77\n",
      "  95. text\n",
      "     Normalized: x=0.681, y=0.108, w=0.012, h=0.129\n",
      "     Pixels:     x=545, y=65, w=10, h=77\n",
      "  96. text\n",
      "     Normalized: x=0.693, y=0.108, w=0.012, h=0.129\n",
      "     Pixels:     x=554, y=65, w=10, h=77\n",
      "  97. text\n",
      "     Normalized: x=0.705, y=0.108, w=0.012, h=0.129\n",
      "     Pixels:     x=564, y=65, w=10, h=77\n",
      "  98. text\n",
      "     Normalized: x=0.717, y=0.108, w=0.012, h=0.129\n",
      "     Pixels:     x=574, y=65, w=10, h=77\n",
      "  99. text\n",
      "     Normalized: x=0.729, y=0.108, w=0.012, h=0.129\n",
      "     Pixels:     x=583, y=65, w=10, h=77\n",
      "  100. text\n",
      "     Normalized: x=0.741, y=0.108, w=0.012, h=0.129\n",
      "     Pixels:     x=593, y=65, w=10, h=77\n",
      "  101. text\n",
      "     Normalized: x=0.753, y=0.108, w=0.012, h=0.129\n",
      "     Pixels:     x=602, y=65, w=10, h=77\n",
      "  102. text\n",
      "     Normalized: x=0.765, y=0.108, w=0.012, h=0.129\n",
      "     Pixels:     x=612, y=65, w=10, h=77\n",
      "  103. text\n",
      "     Normalized: x=0.777, y=0.108, w=0.012, h=0.129\n",
      "     Pixels:     x=622, y=65, w=10, h=77\n",
      "  104. text\n",
      "     Normalized: x=0.789, y=0.108, w=0.012, h=0.129\n",
      "     Pixels:     x=631, y=65, w=10, h=77\n",
      "  105. text\n",
      "     Normalized: x=0.801, y=0.108, w=0.012, h=0.129\n",
      "     Pixels:     x=641, y=65, w=10, h=77\n",
      "  106. text\n",
      "     Normalized: x=0.813, y=0.108, w=0.012, h=0.129\n",
      "     Pixels:     x=650, y=65, w=10, h=77\n",
      "  107. text\n",
      "     Normalized: x=0.825, y=0.108, w=0.012, h=0.129\n",
      "     Pixels:     x=660, y=65, w=10, h=77\n",
      "  108. text\n",
      "     Normalized: x=0.837, y=0.108, w=0.012, h=0.129\n",
      "     Pixels:     x=670, y=65, w=10, h=77\n",
      "  109. text\n",
      "     Normalized: x=0.849, y=0.108, w=0.012, h=0.129\n",
      "     Pixels:     x=679, y=65, w=10, h=77\n",
      "  110. text\n",
      "     Normalized: x=0.861, y=0.108, w=0.012, h=0.129\n",
      "     Pixels:     x=689, y=65, w=10, h=77\n",
      "  111. text\n",
      "     Normalized: x=0.873, y=0.108, w=0.012, h=0.129\n",
      "     Pixels:     x=698, y=65, w=10, h=77\n",
      "  112. text\n",
      "     Normalized: x=0.885, y=0.108, w=0.012, h=0.129\n",
      "     Pixels:     x=708, y=65, w=10, h=77\n",
      "  113. text\n",
      "     Normalized: x=0.897, y=0.108, w=0.012, h=0.129\n",
      "     Pixels:     x=718, y=65, w=10, h=77\n",
      "  114. text\n",
      "     Normalized: x=0.909, y=0.108, w=0.012, h=0.129\n",
      "     Pixels:     x=727, y=65, w=10, h=77\n",
      "  115. text\n",
      "     Normalized: x=0.921, y=0.108, w=0.012, h=0.129\n",
      "     Pixels:     x=737, y=65, w=10, h=77\n",
      "  116. text\n",
      "     Normalized: x=0.933, y=0.108, w=0.012, h=0.129\n",
      "     Pixels:     x=746, y=65, w=10, h=77\n",
      "  117. text\n",
      "     Normalized: x=0.945, y=0.108, w=0.012, h=0.129\n",
      "     Pixels:     x=756, y=65, w=10, h=77\n",
      "  118. text\n",
      "     Normalized: x=0.957, y=0.108, w=0.012, h=0.129\n",
      "     Pixels:     x=766, y=65, w=10, h=77\n",
      "  119. text\n",
      "     Normalized: x=0.969, y=0.108, w=0.012, h=0.129\n",
      "     Pixels:     x=775, y=65, w=10, h=77\n",
      "  120. text\n",
      "     Normalized: x=0.981, y=0.108, w=0.012, h=0.129\n",
      "     Pixels:     x=785, y=65, w=10, h=77\n",
      "  121. text\n",
      "     Normalized: x=0.993, y=0.108, w=0.007, h=0.129\n",
      "     Pixels:     x=794, y=65, w=6, h=77\n",
      "  122. text\n",
      "     Normalized: x=0.993, y=0.108, w=0.007, h=0.129\n",
      "     Pixels:     x=794, y=65, w=6, h=77\n",
      "  123. text\n",
      "     Normalized: x=0.993, y=0.108, w=0.007, h=0.129\n",
      "     Pixels:     x=794, y=65, w=6, h=77\n",
      "  124. text\n",
      "     Normalized: x=0.993, y=0.108, w=0.007, h=0.129\n",
      "     Pixels:     x=794, y=65, w=6, h=77\n",
      "  125. text\n",
      "     Normalized: x=0.993, y=0.108, w=0.007, h=0.129\n",
      "     Pixels:     x=794, y=65, w=6, h=77\n",
      "  126. text\n",
      "     Normalized: x=0.993, y=0.108, w=0.007, h=0.129\n",
      "     Pixels:     x=794, y=65, w=6, h=77\n",
      "  127. text\n",
      "     Normalized: x=0.993, y=0.108, w=0.007, h=0.129\n",
      "     Pixels:     x=794, y=65, w=6, h=77\n",
      "  128. text\n",
      "     Normalized: x=0.993, y=0.108, w=0.007, h=0.129\n",
      "     Pixels:     x=794, y=65, w=6, h=77\n",
      "  129. text\n",
      "     Normalized: x=0.993, y=0.108, w=0.007, h=0.129\n",
      "     Pixels:     x=794, y=65, w=6, h=77\n",
      "  130. text\n",
      "     Normalized: x=0.993, y=0.108, w=0.007, h=0.129\n",
      "     Pixels:     x=794, y=65, w=6, h=77\n",
      "  131. text\n",
      "     Normalized: x=0.993, y=0.108, w=0.007, h=0.129\n",
      "     Pixels:     x=794, y=65, w=6, h=77\n",
      "  132. text\n",
      "     Normalized: x=0.993, y=0.108, w=0.007, h=0.129\n",
      "     Pixels:     x=794, y=65, w=6, h=77\n",
      "  133. text\n",
      "     Normalized: x=0.993, y=0.108, w=0.007, h=0.129\n",
      "     Pixels:     x=794, y=65, w=6, h=77\n",
      "  134. text\n",
      "     Normalized: x=0.993, y=0.108, w=0.007, h=0.129\n",
      "     Pixels:     x=794, y=65, w=6, h=77\n",
      "  135. text\n",
      "     Normalized: x=0.993, y=0.108, w=0.007, h=0.129\n",
      "     Pixels:     x=794, y=65, w=6, h=77\n",
      "  136. text\n",
      "     Normalized: x=0.993, y=0.108, w=0.007, h=0.129\n",
      "     Pixels:     x=794, y=65, w=6, h=77\n",
      "  137. text\n",
      "     Normalized: x=0.993, y=0.108, w=0.007, h=0.129\n",
      "     Pixels:     x=794, y=65, w=6, h=77\n",
      "  138. text\n",
      "     Normalized: x=0.993, y=0.108, w=0.007, h=0.129\n",
      "     Pixels:     x=794, y=65, w=6, h=77\n",
      "  139. text\n",
      "     Normalized: x=0.993, y=0.108, w=0.007, h=0.129\n",
      "     Pixels:     x=794, y=65, w=6, h=77\n",
      "  140. text\n",
      "     Normalized: x=0.993, y=0.108, w=0.007, h=0.129\n",
      "     Pixels:     x=794, y=65, w=6, h=77\n",
      "  141. text\n",
      "     Normalized: x=0.993, y=0.108, w=0.007, h=0.129\n",
      "     Pixels:     x=794, y=65, w=6, h=77\n",
      "  142. text\n",
      "     Normalized: x=0.993, y=0.108, w=0.007, h=0.129\n",
      "     Pixels:     x=794, y=65, w=6, h=77\n",
      "  143. text\n",
      "     Normalized: x=0.993, y=0.108, w=0.007, h=0.129\n",
      "     Pixels:     x=794, y=65, w=6, h=77\n",
      "  144. text\n",
      "     Normalized: x=0.993, y=0.108, w=0.007, h=0.129\n",
      "     Pixels:     x=794, y=65, w=6, h=77\n",
      "  145. text\n",
      "     Normalized: x=0.993, y=0.108, w=0.007, h=0.129\n",
      "     Pixels:     x=794, y=65, w=6, h=77\n",
      "  146. text\n",
      "     Normalized: x=0.993, y=0.108, w=0.007, h=0.129\n",
      "     Pixels:     x=794, y=65, w=6, h=77\n",
      "  147. text\n",
      "     Normalized: x=0.993, y=0.108, w=0.007, h=0.129\n",
      "     Pixels:     x=794, y=65, w=6, h=77\n",
      "  148. text\n",
      "     Normalized: x=0.993, y=0.108, w=0.007, h=0.129\n",
      "     Pixels:     x=794, y=65, w=6, h=77\n",
      "  149. text\n",
      "     Normalized: x=0.993, y=0.108, w=0.007, h=0.129\n",
      "     Pixels:     x=794, y=65, w=6, h=77\n",
      "  150. text\n",
      "     Normalized: x=0.993, y=0.108, w=0.007, h=0.129\n",
      "     Pixels:     x=794, y=65, w=6, h=77\n",
      "  151. text\n",
      "     Normalized: x=0.993, y=0.108, w=0.007, h=0.129\n",
      "     Pixels:     x=794, y=65, w=6, h=77\n",
      "  152. text\n",
      "     Normalized: x=0.993, y=0.108, w=0.007, h=0.129\n",
      "     Pixels:     x=794, y=65, w=6, h=77\n",
      "  153. text\n",
      "     Normalized: x=0.993, y=0.108, w=0.007, h=0.129\n",
      "     Pixels:     x=794, y=65, w=6, h=77\n",
      "  154. text\n",
      "     Normalized: x=0.993, y=0.108, w=0.007, h=0.129\n",
      "     Pixels:     x=794, y=65, w=6, h=77\n",
      "  155. text\n",
      "     Normalized: x=0.993, y=0.108, w=0.007, h=0.129\n",
      "     Pixels:     x=794, y=65, w=6, h=77\n",
      "\n",
      "üìä Objects Detected by Category:\n",
      "‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê\n",
      "‚îÇ (idx)   ‚îÇ Values ‚îÇ\n",
      "‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§\n",
      "‚îÇ person  ‚îÇ     10 ‚îÇ\n",
      "‚îÇ vehicle ‚îÇ     51 ‚îÇ\n",
      "‚îÇ face    ‚îÇ      7 ‚îÇ\n",
      "‚îÇ text    ‚îÇ     87 ‚îÇ\n",
      "‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò\n"
     ]
    }
   ],
   "source": [
    "// Street scene image\n",
    "const streetImageUrl = \"https://storage.googleapis.com/vlm-data-public-prod/hub/examples/image.object-detection/crossroad.jpg\";\n",
    "\n",
    "console.log(\"üì∑ Image URL:\", streetImageUrl);\n",
    "console.log(\"\\nüîç Detecting all objects in the street scene...\\n\");\n",
    "\n",
    "// Perform general object detection\n",
    "const objectResult = await detectObjects(streetImageUrl, \"Detect all objects in this image including people, vehicles, and other objects\");\n",
    "\n",
    "// Format and display results with assumed image dimensions\n",
    "formatDetections(objectResult.detections, 800, 600);\n",
    "\n",
    "// Show counts by category\n",
    "const objectCounts = countByLabel(objectResult.detections);\n",
    "console.log(\"\\nüìä Objects Detected by Category:\");\n",
    "console.table(objectCounts);\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example 3: Face Detection\n",
    "\n",
    "Let's specifically detect faces in an image.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üì∑ Image URL: https://images.unsplash.com/photo-1511632765486-a01980e01a18?w=800\n",
      "\n",
      "üîç Detecting faces using VLM Run SDK...\n",
      "\n",
      "Found 4 faces:\n",
      "  Face 1: x=0.323, y=0.267, w=0.123, h=0.620\n",
      "  Face 2: x=0.399, y=0.273, w=0.143, h=0.624\n",
      "  Face 3: x=0.485, y=0.275, w=0.142, h=0.628\n",
      "  Face 4: x=0.592, y=0.240, w=0.111, h=0.653\n"
     ]
    }
   ],
   "source": [
    "// Use the same people image for face detection\n",
    "console.log(\"üì∑ Image URL:\", peopleImageUrl);\n",
    "console.log(\"\\nüîç Detecting faces using VLM Run SDK...\\n\");\n",
    "\n",
    "// Perform face detection\n",
    "const faceResult = await detectObjects(peopleImageUrl, \"Detect all faces in this image\");\n",
    "\n",
    "// Display face detection results\n",
    "console.log(`Found ${faceResult.detections.length} faces:`);\n",
    "faceResult.detections.forEach((det, i) => {\n",
    "    const [x, y, w, h] = det.xywh;\n",
    "    console.log(`  Face ${i + 1}: x=${x.toFixed(3)}, y=${y.toFixed(3)}, w=${w.toFixed(3)}, h=${h.toFixed(3)}`);\n",
    "});\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example 4: Batch Processing Multiple Images\n",
    "\n",
    "Process multiple images and aggregate detection results using the SDK.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing: Street Scene...\n",
      "Processing: Group of People...\n",
      "Processing: Cat...\n",
      "\n",
      "=== Batch Detection Results ===\n",
      "\n",
      "‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê\n",
      "‚îÇ (idx) ‚îÇ Image             ‚îÇ Objects Found ‚îÇ Unique Labels          ‚îÇ\n",
      "‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§\n",
      "‚îÇ     0 ‚îÇ \"Street Scene\"    ‚îÇ            97 ‚îÇ \"person\"               ‚îÇ\n",
      "‚îÇ     1 ‚îÇ \"Group of People\" ‚îÇ            12 ‚îÇ \"person, shirt, pants\" ‚îÇ\n",
      "‚îÇ     2 ‚îÇ \"Cat\"             ‚îÇ             3 ‚îÇ \"cat, bamboo, face\"    ‚îÇ\n",
      "‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò\n"
     ]
    }
   ],
   "source": [
    "// Multiple images for batch processing\n",
    "const batchImages = [\n",
    "    { url: \"https://storage.googleapis.com/vlm-data-public-prod/hub/examples/image.object-detection/crossroad.jpg\", name: \"Street Scene\" },\n",
    "    { url: \"https://images.unsplash.com/photo-1511632765486-a01980e01a18?w=800\", name: \"Group of People\" },\n",
    "    { url: \"https://images.unsplash.com/photo-1514888286974-6c03e2ca1dba?w=400\", name: \"Cat\" },\n",
    "];\n",
    "\n",
    "interface BatchDetectionResult {\n",
    "    imageName: string;\n",
    "    url: string;\n",
    "    objectCount: number;\n",
    "    labels: string[];\n",
    "    error?: string;\n",
    "}\n",
    "\n",
    "async function processBatch(images: { url: string; name: string }[]): Promise<BatchDetectionResult[]> {\n",
    "    const results: BatchDetectionResult[] = [];\n",
    "    \n",
    "    for (const img of images) {\n",
    "        try {\n",
    "            console.log(`Processing: ${img.name}...`);\n",
    "            const detections = await detectObjects(img.url);\n",
    "            const uniqueLabels = [...new Set(detections.detections.map(d => d.label))];\n",
    "            \n",
    "            results.push({\n",
    "                imageName: img.name,\n",
    "                url: img.url,\n",
    "                objectCount: detections.detections.length,\n",
    "                labels: uniqueLabels\n",
    "            });\n",
    "        } catch (error) {\n",
    "            console.log(`Error processing ${img.name}: ${error}`);\n",
    "            results.push({\n",
    "                imageName: img.name,\n",
    "                url: img.url,\n",
    "                objectCount: 0,\n",
    "                labels: [],\n",
    "                error: String(error)\n",
    "            });\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    return results;\n",
    "}\n",
    "\n",
    "// Process all images\n",
    "const batchResults = await processBatch(batchImages);\n",
    "\n",
    "// Display batch results\n",
    "console.log(\"\\n=== Batch Detection Results ===\\n\");\n",
    "console.table(batchResults.map(r => ({\n",
    "    Image: r.imageName,\n",
    "    \"Objects Found\": r.objectCount,\n",
    "    \"Unique Labels\": r.labels.join(\", \") || \"Error\"\n",
    "})));\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example 5: Using VLM Run SDK Image Predictions API\n",
    "\n",
    "The VLM Run SDK also provides a dedicated `image` API for predictions. Let's explore this approach:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úì VLM Run main API client initialized!\n",
      "\n",
      "üì∑ Using VLM Run Image API for detection...\n",
      "\n",
      "Image Prediction Response:\n",
      "{\n",
      "  \"usage\": {\n",
      "    \"elements_processed\": 1,\n",
      "    \"element_type\": \"image\",\n",
      "    \"credits_used\": 4,\n",
      "    \"steps\": null,\n",
      "    \"message\": null,\n",
      "    \"duration_seconds\": 0\n",
      "  },\n",
      "  \"id\": \"285d6122-aea4-4a9c-85d4-cac02a8c7e27\",\n",
      "  \"created_at\": \"2025-12-18T14:06:00.827583\",\n",
      "  \"completed_at\": \"2025-12-18T14:06:21.396589Z\",\n",
      "  \"response\": {\n",
      "    \"content\": \"A busy New York City street scene shows several pedestrians crossing the street on a <Object id='obj-15'/>. On the left, a <Object id='obj-0'/> wearing a backpack walks away from the viewer, with a red <Object id='obj-2'/> parked behind him. A bright yellow <Object id='obj-1'/> is visible in the center, next to a partially obscured <Object id='obj-8'/>. To the right, a <Object id='obj-3'/> in a light blue shirt walks towards the right, alongside a <Object id='obj-4'/>. A <Object id='obj-5'/> is seen pushing a <Object id='obj-6'/> which carries another <Object id='obj-7'/>. In the background, numerous vehicles are stopped in traffic, including a silver <Object id='obj-9'/>, a red <Object id='obj-10'/>, a dark <Object id='obj-11'/>, and a white <Object id='obj-12'/>. Tall <Object id='obj-13'/> and <Object id='obj-14'/> line both sides of the street.\",\n",
      "    \"content_metadata\": null,\n",
      "    \"man-1_page0\": \"man-1\",\n",
      "    \"man-1_page0_metadata\": {\n",
      "      \"confidence\": \"hi\",\n",
      "      \"bboxes\": [\n",
      "        {\n",
      "          \"content\": \"man-1\",\n",
      "          \"bbox\": {\n",
      "            \"xywh\": [\n",
      "              0.14,\n",
      "              0.388,\n",
      "              0.106,\n",
      "              0.443\n",
      "            ]\n",
      "          },\n",
      "          \"page\": 0,\n",
      "          \"confidence\": null\n",
      "        }\n",
      "      ]\n",
      "    },\n",
      "    \"taxi-1_page0\": \"taxi-1\",\n",
      "    \"taxi-1_page0_metadata\": {\n",
      "      \"confidence\": \"hi\",\n",
      "      \"bboxes\": [\n",
      "        {\n",
      "          \"content\": \"taxi-1\",\n",
      "          \"bbox\": {\n",
      "            \"xywh\": [\n",
      "              0.269,\n",
      "              0.426,\n",
      "              0.142,\n",
      "              0.286\n",
      "            ]\n",
      "          },\n",
      "          \"page\": 0,\n",
      "          \"confidence\": null\n",
      "        }\n",
      "      ]\n",
      "    },\n",
      "    \"suv-1_page0\": \"suv-1\",\n",
      "    \"suv-1_page0_metadata\": {\n",
      "      \"confidence\": \"hi\",\n",
      "      \"bboxes\": [\n",
      "        {\n",
      "          \"content\": \"suv-1\",\n",
      "          \"bbox\": {\n",
      "            \"xywh\": [\n",
      "              0,\n",
      "              0.441,\n",
      "              0.142,\n",
      "              0.274\n",
      "            ]\n",
      "          },\n",
      "          \"page\": 0,\n",
      "          \"confidence\": null\n",
      "        }\n",
      "      ]\n",
      "    },\n",
      "    \"man-2_page0\": \"man-2\",\n",
      "    \"man-2_page0_metadata\": {\n",
      "      \"confidence\": \"hi\",\n",
      "      \"bboxes\": [\n",
      "        {\n",
      "          \"content\": \"man-2\",\n",
      "          \"bbox\": {\n",
      "            \"xywh\": [\n",
      "              0.545,\n",
      "              0.365,\n",
      "              0.21,\n",
      "              0.581\n",
      "            ]\n",
      "          },\n",
      "          \"page\": 0,\n",
      "          \"confidence\": null\n",
      "        }\n",
      "      ]\n",
      "    },\n",
      "    \"woman-1_page0\": \"woman-1\",\n",
      "    \"woman-1_page0_metadata\": {\n",
      "      \"confidence\": \"hi\",\n",
      "      \"bboxes\": [\n",
      "        {\n",
      "          \"content\": \"woman-1\",\n",
      "          \"bbox\": {\n",
      "            \"xywh\": [\n",
      "              0.549,\n",
      "              0.472,\n",
      "              0.11,\n",
      "              0.415\n",
      "            ]\n",
      "          },\n",
      "          \"page\": 0,\n",
      "          \"confidence\": null\n",
      "        }\n",
      "      ]\n",
      "    },\n",
      "    \"child-1_page0\": \"child-1\",\n",
      "    \"child-1_page0_metadata\": {\n",
      "      \"confidence\": \"hi\",\n",
      "      \"bboxes\": [\n",
      "        {\n",
      "          \"content\": \"child-1\",\n",
      "          \"bbox\": {\n",
      "            \"xywh\": [\n",
      "              0.513,\n",
      "              0.617,\n",
      "              0.084,\n",
      "              0.275\n",
      "            ]\n",
      "          },\n",
      "          \"page\": 0,\n",
      "          \"confidence\": null\n",
      "        }\n",
      "      ]\n",
      "    },\n",
      "    \"stroller-1_page0\": \"stroller-1\",\n",
      "    \"stroller-1_page0_metadata\": {\n",
      "      \"confidence\": \"hi\",\n",
      "      \"bboxes\": [\n",
      "        {\n",
      "          \"content\": \"stroller-1\",\n",
      "          \"bbox\": {\n",
      "            \"xywh\": [\n",
      "              0.396,\n",
      "              0.684,\n",
      "              0.125,\n",
      "              0.219\n",
      "            ]\n",
      "          },\n",
      "          \"page\": 0,\n",
      "          \"confidence\": null\n",
      "        }\n",
      "      ]\n",
      "    },\n",
      "    \"child-2_page0\": \"child-2\",\n",
      "    \"child-2_page0_metadata\": {\n",
      "      \"confidence\": \"hi\",\n",
      "      \"bboxes\": [\n",
      "        {\n",
      "          \"content\": \"child-2\",\n",
      "          \"bbox\": {\n",
      "            \"xywh\": [\n",
      "              0.442,\n",
      "              0.735,\n",
      "              0.059,\n",
      "              0.118\n",
      "            ]\n",
      "          },\n",
      "          \"page\": 0,\n",
      "          \"confidence\": null\n",
      "        }\n",
      "      ]\n",
      "    },\n",
      "    \"man-3_page0\": \"man-3\",\n",
      "    \"man-3_page0_metadata\": {\n",
      "      \"confidence\": \"hi\",\n",
      "      \"bboxes\": [\n",
      "        {\n",
      "          \"content\": \"man-3\",\n",
      "          \"bbox\": {\n",
      "            \"xywh\": [\n",
      "              0.39,\n",
      "              0.416,\n",
      "              0.071,\n",
      "              0.256\n",
      "            ]\n",
      "          },\n",
      "          \"page\": 0,\n",
      "          \"confidence\": null\n",
      "        }\n",
      "      ]\n",
      "    },\n",
      "    \"sedan-1_page0\": \"sedan-1\",\n",
      "    \"sedan-1_page0_metadata\": {\n",
      "      \"confidence\": \"hi\",\n",
      "      \"bboxes\": [\n",
      "        {\n",
      "          \"content\": \"sedan-1\",\n",
      "          \"bbox\": {\n",
      "            \"xywh\": [\n",
      "              0.471,\n",
      "              0.438,\n",
      "              0.11,\n",
      "              0.062\n",
      "            ]\n",
      "          },\n",
      "          \"page\": 0,\n",
      "          \"confidence\": null\n",
      "        }\n",
      "      ]\n",
      "    },\n",
      "    \"car-1_page0\": \"car-1\",\n",
      "    \"car-1_page0_metadata\": {\n",
      "      \"confidence\": \"hi\",\n",
      "      \"bboxes\": [\n",
      "        {\n",
      "          \"content\": \"car-1\",\n",
      "          \"bbox\": {\n",
      "            \"xywh\": [\n",
      "              0.443,\n",
      "              0.491,\n",
      "              0.107,\n",
      "              0.099\n",
      "            ]\n",
      "          },\n",
      "          \"page\": 0,\n",
      "          \"confidence\": null\n",
      "        }\n",
      "      ]\n",
      "    },\n",
      "    \"suv-2_page0\": \"suv-2\",\n",
      "    \"suv-2_page0_metadata\": {\n",
      "      \"confidence\": \"hi\",\n",
      "      \"bboxes\": [\n",
      "        {\n",
      "          \"content\": \"suv-2\",\n",
      "          \"bbox\": {\n",
      "            \"xywh\": [\n",
      "              0.594,\n",
      "              0.496,\n",
      "              0.12,\n",
      "              0.169\n",
      "            ]\n",
      "          },\n",
      "          \"page\": 0,\n",
      "          \"confidence\": null\n",
      "        }\n",
      "      ]\n",
      "    },\n",
      "    \"van-1_page0\": \"van-1\",\n",
      "    \"van-1_page0_metadata\": {\n",
      "      \"confidence\": \"hi\",\n",
      "      \"bboxes\": [\n",
      "        {\n",
      "          \"content\": \"van-1\",\n",
      "          \"bbox\": {\n",
      "            \"xywh\": [\n",
      "              0.762,\n",
      "              0.497,\n",
      "              0.175,\n",
      "              0.153\n",
      "            ]\n",
      "          },\n",
      "          \"page\": 0,\n",
      "          \"confidence\": null\n",
      "        }\n",
      "      ]\n",
      "    },\n",
      "    \"buildings-1_page0\": \"buildings-1\",\n",
      "    \"buildings-1_page0_metadata\": {\n",
      "      \"confidence\": \"hi\",\n",
      "      \"bboxes\": [\n",
      "        {\n",
      "          \"content\": \"buildings-1\",\n",
      "          \"bbox\": {\n",
      "            \"xywh\": [\n",
      "              0,\n",
      "              0,\n",
      "              0.412,\n",
      "              0.529\n",
      "            ]\n",
      "          },\n",
      "          \"page\": 0,\n",
      "          \"confidence\": null\n",
      "        }\n",
      "      ]\n",
      "    },\n",
      "    \"buildings-2_page0\": \"buildings-2\",\n",
      "    \"buildings-2_page0_metadata\": {\n",
      "      \"confidence\": \"hi\",\n",
      "      \"bboxes\": [\n",
      "        {\n",
      "          \"content\": \"buildings-2\",\n",
      "          \"bbox\": {\n",
      "            \"xywh\": [\n",
      "              0.497,\n",
      "              0,\n",
      "              0.503,\n",
      "              0.499\n",
      "            ]\n",
      "          },\n",
      "          \"page\": 0,\n",
      "          \"confidence\": null\n",
      "        }\n",
      "      ]\n",
      "    },\n",
      "    \"crosswalk-1_page0\": \"crosswalk-1\",\n",
      "    \"crosswalk-1_page0_metadata\": {\n",
      "      \"confidence\": \"hi\",\n",
      "      \"bboxes\": [\n",
      "        {\n",
      "          \"content\": \"crosswalk-1\",\n",
      "          \"bbox\": {\n",
      "            \"xywh\": [\n",
      "              0,\n",
      "              0.722,\n",
      "              0.999,\n",
      "              0.277\n",
      "            ]\n",
      "          },\n",
      "          \"page\": 0,\n",
      "          \"confidence\": null\n",
      "        }\n",
      "      ]\n",
      "    }\n",
      "  },\n",
      "  \"status\": \"completed\",\n",
      "  \"domain\": \"image.object-detection\"\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "// Initialize a separate client for the main API (not agent API)\n",
    "const vlmClient = new VlmRun({\n",
    "    apiKey: VLMRUN_API_KEY,\n",
    "    baseURL: \"https://api.vlm.run/v1\"  // Main API endpoint\n",
    "});\n",
    "\n",
    "console.log(\"‚úì VLM Run main API client initialized!\");\n",
    "\n",
    "// Using the image predictions API\n",
    "console.log(\"\\nüì∑ Using VLM Run Image API for detection...\\n\");\n",
    "\n",
    "try {\n",
    "    // Generate detection using the image.generate method\n",
    "    const imagePrediction = await vlmClient.image.generate({\n",
    "        images: [streetImageUrl],\n",
    "        model: \"vlm-1\",\n",
    "        domain: \"image.object-detection\",\n",
    "        config: {\n",
    "            jsonSchema: detectionsJsonSchema\n",
    "        }\n",
    "    });\n",
    "    \n",
    "    console.log(\"Image Prediction Response:\");\n",
    "    console.log(JSON.stringify(imagePrediction, null, 2));\n",
    "} catch (error) {\n",
    "    console.log(\"Note: Image predictions API requires specific domain access.\");\n",
    "    console.log(\"For general detection, use the agent.completions API as shown in previous examples.\");\n",
    "    console.log(\"Error:\", error);\n",
    "}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## VLM Run SDK Features\n",
    "\n",
    "The VLM Run SDK (`vlmrun` npm package) provides several advantages over using the OpenAI client directly:\n",
    "\n",
    "### Key Features\n",
    "\n",
    "| Feature | Description |\n",
    "|---------|-------------|\n",
    "| `agent.completions` | OpenAI-compatible chat completions for agent API |\n",
    "| `image.generate()` | Direct image prediction API access |\n",
    "| `document.generate()` | Document processing capabilities |\n",
    "| `audio.generate()` | Audio transcription and analysis |\n",
    "| `video.generate()` | Video understanding and analysis |\n",
    "| `files.upload()` | File upload for processing |\n",
    "| `hub.list()` | Browse available models and domains |\n",
    "| `models.list()` | List available models |\n",
    "\n",
    "### SDK Initialization Options\n",
    "\n",
    "```typescript\n",
    "const client = new VlmRun({\n",
    "    apiKey: \"your-api-key\",\n",
    "    baseURL: \"https://api.vlm.run/v1\",  // or \"https://agent.vlm.run/v1\"\n",
    "    timeout: 60000,  // Request timeout in ms\n",
    "    maxRetries: 3    // Number of retries for failed requests\n",
    "});\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Understanding Bounding Box Format\n",
    "\n",
    "VLM Run returns bounding boxes in **normalized xywh format**:\n",
    "\n",
    "| Field | Description | Range |\n",
    "|-------|-------------|-------|\n",
    "| `x` | Left edge of bounding box | 0.0 - 1.0 |\n",
    "| `y` | Top edge of bounding box | 0.0 - 1.0 |\n",
    "| `w` | Width of bounding box | 0.0 - 1.0 |\n",
    "| `h` | Height of bounding box | 0.0 - 1.0 |\n",
    "\n",
    "To convert to pixel coordinates, multiply by image dimensions:\n",
    "```typescript\n",
    "const x_px = x * imageWidth;\n",
    "const y_px = y * imageHeight;\n",
    "const w_px = w * imageWidth;\n",
    "const h_px = h * imageHeight;\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Best Practices & Tips\n",
    "\n",
    "### Detection Quality\n",
    "- Use specific prompts for better results (e.g., \"Detect all cars\" vs \"Detect objects\")\n",
    "- For crowded scenes, consider detecting specific object types separately\n",
    "- Use higher resolution images for better small object detection\n",
    "\n",
    "### SDK Usage\n",
    "- Use `agent.completions` for OpenAI-compatible chat interface with image support\n",
    "- Use `image.generate()` for domain-specific image predictions\n",
    "- Set appropriate timeouts for large image processing\n",
    "- Use retry logic for production applications\n",
    "\n",
    "### Supported Object Classes\n",
    "VLM Run can detect 80+ COCO dataset classes including:\n",
    "- **People**: person, face\n",
    "- **Vehicles**: car, truck, bus, motorcycle, bicycle, airplane, boat\n",
    "- **Animals**: cat, dog, bird, horse, cow, sheep\n",
    "- **Objects**: chair, table, laptop, phone, book, bottle, cup\n",
    "\n",
    "### Performance Tips\n",
    "- Batch similar detection tasks for efficiency\n",
    "- Cache detection results for frequently analyzed images\n",
    "- Use appropriate image sizes (800-1200px width is usually sufficient)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Additional Resources\n",
    "\n",
    "- [VLM Run Documentation](https://docs.vlm.run)\n",
    "- [VLM Run Node.js SDK](https://github.com/vlm-run/vlmrun-node-sdk)\n",
    "- [SDK API Reference](https://docs.vlm.run/sdks/node-sdk)\n",
    "- [Detection API Reference](https://docs.vlm.run/agents/capabilities/image/detection)\n",
    "- [More Examples](https://github.com/vlm-run/vlmrun-cookbook)\n",
    "- [Discord Community](https://discord.gg/AMApC2UzVY)\n",
    "\n",
    "### Installation\n",
    "\n",
    "```bash\n",
    "npm install vlmrun openai zod zod-to-json-schema\n",
    "```\n",
    "\n",
    "### Quick Start\n",
    "\n",
    "```typescript\n",
    "import { VlmRun } from \"vlmrun\";\n",
    "import { z } from \"zod\";\n",
    "import { zodToJsonSchema } from \"zod-to-json-schema\";\n",
    "\n",
    "const client = new VlmRun({\n",
    "    apiKey: process.env.VLMRUN_API_KEY,\n",
    "    baseURL: \"https://agent.vlm.run/v1\"\n",
    "});\n",
    "\n",
    "const DetectionsSchema = z.object({\n",
    "    detections: z.array(z.object({\n",
    "        label: z.string(),\n",
    "        xywh: z.tuple([z.number(), z.number(), z.number(), z.number()])\n",
    "    }))\n",
    "});\n",
    "\n",
    "const response = await client.agent.completions.create({\n",
    "    model: \"vlm-agent-1\",\n",
    "    messages: [\n",
    "        {\n",
    "            role: \"user\",\n",
    "            content: [\n",
    "                { type: \"text\", text: \"Detect all objects in this image\" },\n",
    "                { type: \"image_url\", image_url: { url: \"https://example.com/image.jpg\" } }\n",
    "            ]\n",
    "        }\n",
    "    ],\n",
    "    response_format: { \n",
    "        type: \"json_schema\", \n",
    "        schema: zodToJsonSchema(DetectionsSchema)\n",
    "    }\n",
    "});\n",
    "\n",
    "console.log(JSON.parse(response.choices[0].message.content));\n",
    "```\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Deno",
   "language": "typescript",
   "name": "deno"
  },
  "language_info": {
   "codemirror_mode": "typescript",
   "file_extension": ".ts",
   "mimetype": "text/x.typescript",
   "name": "typescript",
   "nbconvert_exporter": "script",
   "pygments_lexer": "typescript",
   "version": "5.9.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
