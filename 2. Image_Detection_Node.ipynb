{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "<div align=\"center\">\n",
        "<p align=\"center\" style=\"width: 100%;\">\n",
        "    <img src=\"https://raw.githubusercontent.com/vlm-run/.github/refs/heads/main/profile/assets/vlm-black.svg\" alt=\"VLM Run Logo\" width=\"80\" style=\"margin-bottom: -5px; color: #2e3138; vertical-align: middle; padding-right: 5px;\"><br>\n",
        "</p>\n",
        "<p align=\"center\"><a href=\"https://docs.vlm.run\"><b>Website</b></a> | <a href=\"https://docs.vlm.run/\"><b>API Docs</b></a> | <a href=\"https://docs.vlm.run/blog\"><b>Blog</b></a> | <a href=\"https://discord.gg/AMApC2UzVY\"><b>Discord</b></a>\n",
        "</p>\n",
        "<p align=\"center\">\n",
        "<a href=\"https://discord.gg/AMApC2UzVY\"><img alt=\"Discord\" src=\"https://img.shields.io/badge/discord-chat-purple?color=%235765F2&label=discord&logo=discord\"></a>\n",
        "<a href=\"https://twitter.com/vlmrun\"><img alt=\"Twitter Follow\" src=\"https://img.shields.io/twitter/follow/vlmrun.svg?style=social&logo=twitter\"></a>\n",
        "</p>\n",
        "</div>\n",
        "\n",
        "Welcome to **[VLM Run Cookbooks](https://github.com/vlm-run/vlmrun-cookbook)**, a comprehensive collection of examples and notebooks demonstrating the power of structured visual understanding using the [VLM Run Platform](https://app.vlm.run).\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Case Study: Object Detection with VLM Run (Node.js)\n",
        "\n",
        "This notebook demonstrates how to use VLM Run's detection capabilities to identify and locate objects, people, and faces in images using **Node.js/TypeScript**. We'll cover:\n",
        "\n",
        "- **Object Detection**: Detect 80+ COCO dataset classes (person, car, cat, etc.)\n",
        "- **Person Detection**: Specialized detection for people in images\n",
        "- **Face Detection**: Detect and locate faces with high precision\n",
        "\n",
        "All detections return bounding boxes in normalized `xywh` format (x, y, width, height) where values are between 0-1, along with labels.\n",
        "\n",
        "Reference: [VLM Run Detection Documentation](https://docs.vlm.run/agents/capabilities/image/detection)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Environment Setup\n",
        "\n",
        "To get started, install the required packages and sign up for an API key on the [VLM Run App](https://app.vlm.run).\n",
        "- Store the VLM Run API key under the `VLMRUN_API_KEY` environment variable.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Prerequisites\n",
        "\n",
        "* Node.js 18+\n",
        "* VLM Run API key (get one at [app.vlm.run](https://app.vlm.run))\n",
        "* Deno or tslab kernel for running TypeScript in Jupyter\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Setup\n",
        "\n",
        "First, let's install the required packages:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "// If using Deno kernel, install dependencies via npm specifiers\n",
        "// For tslab, run: npm install openai zod zod-to-json-schema in your project directory\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [],
      "source": [
        "import OpenAI from \"openai\";\n",
        "import { z } from \"zod\";\n",
        "import { zodToJsonSchema } from \"npm:zod-to-json-schema\";\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "âœ“ API Key loaded successfully\n"
          ]
        }
      ],
      "source": [
        "// Get API key from environment variable\n",
        "const VLMRUN_API_KEY = \"7R8aQ5XlH00LlGdlpe_SLywfsAwszz\";\n",
        "\n",
        "if (!VLMRUN_API_KEY) {\n",
        "    throw new Error(\"Please set the VLMRUN_API_KEY environment variable\");\n",
        "}\n",
        "\n",
        "console.log(\"âœ“ API Key loaded successfully\");\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Let's initialize the OpenAI client configured for VLM Run\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "âœ“ VLM Run client initialized!\n"
          ]
        }
      ],
      "source": [
        "// Initialize the OpenAI client with VLM Run's Agent API\n",
        "const client = new OpenAI({\n",
        "    baseURL: \"https://agent.vlm.run/v1/openai\",\n",
        "    apiKey: VLMRUN_API_KEY\n",
        "});\n",
        "\n",
        "console.log(\"âœ“ VLM Run client initialized!\");\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Define Detection Schemas\n",
        "\n",
        "We'll define Zod schemas for the detection responses (TypeScript equivalent of Python's Pydantic):\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "âœ“ Detection schemas defined\n",
            "JSON Schema: {\n",
            "  \"type\": \"object\",\n",
            "  \"properties\": {\n",
            "    \"detections\": {\n",
            "      \"type\": \"array\",\n",
            "      \"items\": {\n",
            "        \"type\": \"object\",\n",
            "        \"properties\": {\n",
            "          \"label\": {\n",
            "            \"type\": \"string\",\n",
            "            \"description\": \"Name of the detected object\"\n",
            "          },\n",
            "          \"xywh\": {\n",
            "            \"type\": \"array\",\n",
            "            \"minItems\": 4,\n",
            "            \"maxItems\": 4,\n",
            "            \"items\": [\n",
            "              {\n",
            "                \"type\": \"number\"\n",
            "              },\n",
            "              {\n",
            "                \"type\": \"number\"\n",
            "              },\n",
            "              {\n",
            "                \"type\": \"number\"\n",
            "              },\n",
            "              {\n",
            "                \"type\": \"number\"\n",
            "              }\n",
            "            ],\n",
            "            \"description\": \"Bounding box (x, y, width, height) - normalized values 0-1\"\n",
            "          }\n",
            "        },\n",
            "        \"required\": [\n",
            "          \"label\",\n",
            "          \"xywh\"\n",
            "        ],\n",
            "        \"additionalProperties\": false\n",
            "      },\n",
            "      \"description\": \"List of detected objects\"\n",
            "    }\n",
            "  },\n",
            "  \"required\": [\n",
            "    \"detections\"\n",
            "  ],\n",
            "  \"additionalProperties\": false,\n",
            "  \"$schema\": \"http://json-schema.org/draft-07/schema#\"\n",
            "}\n"
          ]
        }
      ],
      "source": [
        "// Define the Detection schema using Zod\n",
        "const DetectionSchema = z.object({\n",
        "    label: z.string().describe(\"Name of the detected object\"),\n",
        "    xywh: z.tuple([z.number(), z.number(), z.number(), z.number()])\n",
        "        .describe(\"Bounding box (x, y, width, height) - normalized values 0-1\")\n",
        "});\n",
        "\n",
        "// Define the Detections response schema\n",
        "const DetectionsSchema = z.object({\n",
        "    detections: z.array(DetectionSchema).describe(\"List of detected objects\")\n",
        "});\n",
        "\n",
        "// Type inference from schemas\n",
        "type Detection = z.infer<typeof DetectionSchema>;\n",
        "type Detections = z.infer<typeof DetectionsSchema>;\n",
        "\n",
        "// Convert Zod schema to JSON Schema for API\n",
        "const detectionsJsonSchema = zodToJsonSchema(DetectionsSchema);\n",
        "console.log(\"âœ“ Detection schemas defined\");\n",
        "console.log(\"JSON Schema:\", JSON.stringify(detectionsJsonSchema, null, 2));\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Helper Functions\n",
        "\n",
        "Let's create utility functions for detection and displaying results:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "âœ“ Helper functions defined\n"
          ]
        }
      ],
      "source": [
        "/**\n",
        " * Perform object detection on an image using VLM Run's Agent API.\n",
        " * \n",
        " * @param imageUrl - URL of the image to analyze\n",
        " * @param prompt - Detection prompt (e.g., \"Detect all objects\", \"Detect all people\")\n",
        " * @returns Detections object with bounding boxes\n",
        " */\n",
        "async function detectObjects(imageUrl: string, prompt: string = \"Detect all objects in this image\"): Promise<Detections> {\n",
        "    const response = await client.chat.completions.create({\n",
        "        model: \"vlm-agent-1\",\n",
        "        messages: [\n",
        "            {\n",
        "                role: \"user\",\n",
        "                content: [\n",
        "                    { type: \"text\", text: prompt },\n",
        "                    { type: \"image_url\", image_url: { url: imageUrl, detail: \"auto\" } }\n",
        "                ]\n",
        "            }\n",
        "        ],\n",
        "        // Use VLM Run's format - schema at top level\n",
        "        response_format: { \n",
        "            type: \"json_schema\", \n",
        "            schema: detectionsJsonSchema\n",
        "        } as any\n",
        "    });\n",
        "    \n",
        "    const rawContent = response.choices[0].message.content || \"{}\";\n",
        "    const parsed = JSON.parse(rawContent);\n",
        "    return DetectionsSchema.parse(parsed);\n",
        "}\n",
        "\n",
        "/**\n",
        " * Format detection results for display.\n",
        " * \n",
        " * @param detections - Array of Detection objects\n",
        " * @param imageWidth - Width of image in pixels (for coordinate display)\n",
        " * @param imageHeight - Height of image in pixels (for coordinate display)\n",
        " */\n",
        "function formatDetections(detections: Detection[], imageWidth?: number, imageHeight?: number): void {\n",
        "    console.log(`\\nğŸ“¦ Found ${detections.length} objects:\\n`);\n",
        "    \n",
        "    detections.forEach((det, i) => {\n",
        "        const [x, y, w, h] = det.xywh;\n",
        "        \n",
        "        if (imageWidth && imageHeight) {\n",
        "            // Convert normalized coordinates to pixel coordinates\n",
        "            const x_px = Math.round(x * imageWidth);\n",
        "            const y_px = Math.round(y * imageHeight);\n",
        "            const w_px = Math.round(w * imageWidth);\n",
        "            const h_px = Math.round(h * imageHeight);\n",
        "            console.log(`  ${i + 1}. ${det.label}`);\n",
        "            console.log(`     Normalized: x=${x.toFixed(3)}, y=${y.toFixed(3)}, w=${w.toFixed(3)}, h=${h.toFixed(3)}`);\n",
        "            console.log(`     Pixels:     x=${x_px}, y=${y_px}, w=${w_px}, h=${h_px}`);\n",
        "        } else {\n",
        "            console.log(`  ${i + 1}. ${det.label}: xywh=[${x.toFixed(3)}, ${y.toFixed(3)}, ${w.toFixed(3)}, ${h.toFixed(3)}]`);\n",
        "        }\n",
        "    });\n",
        "}\n",
        "\n",
        "/**\n",
        " * Group detections by label and count them.\n",
        " * \n",
        " * @param detections - Array of Detection objects\n",
        " * @returns Object with label counts\n",
        " */\n",
        "function countByLabel(detections: Detection[]): Record<string, number> {\n",
        "    return detections.reduce((acc, det) => {\n",
        "        acc[det.label] = (acc[det.label] || 0) + 1;\n",
        "        return acc;\n",
        "    }, {} as Record<string, number>);\n",
        "}\n",
        "\n",
        "console.log(\"âœ“ Helper functions defined\");\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Example 1: Person Detection\n",
        "\n",
        "Let's detect people in an image of a group.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "ğŸ“· Image URL: https://images.unsplash.com/photo-1511632765486-a01980e01a18?w=800\n",
            "\n",
            "Detecting people in the image...\n",
            "\n",
            "Found 4 people in the image:\n",
            "  Person 1: person, xywh=[0.325, 0.266, 0.125, 0.619]\n",
            "  Person 2: person, xywh=[0.400, 0.271, 0.148, 0.619]\n",
            "  Person 3: person, xywh=[0.487, 0.271, 0.144, 0.619]\n",
            "  Person 4: person, xywh=[0.592, 0.237, 0.113, 0.648]\n",
            "\n",
            "ğŸ“Š Detection Summary:\n",
            "â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
            "â”‚ (idx)  â”‚ Values â”‚\n",
            "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
            "â”‚ person â”‚      4 â”‚\n",
            "â””â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n"
          ]
        }
      ],
      "source": [
        "// Image with people\n",
        "const peopleImageUrl = \"https://images.unsplash.com/photo-1511632765486-a01980e01a18?w=800\";\n",
        "\n",
        "console.log(\"ğŸ“· Image URL:\", peopleImageUrl);\n",
        "console.log(\"\\nDetecting people in the image...\\n\");\n",
        "\n",
        "// Perform person detection\n",
        "const personResult = await detectObjects(peopleImageUrl, \"Detect all people in this image\");\n",
        "\n",
        "// Display results\n",
        "console.log(`Found ${personResult.detections.length} people in the image:`);\n",
        "personResult.detections.forEach((det, i) => {\n",
        "    console.log(`  Person ${i + 1}: ${det.label}, xywh=[${det.xywh.map(v => v.toFixed(3)).join(\", \")}]`);\n",
        "});\n",
        "\n",
        "// Show summary\n",
        "const labelCounts = countByLabel(personResult.detections);\n",
        "console.log(\"\\nğŸ“Š Detection Summary:\");\n",
        "console.table(labelCounts);\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Example 2: General Object Detection\n",
        "\n",
        "Let's detect various objects in a street scene.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "ğŸ“· Image URL: https://storage.googleapis.com/vlm-data-public-prod/hub/examples/image.object-detection/crossroad.jpg\n",
            "\n",
            "Detecting all objects in the street scene...\n",
            "\n",
            "\n",
            "ğŸ“¦ Found 161 objects:\n",
            "\n",
            "  1. car\n",
            "     Normalized: x=0.000, y=0.445, w=0.145, h=0.252\n",
            "     Pixels:     x=0, y=267, w=116, h=151\n",
            "  2. car\n",
            "     Normalized: x=0.219, y=0.408, w=0.183, h=0.312\n",
            "     Pixels:     x=175, y=245, w=146, h=187\n",
            "  3. car\n",
            "     Normalized: x=0.478, y=0.432, w=0.107, h=0.073\n",
            "     Pixels:     x=382, y=259, w=86, h=44\n",
            "  4. car\n",
            "     Normalized: x=0.686, y=0.456, w=0.077, h=0.075\n",
            "     Pixels:     x=549, y=274, w=62, h=45\n",
            "  5. car\n",
            "     Normalized: x=0.756, y=0.442, w=0.080, h=0.115\n",
            "     Pixels:     x=605, y=265, w=64, h=69\n",
            "  6. car\n",
            "     Normalized: x=0.820, y=0.436, w=0.088, h=0.148\n",
            "     Pixels:     x=656, y=262, w=70, h=89\n",
            "  7. car\n",
            "     Normalized: x=0.897, y=0.430, w=0.103, h=0.196\n",
            "     Pixels:     x=718, y=258, w=82, h=118\n",
            "  8. car\n",
            "     Normalized: x=0.585, y=0.427, w=0.043, h=0.037\n",
            "     Pixels:     x=468, y=256, w=34, h=22\n",
            "  9. car\n",
            "     Normalized: x=0.550, y=0.413, w=0.024, h=0.014\n",
            "     Pixels:     x=440, y=248, w=19, h=8\n",
            "  10. car\n",
            "     Normalized: x=0.534, y=0.413, w=0.016, h=0.014\n",
            "     Pixels:     x=427, y=248, w=13, h=8\n",
            "  11. car\n",
            "     Normalized: x=0.514, y=0.413, w=0.020, h=0.014\n",
            "     Pixels:     x=411, y=248, w=16, h=8\n",
            "  12. car\n",
            "     Normalized: x=0.496, y=0.413, w=0.018, h=0.014\n",
            "     Pixels:     x=397, y=248, w=14, h=8\n",
            "  13. car\n",
            "     Normalized: x=0.478, y=0.413, w=0.018, h=0.014\n",
            "     Pixels:     x=382, y=248, w=14, h=8\n",
            "  14. car\n",
            "     Normalized: x=0.464, y=0.413, w=0.014, h=0.014\n",
            "     Pixels:     x=371, y=248, w=11, h=8\n",
            "  15. car\n",
            "     Normalized: x=0.447, y=0.413, w=0.017, h=0.014\n",
            "     Pixels:     x=358, y=248, w=14, h=8\n",
            "  16. car\n",
            "     Normalized: x=0.428, y=0.413, w=0.019, h=0.014\n",
            "     Pixels:     x=342, y=248, w=15, h=8\n",
            "  17. car\n",
            "     Normalized: x=0.411, y=0.413, w=0.017, h=0.014\n",
            "     Pixels:     x=329, y=248, w=14, h=8\n",
            "  18. car\n",
            "     Normalized: x=0.387, y=0.413, w=0.024, h=0.014\n",
            "     Pixels:     x=310, y=248, w=19, h=8\n",
            "  19. car\n",
            "     Normalized: x=0.367, y=0.413, w=0.020, h=0.014\n",
            "     Pixels:     x=294, y=248, w=16, h=8\n",
            "  20. car\n",
            "     Normalized: x=0.354, y=0.413, w=0.013, h=0.014\n",
            "     Pixels:     x=283, y=248, w=10, h=8\n",
            "  21. car\n",
            "     Normalized: x=0.342, y=0.413, w=0.012, h=0.014\n",
            "     Pixels:     x=274, y=248, w=10, h=8\n",
            "  22. car\n",
            "     Normalized: x=0.330, y=0.413, w=0.012, h=0.014\n",
            "     Pixels:     x=264, y=248, w=10, h=8\n",
            "  23. car\n",
            "     Normalized: x=0.318, y=0.413, w=0.012, h=0.014\n",
            "     Pixels:     x=254, y=248, w=10, h=8\n",
            "  24. car\n",
            "     Normalized: x=0.307, y=0.413, w=0.011, h=0.014\n",
            "     Pixels:     x=246, y=248, w=9, h=8\n",
            "  25. car\n",
            "     Normalized: x=0.296, y=0.413, w=0.011, h=0.014\n",
            "     Pixels:     x=237, y=248, w=9, h=8\n",
            "  26. car\n",
            "     Normalized: x=0.285, y=0.413, w=0.011, h=0.014\n",
            "     Pixels:     x=228, y=248, w=9, h=8\n",
            "  27. car\n",
            "     Normalized: x=0.274, y=0.413, w=0.011, h=0.014\n",
            "     Pixels:     x=219, y=248, w=9, h=8\n",
            "  28. car\n",
            "     Normalized: x=0.263, y=0.413, w=0.011, h=0.014\n",
            "     Pixels:     x=210, y=248, w=9, h=8\n",
            "  29. car\n",
            "     Normalized: x=0.252, y=0.413, w=0.011, h=0.014\n",
            "     Pixels:     x=202, y=248, w=9, h=8\n",
            "  30. car\n",
            "     Normalized: x=0.241, y=0.413, w=0.011, h=0.014\n",
            "     Pixels:     x=193, y=248, w=9, h=8\n",
            "  31. car\n",
            "     Normalized: x=0.230, y=0.413, w=0.011, h=0.014\n",
            "     Pixels:     x=184, y=248, w=9, h=8\n",
            "  32. car\n",
            "     Normalized: x=0.219, y=0.413, w=0.011, h=0.014\n",
            "     Pixels:     x=175, y=248, w=9, h=8\n",
            "  33. car\n",
            "     Normalized: x=0.208, y=0.413, w=0.011, h=0.014\n",
            "     Pixels:     x=166, y=248, w=9, h=8\n",
            "  34. car\n",
            "     Normalized: x=0.197, y=0.413, w=0.011, h=0.014\n",
            "     Pixels:     x=158, y=248, w=9, h=8\n",
            "  35. car\n",
            "     Normalized: x=0.186, y=0.413, w=0.011, h=0.014\n",
            "     Pixels:     x=149, y=248, w=9, h=8\n",
            "  36. car\n",
            "     Normalized: x=0.175, y=0.413, w=0.011, h=0.014\n",
            "     Pixels:     x=140, y=248, w=9, h=8\n",
            "  37. car\n",
            "     Normalized: x=0.164, y=0.413, w=0.011, h=0.014\n",
            "     Pixels:     x=131, y=248, w=9, h=8\n",
            "  38. car\n",
            "     Normalized: x=0.153, y=0.413, w=0.011, h=0.014\n",
            "     Pixels:     x=122, y=248, w=9, h=8\n",
            "  39. car\n",
            "     Normalized: x=0.142, y=0.413, w=0.011, h=0.014\n",
            "     Pixels:     x=114, y=248, w=9, h=8\n",
            "  40. car\n",
            "     Normalized: x=0.131, y=0.413, w=0.011, h=0.014\n",
            "     Pixels:     x=105, y=248, w=9, h=8\n",
            "  41. car\n",
            "     Normalized: x=0.120, y=0.413, w=0.011, h=0.014\n",
            "     Pixels:     x=96, y=248, w=9, h=8\n",
            "  42. car\n",
            "     Normalized: x=0.109, y=0.413, w=0.011, h=0.014\n",
            "     Pixels:     x=87, y=248, w=9, h=8\n",
            "  43. car\n",
            "     Normalized: x=0.098, y=0.413, w=0.011, h=0.014\n",
            "     Pixels:     x=78, y=248, w=9, h=8\n",
            "  44. car\n",
            "     Normalized: x=0.087, y=0.413, w=0.011, h=0.014\n",
            "     Pixels:     x=70, y=248, w=9, h=8\n",
            "  45. car\n",
            "     Normalized: x=0.076, y=0.413, w=0.011, h=0.014\n",
            "     Pixels:     x=61, y=248, w=9, h=8\n",
            "  46. car\n",
            "     Normalized: x=0.065, y=0.413, w=0.011, h=0.014\n",
            "     Pixels:     x=52, y=248, w=9, h=8\n",
            "  47. car\n",
            "     Normalized: x=0.054, y=0.413, w=0.011, h=0.014\n",
            "     Pixels:     x=43, y=248, w=9, h=8\n",
            "  48. car\n",
            "     Normalized: x=0.043, y=0.413, w=0.011, h=0.014\n",
            "     Pixels:     x=34, y=248, w=9, h=8\n",
            "  49. car\n",
            "     Normalized: x=0.032, y=0.413, w=0.011, h=0.014\n",
            "     Pixels:     x=26, y=248, w=9, h=8\n",
            "  50. car\n",
            "     Normalized: x=0.021, y=0.413, w=0.011, h=0.014\n",
            "     Pixels:     x=17, y=248, w=9, h=8\n",
            "  51. car\n",
            "     Normalized: x=0.010, y=0.413, w=0.011, h=0.014\n",
            "     Pixels:     x=8, y=248, w=9, h=8\n",
            "  52. car\n",
            "     Normalized: x=0.000, y=0.413, w=0.010, h=0.014\n",
            "     Pixels:     x=0, y=248, w=8, h=8\n",
            "  53. person\n",
            "     Normalized: x=0.408, y=0.413, w=0.045, h=0.257\n",
            "     Pixels:     x=326, y=248, w=36, h=154\n",
            "  54. person\n",
            "     Normalized: x=0.144, y=0.388, w=0.101, h=0.444\n",
            "     Pixels:     x=115, y=233, w=81, h=266\n",
            "  55. person\n",
            "     Normalized: x=0.550, y=0.391, w=0.202, h=0.557\n",
            "     Pixels:     x=440, y=235, w=162, h=334\n",
            "  56. person\n",
            "     Normalized: x=0.548, y=0.472, w=0.078, h=0.423\n",
            "     Pixels:     x=438, y=283, w=62, h=254\n",
            "  57. person\n",
            "     Normalized: x=0.528, y=0.620, w=0.062, h=0.279\n",
            "     Pixels:     x=422, y=372, w=50, h=167\n",
            "  58. person\n",
            "     Normalized: x=0.411, y=0.727, w=0.095, h=0.119\n",
            "     Pixels:     x=329, y=436, w=76, h=71\n",
            "  59. face\n",
            "     Normalized: x=0.548, y=0.472, w=0.078, h=0.423\n",
            "     Pixels:     x=438, y=283, w=62, h=254\n",
            "  60. face\n",
            "     Normalized: x=0.528, y=0.620, w=0.062, h=0.279\n",
            "     Pixels:     x=422, y=372, w=50, h=167\n",
            "  61. face\n",
            "     Normalized: x=0.550, y=0.391, w=0.202, h=0.557\n",
            "     Pixels:     x=440, y=235, w=162, h=334\n",
            "  62. face\n",
            "     Normalized: x=0.144, y=0.388, w=0.101, h=0.444\n",
            "     Pixels:     x=115, y=233, w=81, h=266\n",
            "  63. face\n",
            "     Normalized: x=0.408, y=0.413, w=0.045, h=0.257\n",
            "     Pixels:     x=326, y=248, w=36, h=154\n",
            "  64. face\n",
            "     Normalized: x=0.411, y=0.727, w=0.095, h=0.119\n",
            "     Pixels:     x=329, y=436, w=76, h=71\n",
            "  65. text\n",
            "     Normalized: x=0.245, y=0.622, w=0.034, h=0.028\n",
            "     Pixels:     x=196, y=373, w=27, h=17\n",
            "  66. text\n",
            "     Normalized: x=0.019, y=0.356, w=0.044, h=0.027\n",
            "     Pixels:     x=15, y=214, w=35, h=16\n",
            "  67. text\n",
            "     Normalized: x=0.763, y=0.117, w=0.018, h=0.113\n",
            "     Pixels:     x=610, y=70, w=14, h=68\n",
            "  68. text\n",
            "     Normalized: x=0.732, y=0.162, w=0.015, h=0.060\n",
            "     Pixels:     x=586, y=97, w=12, h=36\n",
            "  69. text\n",
            "     Normalized: x=0.288, y=0.264, w=0.016, h=0.060\n",
            "     Pixels:     x=230, y=158, w=13, h=36\n",
            "  70. text\n",
            "     Normalized: x=0.419, y=0.147, w=0.015, h=0.087\n",
            "     Pixels:     x=335, y=88, w=12, h=52\n",
            "  71. text\n",
            "     Normalized: x=0.445, y=0.269, w=0.012, h=0.116\n",
            "     Pixels:     x=356, y=161, w=10, h=70\n",
            "  72. text\n",
            "     Normalized: x=0.438, y=0.269, w=0.007, h=0.116\n",
            "     Pixels:     x=350, y=161, w=6, h=70\n",
            "  73. text\n",
            "     Normalized: x=0.445, y=0.269, w=0.012, h=0.116\n",
            "     Pixels:     x=356, y=161, w=10, h=70\n",
            "  74. text\n",
            "     Normalized: x=0.457, y=0.269, w=0.007, h=0.116\n",
            "     Pixels:     x=366, y=161, w=6, h=70\n",
            "  75. text\n",
            "     Normalized: x=0.464, y=0.269, w=0.007, h=0.116\n",
            "     Pixels:     x=371, y=161, w=6, h=70\n",
            "  76. text\n",
            "     Normalized: x=0.471, y=0.269, w=0.007, h=0.116\n",
            "     Pixels:     x=377, y=161, w=6, h=70\n",
            "  77. text\n",
            "     Normalized: x=0.478, y=0.269, w=0.007, h=0.116\n",
            "     Pixels:     x=382, y=161, w=6, h=70\n",
            "  78. text\n",
            "     Normalized: x=0.485, y=0.269, w=0.007, h=0.116\n",
            "     Pixels:     x=388, y=161, w=6, h=70\n",
            "  79. text\n",
            "     Normalized: x=0.492, y=0.269, w=0.007, h=0.116\n",
            "     Pixels:     x=394, y=161, w=6, h=70\n",
            "  80. text\n",
            "     Normalized: x=0.499, y=0.269, w=0.007, h=0.116\n",
            "     Pixels:     x=399, y=161, w=6, h=70\n",
            "  81. text\n",
            "     Normalized: x=0.506, y=0.269, w=0.007, h=0.116\n",
            "     Pixels:     x=405, y=161, w=6, h=70\n",
            "  82. text\n",
            "     Normalized: x=0.513, y=0.269, w=0.007, h=0.116\n",
            "     Pixels:     x=410, y=161, w=6, h=70\n",
            "  83. text\n",
            "     Normalized: x=0.520, y=0.269, w=0.007, h=0.116\n",
            "     Pixels:     x=416, y=161, w=6, h=70\n",
            "  84. text\n",
            "     Normalized: x=0.527, y=0.269, w=0.007, h=0.116\n",
            "     Pixels:     x=422, y=161, w=6, h=70\n",
            "  85. text\n",
            "     Normalized: x=0.534, y=0.269, w=0.007, h=0.116\n",
            "     Pixels:     x=427, y=161, w=6, h=70\n",
            "  86. text\n",
            "     Normalized: x=0.541, y=0.269, w=0.007, h=0.116\n",
            "     Pixels:     x=433, y=161, w=6, h=70\n",
            "  87. text\n",
            "     Normalized: x=0.548, y=0.269, w=0.007, h=0.116\n",
            "     Pixels:     x=438, y=161, w=6, h=70\n",
            "  88. text\n",
            "     Normalized: x=0.555, y=0.269, w=0.007, h=0.116\n",
            "     Pixels:     x=444, y=161, w=6, h=70\n",
            "  89. text\n",
            "     Normalized: x=0.562, y=0.269, w=0.007, h=0.116\n",
            "     Pixels:     x=450, y=161, w=6, h=70\n",
            "  90. text\n",
            "     Normalized: x=0.569, y=0.269, w=0.007, h=0.116\n",
            "     Pixels:     x=455, y=161, w=6, h=70\n",
            "  91. text\n",
            "     Normalized: x=0.576, y=0.269, w=0.007, h=0.116\n",
            "     Pixels:     x=461, y=161, w=6, h=70\n",
            "  92. text\n",
            "     Normalized: x=0.583, y=0.269, w=0.007, h=0.116\n",
            "     Pixels:     x=466, y=161, w=6, h=70\n",
            "  93. text\n",
            "     Normalized: x=0.590, y=0.269, w=0.007, h=0.116\n",
            "     Pixels:     x=472, y=161, w=6, h=70\n",
            "  94. text\n",
            "     Normalized: x=0.597, y=0.269, w=0.007, h=0.116\n",
            "     Pixels:     x=478, y=161, w=6, h=70\n",
            "  95. text\n",
            "     Normalized: x=0.604, y=0.269, w=0.007, h=0.116\n",
            "     Pixels:     x=483, y=161, w=6, h=70\n",
            "  96. text\n",
            "     Normalized: x=0.611, y=0.269, w=0.007, h=0.116\n",
            "     Pixels:     x=489, y=161, w=6, h=70\n",
            "  97. text\n",
            "     Normalized: x=0.618, y=0.269, w=0.007, h=0.116\n",
            "     Pixels:     x=494, y=161, w=6, h=70\n",
            "  98. text\n",
            "     Normalized: x=0.625, y=0.269, w=0.007, h=0.116\n",
            "     Pixels:     x=500, y=161, w=6, h=70\n",
            "  99. text\n",
            "     Normalized: x=0.632, y=0.269, w=0.007, h=0.116\n",
            "     Pixels:     x=506, y=161, w=6, h=70\n",
            "  100. text\n",
            "     Normalized: x=0.639, y=0.269, w=0.007, h=0.116\n",
            "     Pixels:     x=511, y=161, w=6, h=70\n",
            "  101. text\n",
            "     Normalized: x=0.646, y=0.269, w=0.007, h=0.116\n",
            "     Pixels:     x=517, y=161, w=6, h=70\n",
            "  102. text\n",
            "     Normalized: x=0.653, y=0.269, w=0.007, h=0.116\n",
            "     Pixels:     x=522, y=161, w=6, h=70\n",
            "  103. text\n",
            "     Normalized: x=0.660, y=0.269, w=0.007, h=0.116\n",
            "     Pixels:     x=528, y=161, w=6, h=70\n",
            "  104. text\n",
            "     Normalized: x=0.667, y=0.269, w=0.007, h=0.116\n",
            "     Pixels:     x=534, y=161, w=6, h=70\n",
            "  105. text\n",
            "     Normalized: x=0.674, y=0.269, w=0.007, h=0.116\n",
            "     Pixels:     x=539, y=161, w=6, h=70\n",
            "  106. text\n",
            "     Normalized: x=0.681, y=0.269, w=0.007, h=0.116\n",
            "     Pixels:     x=545, y=161, w=6, h=70\n",
            "  107. text\n",
            "     Normalized: x=0.688, y=0.269, w=0.007, h=0.116\n",
            "     Pixels:     x=550, y=161, w=6, h=70\n",
            "  108. text\n",
            "     Normalized: x=0.695, y=0.269, w=0.007, h=0.116\n",
            "     Pixels:     x=556, y=161, w=6, h=70\n",
            "  109. text\n",
            "     Normalized: x=0.702, y=0.269, w=0.007, h=0.116\n",
            "     Pixels:     x=562, y=161, w=6, h=70\n",
            "  110. text\n",
            "     Normalized: x=0.709, y=0.269, w=0.007, h=0.116\n",
            "     Pixels:     x=567, y=161, w=6, h=70\n",
            "  111. text\n",
            "     Normalized: x=0.716, y=0.269, w=0.007, h=0.116\n",
            "     Pixels:     x=573, y=161, w=6, h=70\n",
            "  112. text\n",
            "     Normalized: x=0.723, y=0.269, w=0.007, h=0.116\n",
            "     Pixels:     x=578, y=161, w=6, h=70\n",
            "  113. text\n",
            "     Normalized: x=0.730, y=0.269, w=0.007, h=0.116\n",
            "     Pixels:     x=584, y=161, w=6, h=70\n",
            "  114. text\n",
            "     Normalized: x=0.737, y=0.269, w=0.007, h=0.116\n",
            "     Pixels:     x=590, y=161, w=6, h=70\n",
            "  115. text\n",
            "     Normalized: x=0.744, y=0.269, w=0.007, h=0.116\n",
            "     Pixels:     x=595, y=161, w=6, h=70\n",
            "  116. text\n",
            "     Normalized: x=0.751, y=0.269, w=0.007, h=0.116\n",
            "     Pixels:     x=601, y=161, w=6, h=70\n",
            "  117. text\n",
            "     Normalized: x=0.758, y=0.269, w=0.007, h=0.116\n",
            "     Pixels:     x=606, y=161, w=6, h=70\n",
            "  118. text\n",
            "     Normalized: x=0.765, y=0.269, w=0.007, h=0.116\n",
            "     Pixels:     x=612, y=161, w=6, h=70\n",
            "  119. text\n",
            "     Normalized: x=0.772, y=0.269, w=0.007, h=0.116\n",
            "     Pixels:     x=618, y=161, w=6, h=70\n",
            "  120. text\n",
            "     Normalized: x=0.779, y=0.269, w=0.007, h=0.116\n",
            "     Pixels:     x=623, y=161, w=6, h=70\n",
            "  121. text\n",
            "     Normalized: x=0.786, y=0.269, w=0.007, h=0.116\n",
            "     Pixels:     x=629, y=161, w=6, h=70\n",
            "  122. text\n",
            "     Normalized: x=0.793, y=0.269, w=0.007, h=0.116\n",
            "     Pixels:     x=634, y=161, w=6, h=70\n",
            "  123. text\n",
            "     Normalized: x=0.800, y=0.269, w=0.007, h=0.116\n",
            "     Pixels:     x=640, y=161, w=6, h=70\n",
            "  124. text\n",
            "     Normalized: x=0.807, y=0.269, w=0.007, h=0.116\n",
            "     Pixels:     x=646, y=161, w=6, h=70\n",
            "  125. text\n",
            "     Normalized: x=0.814, y=0.269, w=0.007, h=0.116\n",
            "     Pixels:     x=651, y=161, w=6, h=70\n",
            "  126. text\n",
            "     Normalized: x=0.821, y=0.269, w=0.007, h=0.116\n",
            "     Pixels:     x=657, y=161, w=6, h=70\n",
            "  127. text\n",
            "     Normalized: x=0.828, y=0.269, w=0.007, h=0.116\n",
            "     Pixels:     x=662, y=161, w=6, h=70\n",
            "  128. text\n",
            "     Normalized: x=0.835, y=0.269, w=0.007, h=0.116\n",
            "     Pixels:     x=668, y=161, w=6, h=70\n",
            "  129. text\n",
            "     Normalized: x=0.842, y=0.269, w=0.007, h=0.116\n",
            "     Pixels:     x=674, y=161, w=6, h=70\n",
            "  130. text\n",
            "     Normalized: x=0.849, y=0.269, w=0.007, h=0.116\n",
            "     Pixels:     x=679, y=161, w=6, h=70\n",
            "  131. text\n",
            "     Normalized: x=0.856, y=0.269, w=0.007, h=0.116\n",
            "     Pixels:     x=685, y=161, w=6, h=70\n",
            "  132. text\n",
            "     Normalized: x=0.863, y=0.269, w=0.007, h=0.116\n",
            "     Pixels:     x=690, y=161, w=6, h=70\n",
            "  133. text\n",
            "     Normalized: x=0.870, y=0.269, w=0.007, h=0.116\n",
            "     Pixels:     x=696, y=161, w=6, h=70\n",
            "  134. text\n",
            "     Normalized: x=0.877, y=0.269, w=0.007, h=0.116\n",
            "     Pixels:     x=702, y=161, w=6, h=70\n",
            "  135. text\n",
            "     Normalized: x=0.884, y=0.269, w=0.007, h=0.116\n",
            "     Pixels:     x=707, y=161, w=6, h=70\n",
            "  136. text\n",
            "     Normalized: x=0.891, y=0.269, w=0.007, h=0.116\n",
            "     Pixels:     x=713, y=161, w=6, h=70\n",
            "  137. text\n",
            "     Normalized: x=0.898, y=0.269, w=0.007, h=0.116\n",
            "     Pixels:     x=718, y=161, w=6, h=70\n",
            "  138. text\n",
            "     Normalized: x=0.905, y=0.269, w=0.007, h=0.116\n",
            "     Pixels:     x=724, y=161, w=6, h=70\n",
            "  139. text\n",
            "     Normalized: x=0.912, y=0.269, w=0.007, h=0.116\n",
            "     Pixels:     x=730, y=161, w=6, h=70\n",
            "  140. text\n",
            "     Normalized: x=0.919, y=0.269, w=0.007, h=0.116\n",
            "     Pixels:     x=735, y=161, w=6, h=70\n",
            "  141. text\n",
            "     Normalized: x=0.926, y=0.269, w=0.007, h=0.116\n",
            "     Pixels:     x=741, y=161, w=6, h=70\n",
            "  142. text\n",
            "     Normalized: x=0.933, y=0.269, w=0.007, h=0.116\n",
            "     Pixels:     x=746, y=161, w=6, h=70\n",
            "  143. text\n",
            "     Normalized: x=0.940, y=0.269, w=0.007, h=0.116\n",
            "     Pixels:     x=752, y=161, w=6, h=70\n",
            "  144. text\n",
            "     Normalized: x=0.947, y=0.269, w=0.007, h=0.116\n",
            "     Pixels:     x=758, y=161, w=6, h=70\n",
            "  145. text\n",
            "     Normalized: x=0.954, y=0.269, w=0.007, h=0.116\n",
            "     Pixels:     x=763, y=161, w=6, h=70\n",
            "  146. text\n",
            "     Normalized: x=0.961, y=0.269, w=0.007, h=0.116\n",
            "     Pixels:     x=769, y=161, w=6, h=70\n",
            "  147. text\n",
            "     Normalized: x=0.968, y=0.269, w=0.007, h=0.116\n",
            "     Pixels:     x=774, y=161, w=6, h=70\n",
            "  148. text\n",
            "     Normalized: x=0.975, y=0.269, w=0.007, h=0.116\n",
            "     Pixels:     x=780, y=161, w=6, h=70\n",
            "  149. text\n",
            "     Normalized: x=0.982, y=0.269, w=0.007, h=0.116\n",
            "     Pixels:     x=786, y=161, w=6, h=70\n",
            "  150. text\n",
            "     Normalized: x=0.989, y=0.269, w=0.007, h=0.116\n",
            "     Pixels:     x=791, y=161, w=6, h=70\n",
            "  151. text\n",
            "     Normalized: x=0.996, y=0.269, w=0.007, h=0.116\n",
            "     Pixels:     x=797, y=161, w=6, h=70\n",
            "  152. text\n",
            "     Normalized: x=1.003, y=0.269, w=0.007, h=0.116\n",
            "     Pixels:     x=802, y=161, w=6, h=70\n",
            "  153. text\n",
            "     Normalized: x=1.010, y=0.269, w=0.007, h=0.116\n",
            "     Pixels:     x=808, y=161, w=6, h=70\n",
            "  154. text\n",
            "     Normalized: x=1.017, y=0.269, w=0.007, h=0.116\n",
            "     Pixels:     x=814, y=161, w=6, h=70\n",
            "  155. text\n",
            "     Normalized: x=1.024, y=0.269, w=0.007, h=0.116\n",
            "     Pixels:     x=819, y=161, w=6, h=70\n",
            "  156. text\n",
            "     Normalized: x=1.031, y=0.269, w=0.007, h=0.116\n",
            "     Pixels:     x=825, y=161, w=6, h=70\n",
            "  157. text\n",
            "     Normalized: x=1.038, y=0.269, w=0.007, h=0.116\n",
            "     Pixels:     x=830, y=161, w=6, h=70\n",
            "  158. text\n",
            "     Normalized: x=1.045, y=0.269, w=0.007, h=0.116\n",
            "     Pixels:     x=836, y=161, w=6, h=70\n",
            "  159. text\n",
            "     Normalized: x=1.052, y=0.269, w=0.007, h=0.116\n",
            "     Pixels:     x=842, y=161, w=6, h=70\n",
            "  160. text\n",
            "     Normalized: x=1.059, y=0.269, w=0.007, h=0.116\n",
            "     Pixels:     x=847, y=161, w=6, h=70\n",
            "  161. text\n",
            "     Normalized: x=1.066, y=0.269, w=0.007, h=0.116\n",
            "     Pixels:     x=853, y=161, w=6, h=70\n",
            "\n",
            "ğŸ“Š Objects Detected by Category:\n",
            "â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
            "â”‚ (idx)  â”‚ Values â”‚\n",
            "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
            "â”‚ car    â”‚     52 â”‚\n",
            "â”‚ person â”‚      6 â”‚\n",
            "â”‚ face   â”‚      6 â”‚\n",
            "â”‚ text   â”‚     97 â”‚\n",
            "â””â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n"
          ]
        }
      ],
      "source": [
        "// Street scene image (crossroad with vehicles and pedestrians)\n",
        "const streetImageUrl = \"https://storage.googleapis.com/vlm-data-public-prod/hub/examples/image.object-detection/crossroad.jpg\";\n",
        "\n",
        "console.log(\"ğŸ“· Image URL:\", streetImageUrl);\n",
        "console.log(\"\\nDetecting all objects in the street scene...\\n\");\n",
        "\n",
        "// Perform general object detection\n",
        "const objectResult = await detectObjects(streetImageUrl, \"Detect all objects in this image including people, vehicles, and other objects\");\n",
        "\n",
        "// Format and display results with assumed image dimensions\n",
        "formatDetections(objectResult.detections, 800, 600);\n",
        "\n",
        "// Show counts by category\n",
        "const objectCounts = countByLabel(objectResult.detections);\n",
        "console.log(\"\\nğŸ“Š Objects Detected by Category:\");\n",
        "console.table(objectCounts);\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Example 3: Face Detection\n",
        "\n",
        "Let's specifically detect faces in an image.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "ğŸ“· Image URL: https://images.unsplash.com/photo-1511632765486-a01980e01a18?w=800\n",
            "\n",
            "Detecting faces in the image...\n",
            "\n",
            "Found 4 faces:\n",
            "  Face 1: x=0.324, y=0.266, w=0.118, h=0.622\n",
            "  Face 2: x=0.405, y=0.271, w=0.137, h=0.620\n",
            "  Face 3: x=0.491, y=0.273, w=0.144, h=0.622\n",
            "  Face 4: x=0.589, y=0.240, w=0.117, h=0.648\n"
          ]
        }
      ],
      "source": [
        "// Use the same people image for face detection\n",
        "console.log(\"ğŸ“· Image URL:\", peopleImageUrl);\n",
        "console.log(\"\\nDetecting faces in the image...\\n\");\n",
        "\n",
        "// Perform face detection\n",
        "const faceResult = await detectObjects(peopleImageUrl, \"Detect all faces in this image\");\n",
        "\n",
        "// Display face detection results\n",
        "console.log(`Found ${faceResult.detections.length} faces:`);\n",
        "faceResult.detections.forEach((det, i) => {\n",
        "    const [x, y, w, h] = det.xywh;\n",
        "    console.log(`  Face ${i + 1}: x=${x.toFixed(3)}, y=${y.toFixed(3)}, w=${w.toFixed(3)}, h=${h.toFixed(3)}`);\n",
        "});\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Example 4: Batch Processing Multiple Images\n",
        "\n",
        "Process multiple images and aggregate detection results.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Processing: Street Scene...\n",
            "Processing: Group of People...\n",
            "Processing: Cat...\n",
            "\n",
            "=== Batch Detection Results ===\n",
            "\n",
            "â”Œâ”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
            "â”‚ (idx) â”‚ Image             â”‚ Objects Found â”‚ Unique Labels                                                                   â”‚\n",
            "â”œâ”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
            "â”‚     0 â”‚ \"Street Scene\"    â”‚            82 â”‚ \"person, taxi, car, taxi roof sign, license plate, sign, Office, traffic light\" â”‚\n",
            "â”‚     1 â”‚ \"Group of People\" â”‚             4 â”‚ \"person\"                                                                        â”‚\n",
            "â”‚     2 â”‚ \"Cat\"             â”‚             3 â”‚ \"cat, green background, wooden surface\"                                         â”‚\n",
            "â””â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n"
          ]
        }
      ],
      "source": [
        "// Multiple images for batch processing\n",
        "const batchImages = [\n",
        "    { url: \"https://storage.googleapis.com/vlm-data-public-prod/hub/examples/image.object-detection/crossroad.jpg\", name: \"Street Scene\" },\n",
        "    { url: \"https://images.unsplash.com/photo-1511632765486-a01980e01a18?w=800\", name: \"Group of People\" },\n",
        "    { url: \"https://images.unsplash.com/photo-1514888286974-6c03e2ca1dba?w=400\", name: \"Cat\" },\n",
        "];\n",
        "\n",
        "interface BatchDetectionResult {\n",
        "    imageName: string;\n",
        "    url: string;\n",
        "    objectCount: number;\n",
        "    labels: string[];\n",
        "    error?: string;\n",
        "}\n",
        "\n",
        "async function processBatch(images: { url: string; name: string }[]): Promise<BatchDetectionResult[]> {\n",
        "    const results: BatchDetectionResult[] = [];\n",
        "    \n",
        "    for (const img of images) {\n",
        "        try {\n",
        "            console.log(`Processing: ${img.name}...`);\n",
        "            const detections = await detectObjects(img.url);\n",
        "            const uniqueLabels = [...new Set(detections.detections.map(d => d.label))];\n",
        "            \n",
        "            results.push({\n",
        "                imageName: img.name,\n",
        "                url: img.url,\n",
        "                objectCount: detections.detections.length,\n",
        "                labels: uniqueLabels\n",
        "            });\n",
        "        } catch (error) {\n",
        "            console.log(`Error processing ${img.name}: ${error}`);\n",
        "            results.push({\n",
        "                imageName: img.name,\n",
        "                url: img.url,\n",
        "                objectCount: 0,\n",
        "                labels: [],\n",
        "                error: String(error)\n",
        "            });\n",
        "        }\n",
        "    }\n",
        "    \n",
        "    return results;\n",
        "}\n",
        "\n",
        "// Process all images\n",
        "const batchResults = await processBatch(batchImages);\n",
        "\n",
        "// Display batch results\n",
        "console.log(\"\\n=== Batch Detection Results ===\\n\");\n",
        "console.table(batchResults.map(r => ({\n",
        "    Image: r.imageName,\n",
        "    \"Objects Found\": r.objectCount,\n",
        "    \"Unique Labels\": r.labels.join(\", \") || \"Error\"\n",
        "})));\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Understanding Bounding Box Format\n",
        "\n",
        "VLM Run returns bounding boxes in **normalized xywh format**:\n",
        "\n",
        "| Field | Description | Range |\n",
        "|-------|-------------|-------|\n",
        "| `x` | Left edge of bounding box | 0.0 - 1.0 |\n",
        "| `y` | Top edge of bounding box | 0.0 - 1.0 |\n",
        "| `w` | Width of bounding box | 0.0 - 1.0 |\n",
        "| `h` | Height of bounding box | 0.0 - 1.0 |\n",
        "\n",
        "To convert to pixel coordinates, multiply by image dimensions:\n",
        "```typescript\n",
        "const x_px = x * imageWidth;\n",
        "const y_px = y * imageHeight;\n",
        "const w_px = w * imageWidth;\n",
        "const h_px = h * imageHeight;\n",
        "```\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Best Practices & Tips\n",
        "\n",
        "### Detection Quality\n",
        "- Use specific prompts for better results (e.g., \"Detect all cars\" vs \"Detect objects\")\n",
        "- For crowded scenes, consider detecting specific object types separately\n",
        "- Use higher resolution images for better small object detection\n",
        "\n",
        "### Supported Object Classes\n",
        "VLM Run can detect 80+ COCO dataset classes including:\n",
        "- **People**: person, face\n",
        "- **Vehicles**: car, truck, bus, motorcycle, bicycle, airplane, boat\n",
        "- **Animals**: cat, dog, bird, horse, cow, sheep\n",
        "- **Objects**: chair, table, laptop, phone, book, bottle, cup\n",
        "\n",
        "### Performance Tips\n",
        "- Batch similar detection tasks for efficiency\n",
        "- Cache detection results for frequently analyzed images\n",
        "- Use appropriate image sizes (800-1200px width is usually sufficient)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Additional Resources\n",
        "\n",
        "- [VLM Run Documentation](https://docs.vlm.run)\n",
        "- [Detection API Reference](https://docs.vlm.run/agents/capabilities/image/detection)\n",
        "- [VLM Run Node.js SDK](https://github.com/vlm-run/vlmrun-node-sdk)\n",
        "- [More Examples](https://github.com/vlm-run/vlmrun-cookbook)\n",
        "- [Discord Community](https://discord.gg/AMApC2UzVY)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Deno",
      "language": "typescript",
      "name": "deno"
    },
    "language_info": {
      "codemirror_mode": "typescript",
      "file_extension": ".ts",
      "mimetype": "text/x.typescript",
      "name": "typescript",
      "nbconvert_exporter": "script",
      "pygments_lexer": "typescript",
      "version": "5.9.2"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
