{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "<div align=\"center\">\n",
        "<p align=\"center\" style=\"width: 100%;\">\n",
        "    <img src=\"https://raw.githubusercontent.com/vlm-run/.github/refs/heads/main/profile/assets/vlm-black.svg\" alt=\"VLM Run Logo\" width=\"80\" style=\"margin-bottom: -5px; color: #2e3138; vertical-align: middle; padding-right: 5px;\"><br>\n",
        "</p>\n",
        "<p align=\"center\"><a href=\"https://docs.vlm.run\"><b>Website</b></a> | <a href=\"https://docs.vlm.run/\"><b>API Docs</b></a> | <a href=\"https://docs.vlm.run/blog\"><b>Blog</b></a> | <a href=\"https://discord.gg/AMApC2UzVY\"><b>Discord</b></a> | <a href=\"https://chat.vlm.run\"><b>Chat</b></a>\n",
        "</p>\n",
        "</div>\n",
        "\n",
        "# VLM Run Orion - Document Understanding (Node.js)\n",
        "\n",
        "This comprehensive cookbook demonstrates [VLM Run Orion's](https://vlm.run/orion) document understanding capabilities using **Node.js/TypeScript** including OCR, layout detection, redaction, and multi-document classification. For more details on the API, see the [Agent API docs](https://docs.vlm.run/agents/introduction).\n",
        "\n",
        "For this notebook, we'll cover how to use the **VLM Run Agent Chat Completions API** - an OpenAI-compatible interface for building powerful document intelligence with the same familiar chat-completions interface.\n",
        "\n",
        "We'll cover the following topics:\n",
        " 1. **OCR (Optical Character Recognition)** - Extract text, tables, paragraphs, and figures from documents\n",
        " 2. **Layout Detection** - Identify document structure (headers, footers, tables, figures, etc.)\n",
        " 3. **Document Redaction** - Detect and redact sensitive information (PII, financial data, PHI, etc.)\n",
        " 4. **Multi-Document Classification** - Classify documents into categories based on content and structure\n",
        "\n",
        "## Prerequisites\n",
        "\n",
        "- Node.js 18+\n",
        "- VLM Run API key (get one at [app.vlm.run](https://app.vlm.run))\n",
        "- Deno or tslab kernel for running TypeScript in Jupyter\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Setup\n",
        "\n",
        "First, install the required packages and configure the environment.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [],
      "source": [
        "// Install the VLM Run SDK\n",
        "// npm install vlmrun openai zod zod-to-json-schema\n",
        "\n",
        "// If using Deno kernel, install dependencies via npm specifiers\n",
        "// For tslab, run: npm install vlmrun openai zod zod-to-json-schema in your project directory\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [],
      "source": [
        "// Import the VLM Run SDK and dependencies\n",
        "import { VlmRun } from \"vlmrun\";\n",
        "import { z } from \"zod\";\n",
        "import { zodToJsonSchema } from \"zod-to-json-schema\";\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "✓ API Key loaded successfully\n"
          ]
        }
      ],
      "source": [
        "// Get API key from environment variable\n",
        "const VLMRUN_API_KEY = Deno.env.get(\"VLMRUN_API_KEY\");\n",
        "\n",
        "if (!VLMRUN_API_KEY) {\n",
        "    throw new Error(\"Please set the VLMRUN_API_KEY environment variable\");\n",
        "}\n",
        "\n",
        "console.log(\"✓ API Key loaded successfully\");\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Initialize the VLM Run Client\n",
        "\n",
        "We use the OpenAI-compatible chat completions interface through the VLM Run SDK.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "✓ VLM Run client initialized successfully!\n",
            "Base URL: https://agent.vlm.run/v1\n",
            "Model: vlmrun-orion-1\n"
          ]
        }
      ],
      "source": [
        "// Initialize the VLM Run client using the SDK\n",
        "const client = new VlmRun({\n",
        "    apiKey: VLMRUN_API_KEY,\n",
        "    baseURL: \"https://agent.vlm.run/v1\"  // Use the agent API endpoint\n",
        "});\n",
        "\n",
        "console.log(\"✓ VLM Run client initialized successfully!\");\n",
        "console.log(\"Base URL: https://agent.vlm.run/v1\");\n",
        "console.log(\"Model: vlmrun-orion-1\");\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Response Models (Schemas)\n",
        "\n",
        "We define Zod schemas for structured outputs. These schemas provide type-safe, validated responses for document understanding tasks.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "✓ Response schemas defined successfully!\n",
            "Schemas include type-safe validation for structured outputs.\n"
          ]
        }
      ],
      "source": [
        "// Helper function to download images from URLs\n",
        "async function downloadImage(url: string): Promise<Uint8Array> {\n",
        "    const response = await fetch(url);\n",
        "    if (!response.ok) {\n",
        "        throw new Error(`Failed to download image: ${response.statusText}`);\n",
        "    }\n",
        "    return new Uint8Array(await response.arrayBuffer());\n",
        "}\n",
        "\n",
        "// Image URL Response Schema\n",
        "const ImageUrlResponseSchema = z.object({\n",
        "    url: z.string().describe(\"Pre-signed URL to the image\")\n",
        "});\n",
        "\n",
        "type ImageUrlResponse = z.infer<typeof ImageUrlResponseSchema>;\n",
        "\n",
        "// Document URL Response Schema\n",
        "const DocumentUrlResponseSchema = z.object({\n",
        "    url: z.string().describe(\"Pre-signed URL to the document\")\n",
        "});\n",
        "\n",
        "type DocumentUrlResponse = z.infer<typeof DocumentUrlResponseSchema>;\n",
        "\n",
        "// Image URL List Response Schema\n",
        "const ImageUrlListResponseSchema = z.object({\n",
        "    urls: z.array(ImageUrlResponseSchema).describe(\"List of pre-signed image URL responses\")\n",
        "});\n",
        "\n",
        "type ImageUrlListResponse = z.infer<typeof ImageUrlListResponseSchema>;\n",
        "\n",
        "// Detection Schema\n",
        "const DetectionSchema = z.object({\n",
        "    label: z.string().describe(\"Name of the detected object or text\"),\n",
        "    xywh: z.tuple([z.number(), z.number(), z.number(), z.number()])\n",
        "        .describe(\"Bounding box (x, y, width, height) normalized from 0-1\"),\n",
        "    confidence: z.number().nullable().optional().describe(\"Detection confidence score from 0-1\")\n",
        "});\n",
        "\n",
        "// Detections Response Schema\n",
        "const DetectionsResponseSchema = z.object({\n",
        "    detections: z.array(DetectionSchema).describe(\"List of detected objects or text or layout elements with bounding boxes\"),\n",
        "    image_url: z.string().optional().describe(\"Url to the image for the detections\")\n",
        "});\n",
        "\n",
        "type DetectionsResponse = z.infer<typeof DetectionsResponseSchema>;\n",
        "\n",
        "// Document Classification Response Schema\n",
        "const DocumentClassificationResponseSchema = z.object({\n",
        "    rationale: z.string().describe(\"Rationale for the classification\"),\n",
        "    domain: z.string().describe(\"The classified domain of the document\"),\n",
        "    confidence: z.string().describe(\"Confidence level: hi, med, or lo\"),\n",
        "    tags: z.array(z.string()).nullable().optional().describe(\"List of tags describing the document\")\n",
        "});\n",
        "\n",
        "type DocumentClassificationResponse = z.infer<typeof DocumentClassificationResponseSchema>;\n",
        "\n",
        "// Layout Element Schema\n",
        "const LayoutElementSchema = z.object({\n",
        "    category: z.string().describe(\"Category: caption, footnote, formula, list-item, page-footer, page-header, picture, section-header, table, text, title\"),\n",
        "    xywh: z.tuple([z.number(), z.number(), z.number(), z.number()])\n",
        "        .describe(\"Bounding box (x, y, width, height) normalized from 0-1\"),\n",
        "    text: z.string().nullable().optional().describe(\"Text content of the element if available\")\n",
        "});\n",
        "\n",
        "// Layout Detection Response Schema\n",
        "const LayoutDetectionResponseSchema = z.object({\n",
        "    elements: z.array(LayoutElementSchema).describe(\"List of detected layout elements\")\n",
        "});\n",
        "\n",
        "type LayoutDetectionResponse = z.infer<typeof LayoutDetectionResponseSchema>;\n",
        "\n",
        "// Sensitive Item Schema\n",
        "const SensitiveItemSchema = z.object({\n",
        "    item_type: z.string().describe(\"Type of sensitive information\"),\n",
        "    value: z.string().describe(\"The detected sensitive value\")\n",
        "});\n",
        "\n",
        "// Redaction Detection Response Schema\n",
        "const RedactionDetectionResponseSchema = z.object({\n",
        "    detected_items: z.array(SensitiveItemSchema).describe(\"List of detected sensitive items\")\n",
        "});\n",
        "\n",
        "type RedactionDetectionResponse = z.infer<typeof RedactionDetectionResponseSchema>;\n",
        "\n",
        "console.log(\"✓ Response schemas defined successfully!\");\n",
        "console.log(\"Schemas include type-safe validation for structured outputs.\");\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "✓ Helper functions defined!\n"
          ]
        }
      ],
      "source": [
        "/**\n",
        " * Make a chat completion request with optional images, documents, and structured output.\n",
        " * \n",
        " * @param prompt - The text prompt/instruction\n",
        " * @param images - Optional list of images to process (URLs)\n",
        " * @param documents - Optional list of documents to process (URLs)\n",
        " * @param responseSchema - Optional Zod schema for structured output\n",
        " * @param model - Model to use (default: vlmrun-orion-1:auto)\n",
        " * @returns Parsed response if responseSchema provided, else raw response text\n",
        " */\n",
        "async function chatCompletion<T>(\n",
        "    prompt: string,\n",
        "    images?: string[],\n",
        "    documents?: string[],\n",
        "    responseSchema?: z.ZodSchema<T>,\n",
        "    model: string = \"vlmrun-orion-1:auto\"\n",
        "): Promise<T | string> {\n",
        "    const content: any[] = [];\n",
        "    content.push({ type: \"text\", text: prompt });\n",
        "\n",
        "    // Add documents first (if any)\n",
        "    if (documents) {\n",
        "        for (const doc of documents) {\n",
        "            if (typeof doc === \"string\") {\n",
        "                if (!doc.startsWith(\"http\")) {\n",
        "                    throw new Error(\"Document URLs must start with http or https\");\n",
        "                }\n",
        "                content.push({\n",
        "                    type: \"file_url\",\n",
        "                    file_url: { url: doc, detail: \"auto\" }\n",
        "                });\n",
        "            }\n",
        "        }\n",
        "    }\n",
        "\n",
        "    // Add images (if any)\n",
        "    if (images) {\n",
        "        for (const image of images) {\n",
        "            if (typeof image === \"string\") {\n",
        "                if (!image.startsWith(\"http\")) {\n",
        "                    throw new Error(\"Image URLs must start with http or https\");\n",
        "                }\n",
        "                content.push({\n",
        "                    type: \"image_url\",\n",
        "                    image_url: { url: image, detail: \"auto\" }\n",
        "                });\n",
        "            }\n",
        "        }\n",
        "    }\n",
        "\n",
        "    const kwargs: any = {\n",
        "        model: model,\n",
        "        messages: [{ role: \"user\", content: content }]\n",
        "    };\n",
        "\n",
        "    if (responseSchema) {\n",
        "        kwargs.response_format = {\n",
        "            type: \"json_schema\",\n",
        "            schema: zodToJsonSchema(responseSchema)\n",
        "        } as any;\n",
        "    }\n",
        "\n",
        "    const response = await client.agent.completions.create(kwargs);\n",
        "    const responseText = response.choices[0].message.content || \"\";\n",
        "\n",
        "    if (responseSchema) {\n",
        "        const parsed = JSON.parse(responseText);\n",
        "        return responseSchema.parse(parsed) as T;\n",
        "    }\n",
        "\n",
        "    return responseText;\n",
        "}\n",
        "\n",
        "console.log(\"✓ Helper functions defined!\");\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 1. OCR (Optical Character Recognition)\n",
        "\n",
        "Extract text, tables, paragraphs, and figures from documents using OCR capabilities.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            ">> OCR RESULT\n",
            "Today is Thursday, October 20th- But it definitely feels like a Friday. I'm already considering making a second cup of coffee- and I haven't even finished my first. Do I have a problem?\n",
            "Sometimes I'll flip through older notes I've taken, and my handwriting is unrecognizable, Perhaps it depends on the type of pen I use?\n",
            "I've tried writing in all caps But IT Looks So FORCED AND UNNATURAL\n",
            "Often times, I'll just take notes on my laptop, but I still seem to gravitate toward pen and paper. Any advice on what to I'm prove ? I already feel stressed out looking back at what I've just written- it looks like 3 different people wrote this!\n",
            "\n",
            ">> DOCUMENT IMAGE URL: https://storage.googleapis.com/vlm-data-public-prod/hub/examples/agent_use_cases/hand_writting_beautification/image-ocr.jpg\n"
          ]
        }
      ],
      "source": [
        "// Example: Extract text from a document image\n",
        "const IMAGE_URL = \"https://storage.googleapis.com/vlm-data-public-prod/hub/examples/agent_use_cases/hand_writting_beautification/image-ocr.jpg\";\n",
        "\n",
        "const result = await chatCompletion(\n",
        "    \"Extract all text from this document image. Return the full text content with proper formatting.\",\n",
        "    [IMAGE_URL]\n",
        ");\n",
        "\n",
        "console.log(\">> OCR RESULT\");\n",
        "console.log(result);\n",
        "console.log(\"\\n>> DOCUMENT IMAGE URL:\", IMAGE_URL);\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 1b. OCR with Structured Extraction\n",
        "\n",
        "Extract structured information including tables, paragraphs, and figures from documents.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            ">> STRUCTURED OCR RESULT\n",
            ">> RESPONSE\n",
            "Found 19 text fields\n",
            "  1. Irene: xywh=[0.183, 0.119, 0.082, 0.021]\n",
            "  2. Wong: xywh=[0.368, 0.114, 0.073, 0.025]\n",
            "  3. family health: xywh=[0.615, 0.110, 0.172, 0.030]\n",
            "  4. ireneworry@gmail.com: xywh=[0.200, 0.148, 0.268, 0.027]\n",
            "  5. 6753369412: xywh=[0.610, 0.147, 0.237, 0.027]\n",
            "  6. Kevin: xywh=[0.191, 0.201, 0.089, 0.025]\n",
            "  7. chen: xywh=[0.368, 0.204, 0.068, 0.021]\n",
            "  8. 9/15/2001: xywh=[0.616, 0.196, 0.168, 0.027]\n",
            "  9. kerimchen@gmail.com: xywh=[0.228, 0.233, 0.269, 0.025]\n",
            "  10. 6153012311: xywh=[0.611, 0.233, 0.194, 0.025]\n",
            "  ... and 9 more\n",
            "\n",
            ">> Document image URL: https://storage.googleapis.com/vlm-userdata-prod/agents/artifacts/ae8ae740-ddd6-426b-bde1-75540f99f277/70de51de-6233-4e2f-89a7-a6748be6f009/img_521cea.jpg?X-Goog-Algorithm=GOOG4-RSA-SHA256&X-Goog-Credential=vlm-deployments%40vlm-infra-prod.iam.gserviceaccount.com%2F20251219%2Fauto%2Fstorage%2Fgoog4_request&X-Goog-Date=20251219T103556Z&X-Goog-Expires=604800&X-Goog-SignedHeaders=host&X-Goog-Signature=209ebe429524051b423be274bd041c4d31b82f2bbb00220833f3ff04126840b2dee2be3c46c85b3deb7bb3861c3dc8e7de10579d4dd374c7d7b8279a312c931fb154451ae8d2f4eb09f7cd409996b66c4b0be34d4364d831d7b6e60142a46e4d3669f11913f84daa5df63d922e27de793a3c319023b8fa19460e45d9a6525e0c56298b5abc07699721dcb7a868caf71f3bc26631b638bec480e869e7de1a79ff1c73de4c43d643921ebe8ff5c44643f692bead31f5298a76c2806aef3f5e6e8fe6757409e1d55ed7b01a0ebe9431efba6cf0dfa76b021e6ff876b3a0dbdf3e8219c6e06e9b7eb22c469dc8c5aabef9a6a03c25b0b2afb6bb94c0b85060325db6\n"
          ]
        }
      ],
      "source": [
        "// Example: Extract structured content (tables, paragraphs, figures)\n",
        "const DOC_URL = \"https://storage.googleapis.com/vlm-data-public-prod/hub/examples/healthcare.patient-referral/handwritten-patient-referral.pdf\";\n",
        "\n",
        "const result = await chatCompletion(\n",
        "    \"Extract all handwritten text fields and ground the text. Also return the image as a presigned url\",\n",
        "    undefined,\n",
        "    [DOC_URL],\n",
        "    DetectionsResponseSchema\n",
        ") as DetectionsResponse;\n",
        "\n",
        "console.log(\">> STRUCTURED OCR RESULT\");\n",
        "console.log(\">> RESPONSE\");\n",
        "console.log(`Found ${result.detections.length} text fields`);\n",
        "result.detections.slice(0, 10).forEach((det, i) => {\n",
        "    console.log(`  ${i + 1}. ${det.label}: xywh=[${det.xywh.map(v => v.toFixed(3)).join(\", \")}]`);\n",
        "});\n",
        "if (result.detections.length > 10) {\n",
        "    console.log(`  ... and ${result.detections.length - 10} more`);\n",
        "}\n",
        "if (result.image_url) {\n",
        "    console.log(\"\\n>> Document image URL:\", result.image_url);\n",
        "}\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 2. Layout Detection\n",
        "\n",
        "Detect document structure including headers, footers, tables, figures, text blocks, and other layout elements.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            ">> LAYOUT DETECTION RESULT\n",
            "Found 27 layout elements\n",
            "  1. section-header: xywh=[0.353, 0.042, 0.276, 0.019]\n",
            "  2. picture: xywh=[0.013, 0.098, 0.273, 0.209]\n",
            "  3. picture: xywh=[0.280, 0.099, 0.342, 0.126]\n",
            "  4. text: xywh=[0.294, 0.163, 0.253, 0.070]\n",
            "  5. text: xywh=[0.622, 0.125, 0.336, 0.193]\n",
            "  6. picture: xywh=[0.329, 0.238, 0.283, 0.182]\n",
            "  7. text: xywh=[0.035, 0.345, 0.255, 0.159]\n",
            "  8. picture: xywh=[0.176, 0.409, 0.458, 0.150]\n",
            "  9. text: xywh=[0.283, 0.440, 0.220, 0.070]\n",
            "  10. picture: xywh=[0.630, 0.337, 0.340, 0.208]\n",
            "  11. section-header: xywh=[0.402, 0.601, 0.176, 0.016]\n",
            "  12. section-header: xywh=[0.102, 0.638, 0.160, 0.015]\n",
            "  13. text: xywh=[0.030, 0.677, 0.268, 0.040]\n",
            "  14. text: xywh=[0.030, 0.738, 0.275, 0.042]\n",
            "  15. text: xywh=[0.030, 0.803, 0.253, 0.041]\n",
            "  16. text: xywh=[0.030, 0.866, 0.292, 0.042]\n",
            "  17. section-header: xywh=[0.380, 0.639, 0.223, 0.015]\n",
            "  18. text: xywh=[0.337, 0.688, 0.302, 0.053]\n",
            "  19. text: xywh=[0.337, 0.764, 0.286, 0.042]\n",
            "  20. text: xywh=[0.337, 0.828, 0.303, 0.066]\n",
            "  ... and 7 more\n",
            "\n",
            ">> DOCUMENT IMAGE URL: https://storage.googleapis.com/vlm-data-public-prod/hub/examples/document.layout/blackhole.jpeg\n"
          ]
        }
      ],
      "source": [
        "// Example: Detect document layout elements\n",
        "const IMAGE_URL = \"https://storage.googleapis.com/vlm-data-public-prod/hub/examples/document.layout/blackhole.jpeg\";\n",
        "\n",
        "const result = await chatCompletion(\n",
        "    \"Detect and identify all layout elements in this document including headers, footers, titles, paragraphs, tables, figures, and text blocks. Return bounding boxes for each element with their categories.\",\n",
        "    [IMAGE_URL],\n",
        "    undefined,\n",
        "    LayoutDetectionResponseSchema\n",
        ") as LayoutDetectionResponse;\n",
        "\n",
        "console.log(\">> LAYOUT DETECTION RESULT\");\n",
        "console.log(`Found ${result.elements.length} layout elements`);\n",
        "result.elements.slice(0, 20).forEach((element, idx) => {\n",
        "    console.log(`  ${idx + 1}. ${element.category}: xywh=[${element.xywh.map(v => v.toFixed(3)).join(\", \")}]`);\n",
        "    if (element.text) {\n",
        "        console.log(`     Text: ${element.text.substring(0, 50)}...`);\n",
        "    }\n",
        "});\n",
        "if (result.elements.length > 20) {\n",
        "    console.log(`  ... and ${result.elements.length - 20} more`);\n",
        "}\n",
        "console.log(\"\\n>> DOCUMENT IMAGE URL:\", IMAGE_URL);\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 2b. Layout Detection with Categories\n",
        "\n",
        "Detect specific layout categories like tables, figures, and text blocks.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            ">> STRUCTURED LAYOUT Category RESULT\n",
            ">> RESPONSE\n",
            "Found 11 layout elements\n",
            "  1. figure: xywh=[0.088, 0.036, 0.827, 0.940]\n",
            "  2. page-header: xywh=[0.923, 0.044, 0.013, 0.012]\n",
            "  3. page-header: xywh=[0.923, 0.512, 0.045, 0.204]\n",
            "  4. text: xywh=[0.923, 0.737, 0.015, 0.009]\n",
            "  5. text: xywh=[0.940, 0.737, 0.043, 0.009]\n",
            "  6. figure: xywh=[0.923, 0.755, 0.060, 0.065]\n",
            "  7. text: xywh=[0.923, 0.823, 0.028, 0.009]\n",
            "  8. text: xywh=[0.931, 0.844, 0.032, 0.041]\n",
            "  9. text: xywh=[0.923, 0.900, 0.038, 0.009]\n",
            "  10. text: xywh=[0.933, 0.923, 0.022, 0.026]\n",
            "  11. text: xywh=[0.840, 0.962, 0.075, 0.011]\n",
            "\n",
            ">> Document image URL: https://storage.googleapis.com/vlm-userdata-prod/agents/artifacts/ae8ae740-ddd6-426b-bde1-75540f99f277/6c145972-c710-4327-ab59-660e99cf09b3/img_ba5078.jpg?X-Goog-Algorithm=GOOG4-RSA-SHA256&X-Goog-Credential=vlm-deployments%40vlm-infra-prod.iam.gserviceaccount.com%2F20251219%2Fauto%2Fstorage%2Fgoog4_request&X-Goog-Date=20251219T103813Z&X-Goog-Expires=604800&X-Goog-SignedHeaders=host&X-Goog-Signature=1a7f77b8860b23d96c78f3c535dc547c90bc836f8c69ca7fe7411cbdc7eba10f02339ed3f06a5b0ec706f06f6799e71002b3310a779d6679220fcc38ff5c7f9b8c978fc4ada37e75b99c33ae6439b14c59a918637048d324b3bcaf5ef336cfdf7bfbcd23d65b42a5c8e54b69ceb7bf1b721bb5f0653d33364a1a9020de1d1afdd107f02f3eee7700b951a00406340c02d32042e813c00e70450aa6a42f23a847e48c7ba6a05836318918b1dbadd6\n"
          ]
        }
      ],
      "source": [
        "// Example: Detect specific layout categories\n",
        "const DOC_URL = \"https://storage.googleapis.com/vlm-data-public-prod/hub/examples/construction.markdown/sample-construction-plan-set.pdf\";\n",
        "\n",
        "const result = await chatCompletion(\n",
        "    \"Detect all tables, figures, and section headers in the first page of this document. Provide bounding boxes of each detected element and return all detections. Return also pre signed url of the image\",\n",
        "    undefined,\n",
        "    [DOC_URL],\n",
        "    DetectionsResponseSchema\n",
        ") as DetectionsResponse;\n",
        "\n",
        "console.log(\">> STRUCTURED LAYOUT Category RESULT\");\n",
        "console.log(\">> RESPONSE\");\n",
        "console.log(`Found ${result.detections.length} layout elements`);\n",
        "result.detections.forEach((det, i) => {\n",
        "    console.log(`  ${i + 1}. ${det.label}: xywh=[${det.xywh.map(v => v.toFixed(3)).join(\", \")}]`);\n",
        "});\n",
        "if (result.image_url) {\n",
        "    console.log(\"\\n>> Document image URL:\", result.image_url);\n",
        "}\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 3. Document Redaction\n",
        "\n",
        "Detect sensitive information and apply blurring to redact it from the document.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            ">> REDACTION RESULT\n",
            "{\n",
            "  url: \"https://storage.googleapis.com/vlm-userdata-prod/agents/artifacts/ae8ae740-ddd6-426b-bde1-75540f99f277/6a1ca916-2d0e-443b-96ba-7ae9b5d76690/img_44a2ea.jpg?X-Goog-Algorithm=GOOG4-RSA-SHA256&X-Goog-Credential=vlm-deployments%40vlm-infra-prod.iam.gserviceaccount.com%2F20251219%2Fauto%2Fstorage%2Fgoog4_request&X-Goog-Date=20251219T103947Z&X-Goog-Expires=604800&X-Goog-SignedHeaders=host&X-Goog-Signature=75f588a6093c66f2bc9965ef3153237765faf907c4d53ca03aec44af0046ce4d597530e8d39e111996c51a673d50e15ebb69f20b3393ef631a0928da59661b526e54d701be2417089b04aaa7c18f6d6e2f46a5957155087b744a74253516f811a684758eb102ab0c7fe52a2da833ec48388bd49149ec030e8f46df03d993b3965cf9d661c78d66833a621ced28327d85bec774846862a7f6773c5aacdbc29f53d628c290d988ee6a0b1538de12602465c9b68395c60a9011718e5ed783189fcd34f169145834152b1dea0d1fa9af5d37d2e4025612521f321fdb2a0a915b04fd5b602bcc8bd4a23d451349ee3aa390f66b5a3030d90a68d7e39bd3ef00d2693c\"\n",
            "}\n",
            "\n",
            ">> REDACTED DOCUMENT URL: https://storage.googleapis.com/vlm-userdata-prod/agents/artifacts/ae8ae740-ddd6-426b-bde1-75540f99f277/6a1ca916-2d0e-443b-96ba-7ae9b5d76690/img_44a2ea.jpg?X-Goog-Algorithm=GOOG4-RSA-SHA256&X-Goog-Credential=vlm-deployments%40vlm-infra-prod.iam.gserviceaccount.com%2F20251219%2Fauto%2Fstorage%2Fgoog4_request&X-Goog-Date=20251219T103947Z&X-Goog-Expires=604800&X-Goog-SignedHeaders=host&X-Goog-Signature=75f588a6093c66f2bc9965ef3153237765faf907c4d53ca03aec44af0046ce4d597530e8d39e111996c51a673d50e15ebb69f20b3393ef631a0928da59661b526e54d701be2417089b04aaa7c18f6d6e2f46a5957155087b744a74253516f811a684758eb102ab0c7fe52a2da833ec48388bd49149ec030e8f46df03d993b3965cf9d661c78d66833a621ced28327d85bec774846862a7f6773c5aacdbc29f53d628c290d988ee6a0b1538de12602465c9b68395c60a9011718e5ed783189fcd34f169145834152b1dea0d1fa9af5d37d2e4025612521f321fdb2a0a915b04fd5b602bcc8bd4a23d451349ee3aa390f66b5a3030d90a68d7e39bd3ef00d2693c\n"
          ]
        }
      ],
      "source": [
        "// Example: Detect and blur sensitive information\n",
        "const DOC_URL = \"https://storage.googleapis.com/vlm-data-public-prod/hub/examples/document.markdown/playground/1.pdf\";\n",
        "\n",
        "const result = await chatCompletion(\n",
        "    \"Detect all sensitive information (names, addresses, phone numbers, emails, dates) in this document and blur them to redact the information. Return the redacted image.\",\n",
        "    undefined,\n",
        "    [DOC_URL],\n",
        "    ImageUrlResponseSchema\n",
        ") as ImageUrlResponse;\n",
        "\n",
        "console.log(\">> REDACTION RESULT\");\n",
        "console.log(result);\n",
        "console.log(\"\\n>> REDACTED DOCUMENT URL:\", result.url);\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 4. Multi-Document Classification\n",
        "\n",
        "Classify documents into categories based on their content, structure, and visual features.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            ">> RESULT\n",
            "I have successfully extracted the patient referral page, medical insurance card, and identification form from the document. Here is the information in JSON format:\n",
            "\n",
            "```json\n",
            "{\n",
            "  \"patient_referral_page\": \"www.cviga.org\\nCENTER\\nFOR THE\\nVISUALLY\\nIMPAIRED\\nPatient Referral Form\\nPlease fax this form to CVI at 404-875-4568\\nPatient Name: Samuel Jackson\\nDate of Birth: 6/4/72 Patient's Phone: 847-292-8014\\nAddress: 1643 Elmwood Drive\\nCity/State/Zip: Treeslave, NY 10027\\nPreferred Contact Name and Number (if other than patient):\\nDiagnosis: Retinal Detachment\\nVisual Acuities: Distance cc OD:\\n20/40\\ncc OS\\n20160\\nVisual Fields (please fax field chart if available):\\nReferred by:\\nPhysician's name (please print): Johu\\nPhysician's signature: Ph\\nUPIN:\\nFirst\\nA\\nMiddle\\nTravolta\\nLast\\nNPI: 1134562341 Phone:\\n724-891-0707\\nAddress: 583 Raven Lame\\nCity, State, Zip: Elmsford, NY 10734\\nReferral Date: 8128/24 Date of Office Visit: 8/25/24\\nHow did you hear about CVI:\\nQuestions? Contact Client Services at 404.875.9011\\n739 West Peachtree St. NW Atlanta, GA 30308 Ph: 404.875.9011 Fax: 404-875-4568\\nAccredited Member, National Accreditation Council United Way Agency\",\n",
            "  \"medical_insurance_card\": \"Anthem.\\nSAMPLE, JOHN\\nMember ID: BKJ1234567CB\\nGroup:\\nPlan Code:\\nRxBIN:\\nRxPCN:\\nRxGRP:\\nProducts:\\nL00003\\n051\\n020099\\nWG\\nWLEA\\nMedical/Rx\\nHealth and Wellness Services\\nUNIVERSITY OF COLORADO BOULDER\\nUnderwritten by Rocky Mountain\\nHospital and Medical Service, Inc.\\nOffice Visit $40 / Spec $40 / MH $20\\nER Co-pay\\nOSE Urgent Care Co-pay\\nDeductible\\nRx Co-pay\\n$150\\n$75\\n$500\\n$25 / $45/$75\\nDeductible & coinsurance may apply\\nCO-DOI\\nPPO\\nUniversity of Colorado Boulder has hired\\nAmeriBen to handle member contact for\\nhealth plan administration. See back for\\ncontact information.\\n20200611T43 Sh: 0 Bin 1\\nJ041 Env [1] CSets 1 of 1\\n1112-XX 4C36 0820015-01-- M()D()V()\",\n",
            "  \"identification_form\": \"WCC\\nToday's Date: 04/30/2025\\nName:\\nSarah\\n(first)\\nDOB (mm/dd/yyyy): 05\\nCurrent Address:\\n123 Maple Street\\nRiverdale\\n(city)\\n/12\\n/1985\\nWorld Central Community Clinic\\nPatient Identification Form\\nIdentification Information\\nJane\\nThompson\\n(middle)\\n(last)\\n(street)\\nNY\\n(state)\\n4B\\n(apt)\\n10463\\n(zip)\\nWCC\\nCurrent Phone Number: (212) 555-7890\\nEmail Address: sarah.thompson@email.com\\nMother's Name: Elizabeth Thompson\\nFather's Name: Robert Thompson\\nEmergency contact (relationship may be listed): Michael Thompson (Brother) - (212) 555-4321\\nAllergies:\\nPenicillin, Shellfish, Peanuts\\nMedications you are taking:\\nLisinopril 10mg daily, Levothyroxine 50mcg daily, Vitamin D 2000 IU\\nThe WCC considers all important health topics and affordable healthcare access a priority. World Central medical professionals are here to assist\\nyou in the transition to your usual doctor.\\nYes\\nNo\\n*Please note: All visits provided by the WCC Traditional Village or its Divisions, (those all personally or medically required for each of our\\npatients, if you receive a bill, we ask that you contact the clinic immediately) are completely free of charge. Please bring this form to your\\nappointment. Thank you for your time and your patience contacting us for services.\\nLast updated: 03/17/22 19:36\\nPage 1 of 1\"\n",
            "}\n",
            "```\n"
          ]
        }
      ],
      "source": [
        "// Example: Classify the multi-page document and give each page class\n",
        "const DOC_URL = \"https://storage.googleapis.com/vlm-data-public-prod/hub/examples/document.agent/multi-document-input-example.pdf\";\n",
        "\n",
        "const result = await chatCompletion(\n",
        "    \"Analyze this multi-page medical document set. Extract patient referral page, medical insurance card and identification form in 3 separate fields in JSON format.\",\n",
        "    undefined,\n",
        "    [DOC_URL]\n",
        ");\n",
        "\n",
        "console.log(\">> RESULT\");\n",
        "console.log(result);\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Conclusion\n",
        "\n",
        "This cookbook demonstrated the comprehensive document understanding capabilities of the **VLM Run Orion Agent API** using Node.js/TypeScript.\n",
        "\n",
        "### Key Takeaways\n",
        "\n",
        "1. **OCR Capabilities**: Extract text, tables, paragraphs, and figures from documents with high accuracy using advanced OCR models.\n",
        "\n",
        "2. **Layout Detection**: Identify document structure including headers, footers, tables, figures, text blocks, and other layout elements with precise bounding boxes.\n",
        "\n",
        "3. **Document Redaction**: Detect and redact sensitive information including:\n",
        "   - PII (Personally Identifiable Information)\n",
        "   - Financial data (account numbers, SSNs, credit cards)\n",
        "   - PHI (Protected Health Information) for HIPAA compliance\n",
        "   - Domain-specific sensitive data (legal, insurance, real estate, etc.)\n",
        "\n",
        "4. **Multi-Document Classification**: Classify documents into categories based on content, structure, and visual features with confidence scores and rationales.\n",
        "\n",
        "5. **OpenAI-Compatible Interface**: The API follows the OpenAI chat completions format, making it easy to integrate with existing workflows and tools.\n",
        "\n",
        "6. **Structured Outputs**: Use Zod schemas with `response_format` parameter to get type-safe, validated responses with automatic parsing.\n",
        "\n",
        "7. **Streaming Support**: For long-running document processing tasks, enable streaming to receive partial results as they become available.\n",
        "\n",
        "8. **Combined Workflows**: Combine OCR, layout detection, redaction, and classification in a single pipeline for comprehensive document understanding.\n",
        "\n",
        "### Use Cases\n",
        "\n",
        "- **Document Processing**: Automate extraction of structured data from invoices, receipts, forms, and contracts\n",
        "- **Compliance**: Ensure HIPAA, GDPR, PCI-DSS, and other regulatory compliance through automated redaction\n",
        "- **Document Management**: Classify and organize large document collections\n",
        "- **Data Extraction**: Extract tables, figures, and structured content from documents\n",
        "- **Privacy Protection**: Automatically detect and redact sensitive information before sharing documents\n",
        "\n",
        "### Next Steps\n",
        "\n",
        "- Explore the [VLM Run Documentation](https://docs.vlm.run) for more details\n",
        "- Join our [Discord community](https://discord.gg/AMApC2UzVY) for support\n",
        "- Check out more examples in the [VLM Run Cookbook](https://github.com/vlm-run/vlmrun-cookbook)\n",
        "- Review the [VLM Run Node.js SDK](https://github.com/vlm-run/vlmrun-node-sdk) documentation\n",
        "- Review domain-specific redaction agents for financial, healthcare, legal, and other industries\n",
        "\n",
        "Happy building!\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Deno",
      "language": "typescript",
      "name": "deno"
    },
    "language_info": {
      "codemirror_mode": "typescript",
      "file_extension": ".ts",
      "mimetype": "text/x.typescript",
      "name": "typescript",
      "nbconvert_exporter": "script",
      "pygments_lexer": "typescript",
      "version": "5.9.2"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
