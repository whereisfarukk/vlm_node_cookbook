{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "<div align=\"center\">\n",
        "<p align=\"center\" style=\"width: 100%;\">\n",
        "    <img src=\"https://raw.githubusercontent.com/vlm-run/.github/refs/heads/main/profile/assets/vlm-black.svg\" alt=\"VLM Run Logo\" width=\"80\" style=\"margin-bottom: -5px; color: #2e3138; vertical-align: middle; padding-right: 5px;\"><br>\n",
        "</p>\n",
        "<p align=\"center\"><a href=\"https://docs.vlm.run\"><b>Website</b></a> | <a href=\"https://docs.vlm.run/\"><b>API Docs</b></a> | <a href=\"https://docs.vlm.run/blog\"><b>Blog</b></a> | <a href=\"https://discord.gg/AMApC2UzVY\"><b>Discord</b></a> | <a href=\"https://chat.vlm.run\"><b>Chat</b></a>\n",
        "</p>\n",
        "</div>\n",
        "\n",
        "# VLM Run Orion - Document Understanding (Node.js)\n",
        "\n",
        "This comprehensive cookbook demonstrates [VLM Run Orion's](https://vlm.run/orion) document understanding capabilities using **Node.js/TypeScript** including OCR, layout detection, redaction, and multi-document classification. For more details on the API, see the [Agent API docs](https://docs.vlm.run/agents/introduction).\n",
        "\n",
        "For this notebook, we'll cover how to use the **VLM Run Agent Chat Completions API** - an OpenAI-compatible interface for building powerful document intelligence with the same familiar chat-completions interface.\n",
        "\n",
        "We'll cover the following topics:\n",
        " 1. **OCR (Optical Character Recognition)** - Extract text, tables, paragraphs, and figures from documents\n",
        " 2. **Layout Detection** - Identify document structure (headers, footers, tables, figures, etc.)\n",
        " 3. **Document Redaction** - Detect and redact sensitive information (PII, financial data, PHI, etc.)\n",
        " 4. **Multi-Document Classification** - Classify documents into categories based on content and structure\n",
        "\n",
        "## Prerequisites\n",
        "\n",
        "- Node.js 18+\n",
        "- VLM Run API key (get one at [app.vlm.run](https://app.vlm.run))\n",
        "- Deno or tslab kernel for running TypeScript in Jupyter\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Setup\n",
        "\n",
        "First, install the required packages and configure the environment.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "vscode": {
          "languageId": "typescript"
        }
      },
      "outputs": [],
      "source": [
        "// Install the VLM Run SDK\n",
        "// npm install vlmrun openai zod zod-to-json-schema\n",
        "\n",
        "// If using Deno kernel, install dependencies via npm specifiers\n",
        "// For tslab, run: npm install vlmrun openai zod zod-to-json-schema in your project directory\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "vscode": {
          "languageId": "typescript"
        }
      },
      "outputs": [],
      "source": [
        "// Import the VLM Run SDK and dependencies\n",
        "import { VlmRun } from \"npm:vlmrun\";\n",
        "import { z } from \"npm:zod\";\n",
        "import { zodToJsonSchema } from \"npm:zod-to-json-schema\";\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "vscode": {
          "languageId": "typescript"
        }
      },
      "outputs": [],
      "source": [
        "// Get API key from environment variable\n",
        "const VLMRUN_API_KEY = Deno.env.get(\"VLMRUN_API_KEY\");\n",
        "\n",
        "if (!VLMRUN_API_KEY) {\n",
        "    throw new Error(\"Please set the VLMRUN_API_KEY environment variable\");\n",
        "}\n",
        "\n",
        "console.log(\"✓ API Key loaded successfully\");\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Initialize the VLM Run Client\n",
        "\n",
        "We use the OpenAI-compatible chat completions interface through the VLM Run SDK.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "vscode": {
          "languageId": "typescript"
        }
      },
      "outputs": [],
      "source": [
        "// Initialize the VLM Run client using the SDK\n",
        "const client = new VlmRun({\n",
        "    apiKey: VLMRUN_API_KEY,\n",
        "    baseURL: \"https://agent.vlm.run/v1\"  // Use the agent API endpoint\n",
        "});\n",
        "\n",
        "console.log(\"✓ VLM Run client initialized successfully!\");\n",
        "console.log(\"Base URL: https://agent.vlm.run/v1\");\n",
        "console.log(\"Model: vlmrun-orion-1\");\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Response Models (Schemas)\n",
        "\n",
        "We define Zod schemas for structured outputs. These schemas provide type-safe, validated responses for document understanding tasks.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "vscode": {
          "languageId": "typescript"
        }
      },
      "outputs": [],
      "source": [
        "// Helper function to download images from URLs\n",
        "async function downloadImage(url: string): Promise<Uint8Array> {\n",
        "    const response = await fetch(url);\n",
        "    if (!response.ok) {\n",
        "        throw new Error(`Failed to download image: ${response.statusText}`);\n",
        "    }\n",
        "    return new Uint8Array(await response.arrayBuffer());\n",
        "}\n",
        "\n",
        "// Image URL Response Schema\n",
        "const ImageUrlResponseSchema = z.object({\n",
        "    url: z.string().describe(\"Pre-signed URL to the image\")\n",
        "});\n",
        "\n",
        "type ImageUrlResponse = z.infer<typeof ImageUrlResponseSchema>;\n",
        "\n",
        "// Document URL Response Schema\n",
        "const DocumentUrlResponseSchema = z.object({\n",
        "    url: z.string().describe(\"Pre-signed URL to the document\")\n",
        "});\n",
        "\n",
        "type DocumentUrlResponse = z.infer<typeof DocumentUrlResponseSchema>;\n",
        "\n",
        "// Image URL List Response Schema\n",
        "const ImageUrlListResponseSchema = z.object({\n",
        "    urls: z.array(ImageUrlResponseSchema).describe(\"List of pre-signed image URL responses\")\n",
        "});\n",
        "\n",
        "type ImageUrlListResponse = z.infer<typeof ImageUrlListResponseSchema>;\n",
        "\n",
        "// Detection Schema\n",
        "const DetectionSchema = z.object({\n",
        "    label: z.string().describe(\"Name of the detected object or text\"),\n",
        "    xywh: z.tuple([z.number(), z.number(), z.number(), z.number()])\n",
        "        .describe(\"Bounding box (x, y, width, height) normalized from 0-1\"),\n",
        "    confidence: z.number().nullable().optional().describe(\"Detection confidence score from 0-1\")\n",
        "});\n",
        "\n",
        "// Detections Response Schema\n",
        "const DetectionsResponseSchema = z.object({\n",
        "    detections: z.array(DetectionSchema).describe(\"List of detected objects or text or layout elements with bounding boxes\"),\n",
        "    image_url: z.string().optional().describe(\"Url to the image for the detections\")\n",
        "});\n",
        "\n",
        "type DetectionsResponse = z.infer<typeof DetectionsResponseSchema>;\n",
        "\n",
        "// Document Classification Response Schema\n",
        "const DocumentClassificationResponseSchema = z.object({\n",
        "    rationale: z.string().describe(\"Rationale for the classification\"),\n",
        "    domain: z.string().describe(\"The classified domain of the document\"),\n",
        "    confidence: z.string().describe(\"Confidence level: hi, med, or lo\"),\n",
        "    tags: z.array(z.string()).nullable().optional().describe(\"List of tags describing the document\")\n",
        "});\n",
        "\n",
        "type DocumentClassificationResponse = z.infer<typeof DocumentClassificationResponseSchema>;\n",
        "\n",
        "// Layout Element Schema\n",
        "const LayoutElementSchema = z.object({\n",
        "    category: z.string().describe(\"Category: caption, footnote, formula, list-item, page-footer, page-header, picture, section-header, table, text, title\"),\n",
        "    xywh: z.tuple([z.number(), z.number(), z.number(), z.number()])\n",
        "        .describe(\"Bounding box (x, y, width, height) normalized from 0-1\"),\n",
        "    text: z.string().nullable().optional().describe(\"Text content of the element if available\")\n",
        "});\n",
        "\n",
        "// Layout Detection Response Schema\n",
        "const LayoutDetectionResponseSchema = z.object({\n",
        "    elements: z.array(LayoutElementSchema).describe(\"List of detected layout elements\")\n",
        "});\n",
        "\n",
        "type LayoutDetectionResponse = z.infer<typeof LayoutDetectionResponseSchema>;\n",
        "\n",
        "// Sensitive Item Schema\n",
        "const SensitiveItemSchema = z.object({\n",
        "    item_type: z.string().describe(\"Type of sensitive information\"),\n",
        "    value: z.string().describe(\"The detected sensitive value\")\n",
        "});\n",
        "\n",
        "// Redaction Detection Response Schema\n",
        "const RedactionDetectionResponseSchema = z.object({\n",
        "    detected_items: z.array(SensitiveItemSchema).describe(\"List of detected sensitive items\")\n",
        "});\n",
        "\n",
        "type RedactionDetectionResponse = z.infer<typeof RedactionDetectionResponseSchema>;\n",
        "\n",
        "console.log(\"✓ Response schemas defined successfully!\");\n",
        "console.log(\"Schemas include type-safe validation for structured outputs.\");\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "vscode": {
          "languageId": "typescript"
        }
      },
      "outputs": [],
      "source": [
        "/**\n",
        " * Make a chat completion request with optional images, documents, and structured output.\n",
        " * \n",
        " * @param prompt - The text prompt/instruction\n",
        " * @param images - Optional list of images to process (URLs)\n",
        " * @param documents - Optional list of documents to process (URLs)\n",
        " * @param responseSchema - Optional Zod schema for structured output\n",
        " * @param model - Model to use (default: vlmrun-orion-1:auto)\n",
        " * @returns Parsed response if responseSchema provided, else raw response text\n",
        " */\n",
        "async function chatCompletion<T>(\n",
        "    prompt: string,\n",
        "    images?: string[],\n",
        "    documents?: string[],\n",
        "    responseSchema?: z.ZodSchema<T>,\n",
        "    model: string = \"vlmrun-orion-1:auto\"\n",
        "): Promise<T | string> {\n",
        "    const content: any[] = [];\n",
        "    content.push({ type: \"text\", text: prompt });\n",
        "\n",
        "    // Add documents first (if any)\n",
        "    if (documents) {\n",
        "        for (const doc of documents) {\n",
        "            if (typeof doc === \"string\") {\n",
        "                if (!doc.startsWith(\"http\")) {\n",
        "                    throw new Error(\"Document URLs must start with http or https\");\n",
        "                }\n",
        "                content.push({\n",
        "                    type: \"file_url\",\n",
        "                    file_url: { url: doc, detail: \"auto\" }\n",
        "                });\n",
        "            }\n",
        "        }\n",
        "    }\n",
        "\n",
        "    // Add images (if any)\n",
        "    if (images) {\n",
        "        for (const image of images) {\n",
        "            if (typeof image === \"string\") {\n",
        "                if (!image.startsWith(\"http\")) {\n",
        "                    throw new Error(\"Image URLs must start with http or https\");\n",
        "                }\n",
        "                content.push({\n",
        "                    type: \"image_url\",\n",
        "                    image_url: { url: image, detail: \"auto\" }\n",
        "                });\n",
        "            }\n",
        "        }\n",
        "    }\n",
        "\n",
        "    const kwargs: any = {\n",
        "        model: model,\n",
        "        messages: [{ role: \"user\", content: content }]\n",
        "    };\n",
        "\n",
        "    if (responseSchema) {\n",
        "        kwargs.response_format = {\n",
        "            type: \"json_schema\",\n",
        "            schema: zodToJsonSchema(responseSchema)\n",
        "        } as any;\n",
        "    }\n",
        "\n",
        "    const response = await client.agent.completions.create(kwargs);\n",
        "    const responseText = response.choices[0].message.content || \"\";\n",
        "\n",
        "    if (responseSchema) {\n",
        "        const parsed = JSON.parse(responseText);\n",
        "        return responseSchema.parse(parsed) as T;\n",
        "    }\n",
        "\n",
        "    return responseText;\n",
        "}\n",
        "\n",
        "console.log(\"✓ Helper functions defined!\");\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 1. OCR (Optical Character Recognition)\n",
        "\n",
        "Extract text, tables, paragraphs, and figures from documents using OCR capabilities.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "vscode": {
          "languageId": "typescript"
        }
      },
      "outputs": [],
      "source": [
        "// Example: Extract text from a document image\n",
        "const IMAGE_URL = \"https://storage.googleapis.com/vlm-data-public-prod/hub/examples/agent_use_cases/hand_writting_beautification/image-ocr.jpg\";\n",
        "\n",
        "const result = await chatCompletion(\n",
        "    \"Extract all text from this document image. Return the full text content with proper formatting.\",\n",
        "    [IMAGE_URL]\n",
        ");\n",
        "\n",
        "console.log(\">> OCR RESULT\");\n",
        "console.log(result);\n",
        "console.log(\"\\n>> DOCUMENT IMAGE URL:\", IMAGE_URL);\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 1b. OCR with Structured Extraction\n",
        "\n",
        "Extract structured information including tables, paragraphs, and figures from documents.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "vscode": {
          "languageId": "typescript"
        }
      },
      "outputs": [],
      "source": [
        "// Example: Extract structured content (tables, paragraphs, figures)\n",
        "const DOC_URL = \"https://storage.googleapis.com/vlm-data-public-prod/hub/examples/healthcare.patient-referral/handwritten-patient-referral.pdf\";\n",
        "\n",
        "const result = await chatCompletion(\n",
        "    \"Extract all handwritten text fields and ground the text. Also return the image as a presigned url\",\n",
        "    undefined,\n",
        "    [DOC_URL],\n",
        "    DetectionsResponseSchema\n",
        ") as DetectionsResponse;\n",
        "\n",
        "console.log(\">> STRUCTURED OCR RESULT\");\n",
        "console.log(\">> RESPONSE\");\n",
        "console.log(`Found ${result.detections.length} text fields`);\n",
        "result.detections.slice(0, 10).forEach((det, i) => {\n",
        "    console.log(`  ${i + 1}. ${det.label}: xywh=[${det.xywh.map(v => v.toFixed(3)).join(\", \")}]`);\n",
        "});\n",
        "if (result.detections.length > 10) {\n",
        "    console.log(`  ... and ${result.detections.length - 10} more`);\n",
        "}\n",
        "if (result.image_url) {\n",
        "    console.log(\"\\n>> Document image URL:\", result.image_url);\n",
        "}\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 2. Layout Detection\n",
        "\n",
        "Detect document structure including headers, footers, tables, figures, text blocks, and other layout elements.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "vscode": {
          "languageId": "typescript"
        }
      },
      "outputs": [],
      "source": [
        "// Example: Detect document layout elements\n",
        "const IMAGE_URL = \"https://storage.googleapis.com/vlm-data-public-prod/hub/examples/document.layout/blackhole.jpeg\";\n",
        "\n",
        "const result = await chatCompletion(\n",
        "    \"Detect and identify all layout elements in this document including headers, footers, titles, paragraphs, tables, figures, and text blocks. Return bounding boxes for each element with their categories.\",\n",
        "    [IMAGE_URL],\n",
        "    undefined,\n",
        "    LayoutDetectionResponseSchema\n",
        ") as LayoutDetectionResponse;\n",
        "\n",
        "console.log(\">> LAYOUT DETECTION RESULT\");\n",
        "console.log(`Found ${result.elements.length} layout elements`);\n",
        "result.elements.slice(0, 20).forEach((element, idx) => {\n",
        "    console.log(`  ${idx + 1}. ${element.category}: xywh=[${element.xywh.map(v => v.toFixed(3)).join(\", \")}]`);\n",
        "    if (element.text) {\n",
        "        console.log(`     Text: ${element.text.substring(0, 50)}...`);\n",
        "    }\n",
        "});\n",
        "if (result.elements.length > 20) {\n",
        "    console.log(`  ... and ${result.elements.length - 20} more`);\n",
        "}\n",
        "console.log(\"\\n>> DOCUMENT IMAGE URL:\", IMAGE_URL);\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 2b. Layout Detection with Categories\n",
        "\n",
        "Detect specific layout categories like tables, figures, and text blocks.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "vscode": {
          "languageId": "typescript"
        }
      },
      "outputs": [],
      "source": [
        "// Example: Detect specific layout categories\n",
        "const DOC_URL = \"https://storage.googleapis.com/vlm-data-public-prod/hub/examples/construction.markdown/sample-construction-plan-set.pdf\";\n",
        "\n",
        "const result = await chatCompletion(\n",
        "    \"Detect all tables, figures, and section headers in the first page of this document. Provide bounding boxes of each detected element and return all detections. Return also pre signed url of the image\",\n",
        "    undefined,\n",
        "    [DOC_URL],\n",
        "    DetectionsResponseSchema\n",
        ") as DetectionsResponse;\n",
        "\n",
        "console.log(\">> STRUCTURED LAYOUT Category RESULT\");\n",
        "console.log(\">> RESPONSE\");\n",
        "console.log(`Found ${result.detections.length} layout elements`);\n",
        "result.detections.forEach((det, i) => {\n",
        "    console.log(`  ${i + 1}. ${det.label}: xywh=[${det.xywh.map(v => v.toFixed(3)).join(\", \")}]`);\n",
        "});\n",
        "if (result.image_url) {\n",
        "    console.log(\"\\n>> Document image URL:\", result.image_url);\n",
        "}\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 3. Document Redaction\n",
        "\n",
        "Detect sensitive information and apply blurring to redact it from the document.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "vscode": {
          "languageId": "typescript"
        }
      },
      "outputs": [],
      "source": [
        "// Example: Detect and blur sensitive information\n",
        "const DOC_URL = \"https://storage.googleapis.com/vlm-data-public-prod/hub/examples/document.markdown/playground/1.pdf\";\n",
        "\n",
        "const result = await chatCompletion(\n",
        "    \"Detect all sensitive information (names, addresses, phone numbers, emails, dates) in this document and blur them to redact the information. Return the redacted image.\",\n",
        "    undefined,\n",
        "    [DOC_URL],\n",
        "    ImageUrlResponseSchema\n",
        ") as ImageUrlResponse;\n",
        "\n",
        "console.log(\">> REDACTION RESULT\");\n",
        "console.log(result);\n",
        "console.log(\"\\n>> REDACTED DOCUMENT URL:\", result.url);\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 4. Multi-Document Classification\n",
        "\n",
        "Classify documents into categories based on their content, structure, and visual features.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "vscode": {
          "languageId": "typescript"
        }
      },
      "outputs": [],
      "source": [
        "// Example: Classify the multi-page document and give each page class\n",
        "const DOC_URL = \"https://storage.googleapis.com/vlm-data-public-prod/hub/examples/document.agent/multi-document-input-example.pdf\";\n",
        "\n",
        "const result = await chatCompletion(\n",
        "    \"Analyze this multi-page medical document set. Extract patient referral page, medical insurance card and identification form in 3 separate fields in JSON format.\",\n",
        "    undefined,\n",
        "    [DOC_URL]\n",
        ");\n",
        "\n",
        "console.log(\">> RESULT\");\n",
        "console.log(result);\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Conclusion\n",
        "\n",
        "This cookbook demonstrated the comprehensive document understanding capabilities of the **VLM Run Orion Agent API** using Node.js/TypeScript.\n",
        "\n",
        "### Key Takeaways\n",
        "\n",
        "1. **OCR Capabilities**: Extract text, tables, paragraphs, and figures from documents with high accuracy using advanced OCR models.\n",
        "\n",
        "2. **Layout Detection**: Identify document structure including headers, footers, tables, figures, text blocks, and other layout elements with precise bounding boxes.\n",
        "\n",
        "3. **Document Redaction**: Detect and redact sensitive information including:\n",
        "   - PII (Personally Identifiable Information)\n",
        "   - Financial data (account numbers, SSNs, credit cards)\n",
        "   - PHI (Protected Health Information) for HIPAA compliance\n",
        "   - Domain-specific sensitive data (legal, insurance, real estate, etc.)\n",
        "\n",
        "4. **Multi-Document Classification**: Classify documents into categories based on content, structure, and visual features with confidence scores and rationales.\n",
        "\n",
        "5. **OpenAI-Compatible Interface**: The API follows the OpenAI chat completions format, making it easy to integrate with existing workflows and tools.\n",
        "\n",
        "6. **Structured Outputs**: Use Zod schemas with `response_format` parameter to get type-safe, validated responses with automatic parsing.\n",
        "\n",
        "7. **Streaming Support**: For long-running document processing tasks, enable streaming to receive partial results as they become available.\n",
        "\n",
        "8. **Combined Workflows**: Combine OCR, layout detection, redaction, and classification in a single pipeline for comprehensive document understanding.\n",
        "\n",
        "### Use Cases\n",
        "\n",
        "- **Document Processing**: Automate extraction of structured data from invoices, receipts, forms, and contracts\n",
        "- **Compliance**: Ensure HIPAA, GDPR, PCI-DSS, and other regulatory compliance through automated redaction\n",
        "- **Document Management**: Classify and organize large document collections\n",
        "- **Data Extraction**: Extract tables, figures, and structured content from documents\n",
        "- **Privacy Protection**: Automatically detect and redact sensitive information before sharing documents\n",
        "\n",
        "### Next Steps\n",
        "\n",
        "- Explore the [VLM Run Documentation](https://docs.vlm.run) for more details\n",
        "- Join our [Discord community](https://discord.gg/AMApC2UzVY) for support\n",
        "- Check out more examples in the [VLM Run Cookbook](https://github.com/vlm-run/vlmrun-cookbook)\n",
        "- Review the [VLM Run Node.js SDK](https://github.com/vlm-run/vlmrun-node-sdk) documentation\n",
        "- Review domain-specific redaction agents for financial, healthcare, legal, and other industries\n",
        "\n",
        "Happy building!\n"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
